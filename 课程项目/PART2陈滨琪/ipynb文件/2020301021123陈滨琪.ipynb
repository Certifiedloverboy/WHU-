{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part1 01——04论文文本预处理维度\n",
    "01操作 对文本进行分词\n",
    "文本分词是一切文本分析的基础，在这里将用到NLTK库以及其中的分词函数word_tokenize。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "fn = open('计算社会科学论文集.txt','r',encoding='utf-8') # 打开文件，一定注意编码的格式是UTF-8\n",
    "string_data = fn.read()  # 读出整个文件\n",
    "fn.close() # 关闭文件\n",
    "splitresult=nltk.word_tokenize(string_data)#得到分词结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 02操作 移除文本中的停用词\n",
    " 停止词是任何语言中出现频率最高的词，它们只是用来支持句子的结构，对句子的语义没有多大意义。\n",
    " 因此在不牺牲句子含义的情况下,我们可以在NLP过程之前从任何文本中删除NLTK提供的停止词，使分析更有效率。\n",
    " 我们将使用NLTK库提供的停用词列表（stop_words)来过滤文本中包含的停用词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")  #注意！需要下载NLTK中的停止词库（如果失败，可以挂梯子后重新尝试）\n",
    "from nltk.corpus import stopwords #使用NLTK加载停用词\n",
    "stop_words=stopwords.words(\"english\")#通过设定参数，表示分析文本是英文\n",
    "def remove_stop_words(splitresult,stop_words):#定义停用词过滤函数\n",
    "    return\" \".join([word for word in splitresult if word not in stop_words])#循环遍历句子中的每一个单词并检查是否有停用词，最后将结果组合。\n",
    "removeresult=remove_stop_words(splitresult,stop_words)#得到去除了停用词后的文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03操作 文本清洗\n",
    "大多数情况下，一些对句子的意义没有什么真正作用的文字和元素我们需要将其删除掉。\n",
    "这些元素不仅会浪费系统内存和处理时间，还会对结果的准确性产生负面影响。\n",
    "因此我们将对文本进行简单的清洗，去掉一些符号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #导入re库\n",
    "def clean_text(x):\n",
    "    temp=re.sub(r'([^\\s\\w]|_)+','',x).split()#利用正则表达式去除掉文本中除数字、字母和空格外的字符\n",
    "    return \" \".join(word for word in temp)#特别注意！下一步操作需要的数据类型是字符串，而我们上一步生成的是列表，需要转换一下！\n",
    "cleanoutcome=str(clean_text(removeresult))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "04操作 词干提取与词形还原\n",
    "词干提取是去除单词的前后缀得到词根的过程，常见的前后词缀有：名词的复数、进行时、过去分词。\n",
    "词形还原是基于词典，将单词的复杂形态转变成最基础的形态。比如将“is are benn”统一转换为“be”。\n",
    "目前实现词干提取和词形还原的方法均是利用语言中存在的规则/已有字典映射提取词干或获得词的原形。\n",
    "在原理上，词干提取主要是采用缩减的方法，将词转换为词干,如:将cats处理为cat, 将effective处理为effect 。而词形还原主要采用转变的方法，将词转变为其原形，如将drove处理为drive, 将driving处理为drive 。\n",
    "在复杂性上，词干提取方法相对简单，词形还原则需要返回词的原形，需要对词形进行分析，不仅要进行词缀的转化,还要进行词性识别，区分相同词形但原形不同的词的差别。词性标注的准确率也直接影响词形还原的准确率，因此，词形还原更为复杂。\n",
    "在实现方法上，虽然词干提取和词形还原实现的主流方法类似，但二者在具体实现上各有侧重.词干提取的实现方法主要利用规则变化进行词缀的去除和缩减，从而达到词的简化效果。词形还原则相对较复杂，有复杂的形态变化,单纯依据规则无法很好地完成。其更依赖于词典，进行词形变化和原形的映射，生成词典中的有效词。\n",
    "在结果上，词干提取和词形还原也有部分区别。词干提取的结果可能并不是完整的、具有意义的词，而只是词的一部分，如: revival 词干提取的结果为reviv, ailiner 词干提取的结果为airlin. 而经词形还原处理后获得的结果是具有一定意义的、 完整的词，-般为词典中的有效词。\n",
    "在应用领域上，同样各有侧重。虽然二者均被应用于信息检索和文本处理中，但侧重不同。词干提取更多被应用于信息检索领域，如: Solr, Lucene等,用于扩展检索，粒度较粗。词形还原更主要被应用于文本挖掘、自然语言处理，用于更细粒度、更为准确的文本分析和表达。\n",
    "词干提取的算法一般有三种：porter/snowball(porter2)/lancaster，根据使用反馈，选择采用snowball算法，因为其速度更快，消耗内存更少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import stem #使用前要在电脑中用pip安装下载SnowballStemmer: Win键+R，输入cmd，输入\"pip install SnowballStemmer\"\n",
    "def get_stems(word,stemmerresult):\n",
    "    stemmerresult=stem.SnowballStemmer(\"english\")#特别注意！使用snowballstemmer一定要标注好目标语言，目前暂不支持中文哦！\n",
    "    return stemmerresult.stem(word)#返回处理后的函数值\n",
    "stemmeresult=stem.SnowballStemmer(\"english\")\n",
    "snowballoutcome=get_stems(cleanoutcome,stemmeresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词形还原和词干提取比较相似，将一个任意形式的单词转换为语法基础形式。 然而，词形还原是基于词典的.\n",
    "Python 中的NLTK库包含英语单词的词汇数据库。这些单词基于它们的语义关系链接在一起。链接取决于单词的含义。\n",
    "词形还原处理将使用到WordNetLemmatizer ，它是WordNet的NLTK接口。\n",
    "WordNet是一个免费的英语词汇数据库,可以用来生成单词之间的语义关系。NLTK的WordNetLemmatizer 提供了lemmatize() 的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"wordnet\")#注意！需要下载NLTK中的wordnet库！（如果失败，可以挂梯子后重新尝试）\n",
    "from nltk.stem.wordnet import WordNetLemmatizer #导入词形还原工具WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()#创建一个词形还原器\n",
    "def get_lemma(word):\n",
    "    return lemmatizer.lemmatize(word)\n",
    "wordnetoutcome=get_lemma(snowballoutcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#接下来就是把处理好的文本生成txt文件以便下一步分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"文本预处理后的计算社会科学论文集.txt\",\"a+\",encoding=\"utf-8\")as f:\n",
    "    f.write(wordnetoutcome)\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part2 情感分析维度\n",
    "极性(Polarity): 极性是衡量特定语言中情绪的消极或积极程度的指标。使用极性,因为它是简单、易于测量,可以很容易地转换为一个简单的数字刻度。\n",
    "它通常在-1 到1之间。接近1的值反映文档有积极情绪，而接近-1的值反映文档有消极情绪。0 左右的值反映了文档在情绪上是中立的。\n",
    "强度(Intensity):与极性(由负到正)不同, 强度是根据唤醒程度(由低到高)来衡量的。最常见的情况是,情感的强度也包括在情感评分中。\n",
    "它是通过观察得分接近于0或1来衡量的。\n",
    "主客观性(Subjectivity versus Objectivity) :可以检测给定的文本源是主观的还是客观的。\n",
    "我将使用textblob库的TextBlob 类来实现文本情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "fnoutcome = open('文本预处理后的计算社会科学论文集.txt','r',encoding='utf-8') # 打开文件，一定注意编码的格式是UTF-8\n",
    "data = fnoutcome.read()  # 读出整个文件\n",
    "fnoutcome.close() # 关闭文件\n",
    "blob=TextBlob(data) #对整个文件的情感进行分析\n",
    "print(blob.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part3 记录词频并生成词云维度\n",
    "词频（TF）表示词条（关键字）在文本中出现的频率。\n",
    "IDF是逆向文件频率(Inverse Document Frequency)\n",
    "逆向文件频率 (IDF) ：某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到。\n",
    "如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from optparse import OptionParser\n",
    "from wordcloud import WordCloud,ImageColorGenerator\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# -------------------\n",
    "# 下面区间和parser相关的代码只有一个作用：读入要处理的文件名和topK数量\n",
    "\n",
    "USAGE = [\"-f\",\"文本预处理后的计算社会科学论文集.txt\",\"-k\",500,\"文本预处理后的计算社会科学论文集.txt\"]     #用法类似字典，“-字母”后接上对应的值\n",
    "parser = OptionParser()                     #初始化Parser 生成参数构造器\n",
    "parser.add_option(\"-f\", dest=\"file_name\")   #dest：对 -f 这类缩写含义的完整描述\n",
    "parser.add_option(\"-k\", dest=\"topK\")\n",
    "opt, args = parser.parse_args(USAGE)        #解析USAGE列表中参数，可以通过输出观察规律\n",
    "print(\"op : \",opt)                          #观察parser.parse_args输出结果\n",
    "print(\"ar : \",args)\n",
    "\n",
    "#从上面代码程序知道了要分析的文件是经过文本预处理后的计算社会科学论文集.txt， 关键词数为500。\n",
    "#下面是实现jieba关键词分词提取的代码\n",
    "\n",
    "if len(args) < 1:\n",
    "    print(USAGE)            #如果没有args，则程序终止。\n",
    "    sys.exit(1)             #退出\n",
    "file_name = args[0]         #由ar :  ['文本预处理后的计算社会科学论文集.txt']知道， args[0]为'文本预处理后的计算社会科学论文集.txt'\n",
    "                            #opt.file_name也等于'文本预处理后的计算社会科学论文集.txt'\n",
    "\n",
    "if opt.topK is None:        #如果没有输入topK值，则默认为10\n",
    "    topK = 10\n",
    "else:                       #若有topK，则使用自定义的。\n",
    "    topK = int(opt.topK)\n",
    "content = open(file_name, 'rb').read()                  # rb 对文件的操作类型：将文本预处理后的计算社会科学论文集.txt内容读到content中\n",
    "tags = jieba.analyse.extract_tags(content, topK=topK)   #通过分词函数输入文本内容和topK数，得到分词结果tags（列表形式)\n",
    "result=(\",\".join(tags))                                   #用逗号隔开每个分词，输出关键词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "形成根据背景颜色变换的词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=np.array(Image.open(\"头像.png\")) #为词云提供形状背景\n",
    "wordcloud = WordCloud(mask=mask,font_path = 'OLDENGL.TTF',max_words=500,mode='RGBA',background_color=None).generate(result)\n",
    "\n",
    "image_colors = ImageColorGenerator(mask) #从图片的颜色生成词云的颜色\n",
    "wordcloud.recolor(color_func=image_colors)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')# 显示词云\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "wordcloud.to_file('wordcloud.png')# 保存到文件"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

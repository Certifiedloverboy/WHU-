Life in the network: the coming age of computational social science

We live life in the network. When we wake up in the morning, we check our e-mail, make a quick phone call, walk outside (our movements captured by a high definition video camera), get on the bus (swiping our RFID mass transit cards) or drive (using a transponder to zip through the tolls). We arrive at the airport, making sure to purchase a sandwich with a credit card before boarding the plane, and check our BlackBerries shortly before takeoff. Or we visit the doctor or the car mechanic, generating digital records of what our medical or automotive problems are. We post blog entries confiding to the world our thoughts and feelings, or maintain personal social network profiles revealing our friendships and our tastes. Each of these transactions leaves digital breadcrumbs which, when pulled together, offer increasingly comprehensive pictures of both individuals and groups, with the potential of transforming our understanding of our lives, organizations, and societies in a fashion that was barely conceivable just a few years ago.

The capacity to collect and analyze massive amounts of data has unambiguously transformed such fields as biology and physics. The emergence of such a data-driven “computational social science” has been much slower, largely spearheaded by a few intrepid computer scientists, physicists, and social scientists. If one were to look at the leading disciplinary journals in economics, sociology, and political science, there would be minimal evidence of an emerging computational social science engaged in quantitative modeling of these new kinds of digital traces. However, computational social science is occurring, and on a large scale, in places like Google, Yahoo, and the National Security Agency. Computational social science could easily become the almost exclusive domain of private companies and government agencies. Alternatively, there might emerge a “Dead Sea Scrolls” model, with a privileged set of academic researchers sitting on private data from which they produce papers that cannot be critiqued or replicated. Neither scenario will serve the long-term public interest in the accumulation, verification, and dissemination of knowledge.

What potential value might a computational social science, based in an open academic environment, offer society, through an enhanced understanding of individuals and collectives? What are the obstacles that stand in the way of a computational social science?

From individuals to societies

To date the vast majority of existing research on human interactions has relied on one-shot self-reported data on relationships. New technologies, such as video surveillance, e-mail, and ‘smart’ name badges offer a remarkable, second-by-second picture of interactions over extended periods of time, providing information about both the structure and content of relationships. Consider examples of data collection in this area and of the questions they might address:

Video recording and analysis of the first two years of a child’s life (1)

Precisely what kind of interactions with others underlies the development of language? What might be early indicators of autism?

Examination of group interactions through e-mail data

What are the temporal dynamics of human communications—that is, do work groups reach a stasis with little change, or do they dramatically change over time (2, 3)? What interaction patterns predict highly productive groups and individuals? Can the diversity of news and content we receive predict our power or performance (4)?

Examination of face-to-face group interactions over time using sociometers

Small electronics packages (‘sociometers’) worn like a standard ID badge can capture physical proximity, location, movement, and other facets of individual behavior and collective interactions. What are patterns of proximity and communication within an organization, and what flow patterns are associated with high performance at the individual and group levels (5)?

Macro communication patterns

Phone companies have records of call patterns among their customers extending over multiple years, and e-Commerce portals such as Google and Yahoo collect instant messaging data on global communication. Do these data paint a comprehensive picture of societal-level communication patterns? What does the “macro” social network of society look like (6), and how does it evolve over time? In what ways do these interactions affect economic productivity or public health?

Tracking movement

With GPS and related technologies, it is increasingly easy to track the movements of people (7, 8). Mobile phones, in particular, allow the large scale tracing of people’s movements and physical proximities over time (9), where it may be possible to infer even cognitive relationships, such as friendship, from observed behavior (10). How might a pathogen, such as influenza, driven by physical proximity, spread through a population (11)?

Internet

The Internet offers an entirely different channel for understanding what people are saying, and how they are connecting (12). Consider, for example, in this political season, tracing the spread of arguments/rumors/positions in the blogosphere (13), as well as the behavior of individuals surfing the Internet (14), where the concerns of an electorate become visible in the searches they conduct. Virtual worlds, by their nature capturing a complete record of individual behavior, offer ample opportunities for research, for example, experimentation that would be impossible or unacceptable (15). Similarly, social network websites offer an unprecedented opportunity to understand the impact of a person’s structural position on everything from their tastes to their moods to their health (16), while Natural Language Processing offers increased capacity to organize and analyze the vast amounts of text from the Internet and other sources (17).

In short, a computational social science is emerging that leverages the capacity to collect and analyze data with an unprecedented breadth and depth and scale. Substantial barriers, however, might limit progress. Existing ways of conceiving human behavior were developed without access to terabytes of data describing their minute-by-minute interactions and locations of entire populations of individuals. For example, what does existing sociological network theory, built mostly on a foundation of one-time ‘snapshot’ data, typically with only dozens of people, tell us about massively longitudinal datasets of millions of people, including location, financial transactions, and communications? The answer is clearly “something,” but, as with the blind men feeling parts of the elephant, limited perspectives provide only limited insights. These emerging data sets surely must offer some qualitatively new perspectives on collective human behavior.

There are significant barriers to the advancement of a computational social science both in approach and in infrastructure. In terms of approach, the subjects of inquiry in physics and biology present different challenges to observation and intervention. Quarks and cells neither mind when we discover their secrets nor protest if we alter their environments during the discovery process (although, as discussed below, biological research involving humans offers some similar concerns regarding privacy). In terms of infrastructure, the leap from social science to a computational social science is larger than from, say, biology to a computational biology, in large part due to the requirements of distributed monitoring, permission seeking, and encryption. The resources available in the social sciences are significantly smaller, and even the physical (and administrative) distance between social science departments and engineering or computer science departments tends to be greater than for the other sciences. The availability of easy-to-use programs and techniques would greatly magnify the presence of a computational social science. Just as mass-market CAD software revolutionized the engineering world decades ago, common computational social science analysis tools and the sharing of data will lead to significant advances. The development of these tools can, in part, piggyback on those developed in biology, physics and other fields, but also requires substantial investments in applications customized to social science needs.

Perhaps the thorniest challenges exist on the data side, with respect to access and privacy. Many, though not all, of these data are proprietary (e.g., mobile phone and financial transactional data). The debacle following AOL’s public release of “anonymized” search records of many of its customers highlights the potential risk to individuals and corporations in the sharing of personal data by private companies (18). Robust models of collaboration and data sharing between industry and the academy need to be developed that safeguard the privacy of consumers and provide liability protection for corporations.

More generally, properly managing privacy issues is essential. As the recent NRC report on GIS data highlights, it is often possible to pull individual profiles out of even carefully anonymized data (19). To take a non-social science example: this past Summer NIH and the Wellcome Trust abruptly removed a number of genetic databases from online access (20). These databases were seemingly anonymized, simply reporting the aggregate frequency of particular genetic markers. However, research revealed the potential for de-anonymization, based on the statistical power of the sheer quantity of data collected from each individual in the database (21).

A single dramatic incident involving a breach of privacy could produce a set of statutes, rules, and prohibitions that could strangle the nascent field of computational social science in its crib. What is necessary, now, is to produce a self-regulatory regime of procedures, technologies, and rules that reduce this risk but preserve most of the research potential. As a cornerstone of such a self-regulatory regime, Institutional Review Boards (IRBs) must increase their technical knowledge enormously to understand the potential for intrusion and individual harm because new possibilities do not fit their current paradigms for harm. For example, many IRBs today would be poorly equipped to evaluate the possibility that complex data could be de-anonymized. Further, it may be necessary for IRBs to oversee the creation of a secure, centralized data infrastructure. Certainly, the status quo is a recipe for disaster, where existing data sets are scattered among many different groups, with uneven skills and understanding of data security, with widely varying protocols.

Researchers themselves must tackle the privacy issue head on by developing technologies that protect privacy while preserving data essential for research (22). These systems, in turn, may prove useful for industry in managing privacy of customers and security of their proprietary data.

Finally, the emergence of a computational social science shares with other nascent interdisciplinary fields (e.g., sustainability science) the need to develop a paradigm for training new scholars. A key requirement for the emergence of an interdisciplinary area of study is the development of complementary and synergistic explanations spanning different fields and scales. Tenure committees and editorial boards need to understand and reward the effort to publish across disciplines (23). Certainly, in the short run, computational social science needs to be the work of teams of social and computer scientists. In the longer run, the question will be: should academia be building computational social scientists, or teams of computationally literate social scientists and socially literate computer scientists?

The emergence of cognitive science in the 1960s and 1970s offers a powerful model for the development of a computational social science. Cognitive science emerged out of the power of the computational metaphor of the human mind. It has involved fields ranging from neurobiology to philosophy to computer science. It attracted the investment of substantial resources to establish a common field, and it has created enormous progress for public good in the last generation. We would argue that a computational social science has a similar potential, and is worthy of similar investments.
Figure 1
This figure summarizes the link structure within a community of political blogs (from 2004), where red nodes indicate conservative blogs, and blue liberal. Orange links go from liberal to conservative, and purple ones from conservative to liberal. The size of each blog reflects the number of other blogs that link to it.
Figure 2
The location (after adding randomized synthetic noise) of several hundred mobile devices in the city of San Francisco. Each location is color coded to indicate which of 8 “tribes” (or social clusters) each user belongs to. Tribes are computed by clustering (otherwise anonymized) users according to how similar their movement patterns are over a few weeks. The movement analysis is performed using the Minimum Volume Embedding algorithm.
Figure 3
Patterns of email (blue) and face-to-face communication (read) within a German bank over a period of one month. Productivity and information overload is correlated with the sum of both types of communication, but not with either alone.
Manifesto of computational social science
Objectives and opportunities
In a world of demographic explosion, global crises, ethnic and religious disturbances
and increasing crime the understanding of the structure and function of society, as
well as the nature of its changes, is crucial for governance and for the well being of
people. Humanity is currently facing grand challenges. Setting aside environmental
issues and the depletion of natural resources, we have to cope with formidable social
and political problems:
Change of the population structure (change of birth rate, migration);
– Financial and economic instability (trust, consumption and investments; sovereign
debt, taxation, and inflation/deflation; sustainability of social welfare systems, and
so on);
– Social, economic and political divide (among people of different gender, age, education,
income, religion, culture, language, preferences);
– Threats against health (due to the spreading of epidemics, but also to unhealthy
diets and habits);
– Unbalance of power in a multi-polar world;
– Organized crime, including cyber-crime, social unrest and war;
– Uncertainty in institutional design and dynamics (regarding regulations, authority,
corruption, balance between global and local, central and decentralized systems);
– Unethical usage of communication and information systems (cyber risks, violation
of privacy, misuse of sensitive data, spam).
In the last couple of years, social scientists have started to organize and classify the
number, variety, and severity of criticalities, if not pathologies and failures, recurring
in complex social systems [1,2]. These are amongst the most severe social problems,
difficult to predict and treat, and raising serious social alarm.
Furthermore, human society has never before changed as fast as it is changing today.
Technological development has opened entirely new channels of communication,
induced new behavioural patterns, substantially influenced organization principles,
and its products are becoming history-forming factors. We human beings have preserved
our basic, genetically determined biological properties over tens of thousands
of years but our social behaviour seems to be altered with an unprecedented speed,
continuously challenging our adaptivity.
Part of the difficulty for us to respond to the challenges mentioned above is inherent
to fundamental features of social complexity. Complex social systems are characterised
by multiple ontological levels with multidirectional connections, proceeding
not only from the micro to the macroscopic levels but also back from the macro
to the micro-levels [3]. Furthermore, complex social systems present a far-reaching
and accelerated diffusion of phenomena, behaviours and cultural traits. Accelerated
contagion leads on one hand to new systems’ properties emerging at the aggregate
level – for example new public opinions and political movements, new global and
local identities, collective preferences, attitudes, even moods, etc. – and on the other
to major critical event in the social economic and/or political spheres, such as global
financial crises and the collapse of regimes.
Finally, complex social systems do often show interdependences and interferences
among their properties and processes of transformation. The interplay between cultural
and biological evolution shows unexpected intricacies, far from the parallel predicted
by the Dual Inheritance Theory [4], as shown by the Demographic Transition
(DT) model [5]. Based on an interpretation of demographic history developed in 1929
by the American demographer Warren Thompson, the DT model points to a growing
gap between economic and demographic growth: all over the world, the higher the
average income of the population the lower its birth rate.
These problems depend on the same circumstances that might help us find solutions:
a high degree of poorly understood and poorly investigated technology-driven
innovation. ICT applications seem to act both in favour and against our capacity
to answer the grand challenges before us. The widespread access to the Internet is
seriously and often positively impacting the frequency, range and style of human
communication and interaction, leading to heterogeneous interconnected networks.
Electronic communications seem to have played a fundamental role in the diffusion
and organization of the protest movements arising in Northern Africa, and leading
to regime change. At the same time, the view that social networks connect people is oversimplified, and the question remains open as to what types of connections are established
among them, whether, for example, pro or antisocial, and – since resources
like time are limited – to the expense of what and of whom. The alternative offered by
Internet to the hierarchical organization of cultural production and specialised professional
advice pushes up symmetrical, horizontal interactions. At the same time, the
Open Source community challenges the foundation of intellectual property as well
as the institution of truth certification. Are these effects only signs of providential
progress? What about the nature and functioning of economic, cultural and political
institutions? What about the credibility of the information spread and its effective
truth-value?
That much for the negative side of the coin. But there is also a positive side.
Information and communication technologies can greatly enhance the possibility to
uncover the laws of the society. First, ICT produces a flood of data. These data
represent traces of almost all kinds of activities of individuals enabling an entirely new
scientific approach for social analysis. Second, the development of computer capacities
makes it possible to handle the data deluge and to invent models that reflect the
diversity and complexity of the society.
The analysis of huge data sets as obtained, say, from mobile phone calls, social
networks, or commercial activities provides insight into phenomena and processes at
the societal level. Investigating peoples’ electronic footprints did already contribute
to understand the relationship between the structure of the society and the intensity
of relationships [6] and the way pandemic diseases spread [7], as well as to identify
the main laws of human communication behaviour [8].
The traditional tools of social sciences would at most scratch the surface of
these issues, whereas new tools can shed light onto social behaviour from totally
different angles. Possibilities ranging from supercomputers to distributed computing
make the execution of large-scale, heterogeneous multi-agent programs possible, programs
which prove particularly apt to model the complexity of social and behavioural
systems.
The new ICT-enabled study of society has been named computational social science
[9]. This is a truly interdisciplinary approach, where social and behavioural scientists,
cognitive scientists, agent theorists, computer scientists, mathematicians and
physicists cooperate side-by-side to come up with innovative and theory-grounded
models of the target phenomena. Computational social scientists strongly believe
that a new era has started in the understanding of the structure and function of our
society at different levels [9].
On the one hand, computational social science is aimed to favour and take advantage
of massive ICT data. On the other, it is a model-based science yielding both
predictive and explanatory models. Hence, it is intended to profit from the modelling
instruments made available by ICT for producing generative models of large-scale
multi-agent systems. Both objectives must be achieved to turn social science into
applicable tools that can inform decision makers about issues of major concern.
In this paper, we will work towards the drafting of a Manifesto for the new
Computational Social Science.
The paper will unfold as follows: in the next section, the state of the art of the field
will be discussed. In the third section, its main characteristics will be examined. In the
fourth section, the main challenges the new field is facing will be addressed. Finally,
in the fifth section, we will turn our attention to compare and discuss the types of
models that are compatible or necessary for a computational social scientific program
as outlined in the previous part of the paper. Final considerations will conclude the
paper, but not the process to constitute the new discipline, that for a Computational
Social Science is an inherently dynamic scientific program. This paper is only part of
the beginning of it.
2 State of the art
2.1 Emergent phenomena at the aggregate level
The computational study of social phenomena has been focused on the emergence of
all sorts of collective phenomena and behaviours from among individual systems in
interaction – including segregation [10], cooperation [11], reciprocity [13], social norms
[14–17], institutions [18], etc. Let us briefly re-examine the major research directions.
Emergent social behaviour. The study of emergent social behaviour did greatly
benefit from computational and simulation-based modelling. Although even phenomena
like civil violence and rebellion have been investigated [19], computational modelling
has so far been mainly applied to behaviours like altruism, cooperation and
norm conformity.
The study of altruism has generally been cast into the evolutionary framework.
However, computational studies of the cultural dimension of this phenomenon exist
– see the economy of [20,21]; cf. later on in the paper, 4.1.3.
While the perspective in which the study of cooperation is generally framed is
game theoretic, the current frontiers in the computational study of cooperation arise
in other formal fields, like complex systems science. Socio-semantic systems dragged
the attention of the scientific community to investigate quantitatively how cooperative
phenomena emerge and can be harnessed to improve the performance of collective
tasks [22–24].
The most insightful computational studies of altruism are due to Nowak and
Sigmund Nowak and Sigmund 1998, who had the merit, among others, to point out
the role of image scoring in the evolution of donation. In turn, image scoring gave
impulse to the study of reputation (for two recent reviews see Walker [26] and Ebbers
and Wijnberg [27] in the emergence of cooperation from repeated interaction (the so
called shadow of the future, [28]) and from networks of interconnected agents [29].
Thanks to its evolutionary background, the computational modelling of prosocial
behaviour was aimed to identify the distal causes of altruism and cooperation, i.e.,
their impact on the individual and the social group’s chances for survival. One of
the principal directions of investigation of the distal causes of cooperation concerned
strong reciprocity, i.e., the spontaneous attitude to punish free-riders. Evolutionary
game theorists showed the positive effect of strong reciprocity on both the cooperators’
and the groups’ fitness by simulating artificial hunter-gatherers populations.
The missed point in the evolutionary study of prosocial behaviour is the proximate
causes, i.e., the behavioural and mental mechanisms on which reciprocity is implemented.
Evolutionary theories point to the competitive advantage (distal cause) of
the behaviours to explain, leaving aside the question of how they could have ever
appeared in the behavioural repertoire of a species (proximate cause). The application
of computational methodologies to study also the proximate causes represents a
major challenge for the future computational social science.
Emergent social aggregates. The best known and most influential work in this
area is Schelling’s seminal paper on segregation [10], which gave impulse to a great
deal of computational studies of emergent structures at the aggregate level (see e.g.
[30–32]). Computational studies of spontaneous group formation and the emergence
of coalitions and collective entities are also at study among social scientists and game
theorists since long [33]. This research direction had the merit to point out the role
of extortion and tribute as mechanisms of political coalition formation.
However, no much attention was given to the opposite direction of influence, i.e.,
downward causation or second order emergence [34,35]. Furthermore, the study of
emerging phenomena at the aggregate level not always shed light on the foundations of
social structures. Social networks, for example, are investigated by the properties they exhibit once they have emerged (e.g., scale-freeness). Instead, poor attention has been
paid so far to the conditions favouring, and allowing to predict, the emergence of social
networks (see [36] and later on in this paper, 4.1.2). Agent-based modellers have begun
to address this issue and to study the link between fundamental behavioural processes,
social conditions, and the macroscopic structure of emergent complex networks [37].
Emergent institutions. While the study of emergent social behaviour has mainly
addressed positive social action, a wide spectrum of institutions has been observed
to emerge in computational environments, from the market [38] to money [39]; from
social organization [40] to the modern state [41].
As to theoretical frameworks, the study of emerging conventions and social norms
[14,19,42–44] is greatly indebted to the game theoretic framework and to the philosophical
bases of the rationality theory [16,45]. Nonetheless, agent-based models of
norm emergence have been developed by authors not committed to a rational view
of agency [46–48]. The study of norm emergence turned out to be one of the most
prolific domains of investigation of computational social science. Nonetheless, a number
of questions have been left open, the most important of which perhaps is how to
account for norm compliance.
In sum, the study of emergent phenomena has largely profited from the adoption
of computational methodologies. Rather than conclusive, however, the results
obtained so far indicate new promising directions of research, in particular, (a) the
micro-foundations of social structures and networks; (b) the proximate causes in evolutionary
explanation, i.e., the internal mechanisms that contribute to explain the
advantages of the target behaviours for the individual and for the group and (c) the
way back in the dynamics of emergent phenomena like norms and institutions, and
how they manage to be adopted by executors. These are all important challenges for a
field, like computational social science, that has the potential to re-found the science
of society.
2.2 Social learning systems and mechanisms
Rather than a topic of investigation per se, learning is a property on which a great
deal of computational social science builds upon (see N 4.5 Democratising Big Data,
Complexity Modelling and Collective Intelligence). All sorts of social dynamics are attributed
to learning processes, based on direct reinforcement or imitation. Replicator
dynamics has been strongly influential in the study of social and economic processes,
and still is one of the techniques on which computational models of social dynamics
are implemented.
Despite the conceptual and theoretical weaknesses of the models and techniques
used, learning systems have had a strongly innovative effect on the study of social
influence, yielding some of the most brilliant results ever achieved by computational
social science so far. One example is the out-of-equilibrium economy (see [49]; see
also the non-equilibrium social science of Hales and Johnson, in which what matters
is not the equilibrium obtained, but how it is obtained. Learning is a fundamental
mechanism that may lead, but not necessarily, to achieve an equilibrium under specific
conditions. As shown by Arthur’s agent-based model of an artificial rudimentary stock
market, the possibility to achieve an equilibrium depends on the speed of the learning
process: the faster the process, the more unstable the equilibrium. Under special
conditions, the non-linear and inconsistent effects of learning become paradoxical:
for instance, in the minority game [50], people adjust their behaviour to their own
expectations concerning certain events. However, while adjusting their behaviour,
people modify the expected conditions, somehow contributing to disconfirm their
expectations. This problem, which has received a great deal of attention in the last couple of decades, shows the paradoxical effects of learning and the complex, out-ofequilibrium
character of complex problems. Further computational work has revealed
how learning dynamics can lead to solutions of the problem of cooperation that are
out-of-equilibrium from a rational actor perspective, but may be robust when agents
are boundedly rational [51]. The non-equilibrium phenomena generated by learning
dynamics are a decisive battlefield for computational social science.
2.3 Quantitative computational social science
Computational social science, in terms of agent-based models (ABM), has existed for
a few years. To date, it has been used more as a qualitative tool, e.g., to provide
plausible explanations to social phenomena [52]. However, much effort has recently
been made towards achieving a more quantitative orientation of this kind of research,
and advances have been made in several important directions.
One important topic in this line of research is the assessment of the validity of
simulations: As Gilbert [34] put it:
“You should assume that, no matter how carefully you have designed and built
your simulation, it will contain bugs (code that does something different to what you
wanted and expected).”
Techniques to verify the correctness of the code have been developed and are increasingly
used [53] including, for instance, comparison of simplified versions of the
model with analytical results or duplication of simulations in different machines, languages
or both. Less progress has been made, though, towards simulations that can
be quantitatively compared with specific social phenomena, and validation (i.e., assessing
the validity of a model by the extent that it provides a satisfactory range of
accuracy consistent with its intended application) is often only qualitative. Quantitative
validation will require detailed explorations of the parameter space that, in turn,
would call for appropriate computational facilities, such as grid computing; these
would bring back the issue of code verification. Suitable quantities for meaningful
comparisons need to be identified and measured in the social context of interest (or
at least in controlled experiments). These are crucial steps that need to be carefully
addressed if computational science, and in particular model simulation, are to be
really quantitative.
A second relevant problem with which computational social science is dealing in
its progress towards becoming quantitative is massive data analysis. In addition to the
computational requirements, common to all the questions mentioned in this section,
the design and implementation of efficient and reliable analysis algorithms is at the
core of the research efforts these days. Examples like the controversy on reports of
contagion of obesity in social networks [54,55] clearly show the importance of rigor in
analyzing data in a meaningful way, allowing to distinguish between factors leading
to certain behaviors and to identify causation when possible. Important progress has
here been made by methods that tackle the problem of statistically modeling complex
and interdependent dynamics of behavior and networks with integrating agent-based
simulation techniques with statistical approaches for parameter selection [56]. The
further development of these approaches becomes even more acute when data are so
massive as to prevent the use of well-known algorithms, something that is become
more and more frequent as new sources of data become available.
Finally and importantly, the previous topic is certainly related with data driven
simulations, carried out to compare with, understand, and if possible predict real-life
phenomena. Simulation models such as ABMs can be constructed to support persistent
run-time interactions between computer agents and real-world entities via general types of input-output data streams [57]. In this way, ABMs become data-driven dynamic
applications systems [58] entailing the ability to incorporate additional data
into an executable application and, vice versa, the ability of applications to dynamically
steer the measurement process. Again, the computational and ICT requirements
to augment this kind of approach with massive sources of data to simulate large social
groups are enormous, and research along this line is at the forefront of the field right now.
3 Innovative approach: How to characterise the field
of computational social science
Are we well equipped to tackle the BigProblems mentioned above? Undoubtedly, we
are developing valuable instruments and techniques for generating, gathering, and
analysing data about grand challenges, but how about BigThinking, grand theories
matching grand challenges? Theories grow slowly, impeded by entrenched assumptions
and lack of data. The large-scale, founding constructs that should drive our
understanding of society are debated and misunderstood. In social sciences, there is
no consensus on the general mechanisms that underlie phenomena like institutions,
norm compliance and enforcement, reputation, trust, etc. Even cooperation, one of
the most studied aspects of social interaction, is still locked between free riding and
punishment. With Putterman [11], we believe that important social dilemmas can
only be solved if:
“The human sociality that evolved in our small-group past is robust enough to
overcome the ever-present temptations to free ride”.
How can we understand and help manage complex social systems if we still do
not understand the basics of sociality? Is the new world of automated information
treatment going to provide any help?
The answer is: yes, ICT can provide significant help for social science. Not only
ICT can help access, analyse and build upon BigData, i.e. new type of massive data,
for addressing BigProblems. It can also help provide instruments for BigThinking.
Indeed, computational social science can be characterised along two main aspects,
which both take advantage, one way or the other, from ICT developments: a) BigData,
and b) the role of computation in inspiring, formalizing and implementing the core
scientific concepts, principles, and ideas of computational social science.
3.1 BigData
Computational social science is intended to process data and run simulations at planetary
scale, where up to the whole world population is considered, in order to get a
better understanding of global social dynamics. This makes sense in a more and more
interconnected world, where the events occurring in one place can have tremendous
consequences on the other side of the globe [12]. For instance, migrations, the diffusion
of diseases, the consumption or production of goods should now all be considered
at planetary scale and involve the whole world population.
The latest evolution of Information and Communication Technologies (ICT) has
increasingly concerned the inclusion of users in the production of information. Nowadays,
users are not only able to exchange messages, images and sounds with other
individual peers, but also with whole communities whose size and composition can
be defined and tuned by the user him/herself. Moreover, the digital paradigm has allowed
the integration of multiple information and communication sources, including
connected PCs, phones and cameras. Accordingly, the distinction between consumers and producers of information, typical of a past era dominated by newspapers and
television, is vanishing.
Such an interconnected communication network has dramatically enlarged the
access to information sources with an undeniable advantage for the citizenship. At
the same time, it is presenting new challenges. Information broadcast by uncontrolled
sources could overload the network with noisy signals, preventing the meaningful ones
from being received by the desirable recipients. As a consequence, users’ attention
could be exhausted by useless information.
To overcome obstacles to the use of this increased amount of data, a number of
technologies have been developed. More sophisticated communication platforms have
emerged, up to current Web 2.0 social networks accessible from PCs and cell phones
where users can collectively categorize and evaluate the content they browse, providing
the community with an efficient information filter. The classification of digital
resources is typically performed by assigning labels (or tags) or scores to resources.
This collaborative categorization has given birth to several web-based folksonomies.
Consequently, the most popular websites now incorporate some sort of collaborative
categorization tools.
This ICT infrastructure has been applied not only to favour data exchange among
people, but also to outsource productive tasks. The main difference between this form
of crowdsourcing and traditional labour markets lies in the absence of prearranged
duties that workers deal with an idiosyncratic effort, while the infrastructure takes
care of summing up all contributions despite the heterogeneity and number of users.
First examples have concerned highly specialized tasks, such as open source software
development or scientific programs [59] that could be broken into smaller operations
performed by uncoordinated volunteers. More recently, the range of activities being
crowdsourced has expanded and forms a world-wide labour market facing tasks proposed
by various agencies and companies asking, e.g., for technological and marketing
solutions, or by research groups looking for volunteers for test and data mining activities
[60,61]. These kinds of infrastructures, therefore, are particularly appropriate
for the involvement of citizens in distributed sensing experiments [62,63].
Devices employed to get connected to communication networks have converged
in size and technological standards, expanding more and more the availability of an
Internet connection throughout daily life. Thus, users can now easily form dedicated
networks providing data that monitor particular issues. Such sensing networks can
be of an opportunistic or participatory type.
The participation of users in the monitoring affects both the resolution and the
quality of the data collected. Traditional sensing generally involves a small number
of highly controlled observation points. The low spatial resolution of the data
gathered in this way is compensated by the high data quality certified by the controlling
agency. On the other hand, distributed sensing relies on the possibility of
gathering large amounts of data from many uncontrolled sources, which cannot ensure
high data quality standards; however, by means of statistical methods together
with the possibility of storing and post-processing large datasets, this quality gap
with respect to traditional sensing can be overcome. Reasonably, users provide larger
quantities of data if the observed phenomenon and its management directly concern
the community involved in participatory sensing experiments. For example, people
might be interested in reporting meteorological observations in order to improve existing
models and receive more accurate weather forecasts, and this, as a virtuous
feedback, could be a reason for a citizen to provide more data to meteorological
centres.
For all these reasons, the application of a novel ICT-based sensing framework
may have a stronger impact here than in other areas, since the knowledge of the
underlying social interaction is crucial from many points of view: the quality of the environment is strongly affected by the behaviour of individuals in their most ordinary
daily situations; citizens’ behaviour, in turn, depends on their awareness; many bad
environmental practices arise when citizens do not coordinate in order to attain a
global optimal usage of collective resources, but rather pursue their own profit selfishly
– resulting in an even worse long term individual performance.
Hence, the field of computational social science is characterised as a new field of
science in which new type of data, largely made available by new ICT applications, can
be used to produce large-scale computational models of social phenomena. However,
the new field is not only characterised by new data at higher levels of temporal and
spatial scale, but also by new principles and concepts.
3.2 Core scientific concepts and ideas
Computational social science is a powerful tool for fostering our understanding of the
complexities of real socio-economic systems, by building “virtual computational social
worlds” that we can analyze, experiment with, feed with and test against empirical
data on a hitherto unprecedented scale. A range of excellent papers has been written
to make this point (e.g., [9]).
Computational models provide quantitative and qualitative models of social phenomena.
One critical application is generative explanations (see 4.4.2), in the form
of computer code, that reproduce some key features of societies. Hence, agent-based
modelling (multi-agent systems) plays a central role in computational social science,
because people (i.e., agents) are the primary subjects of social theories. Social science
is about how people think (psychology), handle wealth (economics), relate to each
other (sociology), govern themselves (political science), and create culture (anthropology).
Agentification is the process of formalizing a social theory as an agent-based
model.
The mind – beliefs, desires, intentions, values, and their processes – is at the root of
human social complexity. Cognitive science and social psychology are both necessary
for computational social science. Agent-based models benefit from richer cognitive
architectures, depending on specific modelling goals. For a specific discussion on this
see Sect. 4.
Computational social science is applied to real-world societies. These must be
complex or could not exist, because a set of critical functions is necessary for every
community to operate and endure. Computational social science is aimed to pay
attention not only and not primarily to variables and equations, but to the entities the
social world consists primarily of, i.e., people, ideas, human-made artefacts, and their
relations within ecosystems. These entities are modelled as computational objects
that encapsulate attributes and dynamics.
Another crucial idea of computational social science is adaptation. Social complexity
results from human adaptation to challenging environments (Simon’s Principle).
Social complexity as an emergent phenomenon is caused by successful adaptation.
Coupled socio-natural and socio-technical systems are typical examples of complex
adaptation.
Social complexity can be caused by uncertainty, which is commonly misunderstood
as something that cannot be known. We need to understand uncertainty and related
ideas (e.g., inequality, entropy). Probability and other scientific theories of uncertainty
(for example theory-driven models of opinion formation, revision and dynamics in
interplay with other mental constructs, like beliefs) are essential for understanding
social complexity.
The possibility of social change is ubiquitous and important in social life. Nothing
social occurs “out of the blue,” without antecedents. What may happen is as
important for social complexity as what has happened.Scaling is another crucial aspect of computational social science. The scaling typical
of a power law (“80-20 Rule”) follows a pattern of “many-some-rare”. Income,
wealth, conflicts, organizations, cities, and other features of social life obey power
laws. The first discoveries of power laws occurred in social science, not physics. Computational
social theories and agent-based models are capable of producing power
laws and other real-world social distributions. Not all scaling is the same; some cases
are special because of their criticality. Social scientists were not the first to recognize
criticality; physicists did. Criticality is insightful for some social phenomena (e.g.,
conflict), but still puzzling for others.
One of the most interesting aspects of social life is out-of-equilibrium. Most social
distributions are not Gaussian, or bell-shaped. They are often heavy-tailed, powerlaw
(Pareto), Weibull (exponential, Rayleigh, others), log-normal. Natural raw data
reflects the typical disequilibrium of social complexity – “normalizing” data using atheoretical
transformations (for regression analysis) may destroy valuable information
about generative processes. Networks account for much of social complexity. One of
the problems is to understand different properties of networks that determine their
emergence (see 3.1.2). Social network analysis (SNA) investigates systems of social
relations, from cognition to the global system. Computational social science allies of
SNA include agent-based modelling, complexity models, and visualization analytics.
Future research directions must include maturation of existing knowledge – both
deeper and more interdisciplinary – as well as new knowledge creation through computational
methods (visualization analytics). Viable social theories should be agentized,
analyzed, and tested with empirical data – from human cognition to international
relations, including fields beyond the five social sciences (linguistics, geography, organization
science, history, communication, law, and others). The complex triadic nexus
among social, artefactual, and natural systems requires computational investigation
within an overall science of complexity, drawing upon existing and new methods from
the social, behavioural, natural, mathematical, and computational sciences.
4 Challenges
Computational social science must be set to answer a number of fundamental scientific
questions, perceived from within the scientific community as pivotal to address and
help manage the BigProblems of society.
4.1 Understand levels and directions of interaction
Levels and directions of interaction are major sources of social complexity. Real-world
societies imply several levels of complexity, which are not reduced to the micro and the
macro levels but include intermediate levels (groups, tribes, networks, communities,
etc.).
Entities belonging to any of these levels interact with one another: individuals
interact with other individuals, groups may conflict with other groups, macro-level
entities need to coordinate with one another (one Ministry with another, the Parliament
with the Government, etc.). But they also interact with entities at any other
levels: group members interact with group artefacts, for example norms, customs,
and habits, and with macro-entities, like justice courts, fiscal agencies, the electoral
system, etc.
Social levels emerge from one another, and retroact on one another. We need to
understand how new levels occur, e.g. how groups and coalitions as well as social
institutions and other macro-entities are formed. But we also need to understand
how these retroact on the produced levels and modify them.4.1.1 A new view on emergence
Emergence is much studied, but this does not mean it is well understood. Insufficient
understanding of emergence phenomena depend on several factors: a) insufficient
analysis of the emergence process, b) inadequate models of the micro-level interacting
units from which emergence is supposed to proceed, c) unsatisfactory account of the
coupling of emergence and downward causation: many social phenomena and entities
emerge while at the same time retroact on the lower levels, and it is not possible
to account for one direction of the dynamics without considering the complementary
one. This is the case with both social artefacts, like institutions, and cultural artefacts,
like technologies.
Here we address the first factor, i.e., the necessity for a more detailed and explicit
analysis of the emergence process. Do we really need one? What are the problems
that emergence helps us address? In the next subsection, the second factor (groups
and networks) will be addressed (4.1.2). The third factor of inadequacy of existing
models of emergence will be treated as the Micro-Macro link problem (4.1.3).
What is meant by emergence, and why do we need such a notion? Let us start by a
definition of social emergent processes [64] as phenomena a) for which the conditions
for its occurence are well defined, b) non-deliberately but spontaneously, c) modifying
the entities involved, in particular, interfering with their fates but unlike learning,
without modifying the internal states of the producing entities, and d) unlike evolutionary
effects, non-transmissible. This definition prevents emergence from collapsing
on the notion of “yet unpredicted events” and allows us to tackle systematic but nondeliberate
processes of social influence that abound in social life and are responsible
for a number of critical social phenomena, from the self-fulfilling prophecy, to social
facilitation, social inertia, stalemates etc. A theoretical understanding of emergence
is particularly important as it often produces paradoxical phenomena where individual
intentions produce unexpected aggregate results with potentially disastrous
consequences, as in rational herding, free-rider behavior, or unintended ethnic segregation.
Typically one expects an emergent social phenomenon to be characterized
by an abrupt collective change that can be well described, in some parameter space,
either in the language of dynamical systems as a bifurcation, or, in the language of
statistical physics as a phase transition.
Emergence is usually seen as a bottom-up process but horizontal emergence occurs
as well. One example is the self-fulfilling prophecy, which is addressed by a great
deal of empirical social psychological studies. In general, a self-fulfilling prophecy
occurs when individuals gradually and non-deliberately tend to assume properties
and behaviours corresponding to the expectations of those with whom they interact.
One interesting question concerns the dual of the self-fulfilling prophecy: it is unclear
whether the mechanism of self-defeating prophecy exists. Finally, a question still open
is whether and under which conditions it is possible to prevent this effect from taking
place.
4.1.2 Networks and group formation
According to the results of the Harvard Symposium on hard social problems in 2010,
one of the top-ten problems is how to achieve good collective behaviour. Unfortunately,
a precondition for developing such a theory, namely a model of individual
behaviour is missing. Rationality theory is now deemed to be insufficient. Can we
model emergent collective behaviour without grounding it on a more plausible model
of individual behaviour than is provided by rationality theory?
Two factors prevent modelling the conditions that favour the emergence and prediction
of social networks (cf. back to 2.1). First, connections are not allowed to emerge from objective relationships, but are either given for granted or created from
agents’ decisions (an example of the latter is the emergence of role differentiation,
[65]). Second, nodes are not modelled as pre-existing entities (agents). Consequently,
it is not possible to predict when a set of entities will generate a network (an exchange
network, a cooperation network, a trust network, a dependence network, etc.). Within
the domain of multiagent systems, based on a more complex view of agency and richer
semantics of their relationships, special type of networks have been shown to emerge
from among heterogeneous agents, characterised by different mental states and action
capacities [66,67]. Interestingly, such networks have been shown to allow for different
types of positive behaviour – one-shot cooperation and exchange – to be predicted
[68]. A system for calculating emergent dependence networks found interesting applications
in organizational design and optimization.
One of the tasks of computational social science is to integrate different inputs from
adjacent fields in a general theory of social networks that accounts for the properties
of existing networks and enables to predict the emergence of new networks.
4.1.3 Multi-level interactions: The Micro-Macro link
The interconnection among different levels of social phenomena [69,70], cannot be
fully accounted for unless multidirectional processes are modelled, including the downward
process from higher-level properties to lower level entities, called Micro-Macro
link. More precisely, the Micro-Macro link (see for a recent collection, [71,72]) is the
loop process by which behaviour at the individual level generates higher-level structures
(bottom-up process), which feedback to the lower level (top-down), sometimes
reinforcing the producing behaviour either directly or indirectly [35].
The Micro-Macro link represents a challenge for our new field of science. How to
characterise the whole dynamics? What type of feedback loops does it include? When
and how does a given macro-effect retroact on the lower level entities, giving rise
to the so-called downward causation? Downward causation is a poorly investigated
process, which appears to play a decisive role in many social phenomena. While an
emergent effect is always implemented upon interacting micro-social entities, it is
not necessarily retroacting on them. When does this happen? When is the MicroMacro
circuit closed? Can we predict the occurrence of a micro-macro loop? Can
we forecast when a certain emergent effect is likely to be reproduced, and what are
the intermediate, or proximate, behavioural or mental causes of its reproduction?
Computational social science is expected to address these questions, not before having
developed an explicit theory of the Micro-Macro link.
4.1.4 Interaction with institutions and compliance
A serious social failure is the breakdown of institutional responsibility that might be
expected to follow from the P2P-driven revival of the Wisdom-of-Crowds culture. As
the production of knowledge becomes decentralised, no definite locus of responsibility
can be traced back and be accountable for the consequences of information spreading.
No matter how frequently Wikipedia is false, and even if it is less frequently so
than the Stanford Encyclopaedia of Philosophy, who might be blamed and asked to
respond for the false news published within it? Should we expect an increasing social
uncertainty to follow, especially as people feel unable to claim repair for the damages
suffered in consequence of wrong information spreading? In the last couple of decades,
we got ourselves busy with questions concerning the impact of ICT on lifestyles,
interaction patterns, and thought facilitation or impairment. Poor if any attention was paid instead to the way and the extent to which ICT affects the societal artefacts
that govern our social lives, and in particular social institutions, and the related
degree of social trust. Low compliance is known to be one of the main consequences
of the breakdown of institutional responsibility and social trust [73–75]. These issues
need to be addressed by Computational Social Science.
4.2 Modelling culture
The pioneering computational work by Robert Axelrod [76] addressed the problem
of cultural dynamics considering the following question: “if people tend to become
more alike in their beliefs, attitudes and behavior when they interact, why do not all
differences eventually disappear?” He proposed a simple model to explore competition
between globalization and the persistence of cultural diversity. Culture is defined
as a set of individual attributes subject to social influence. The model implements a
mechanism of homophilic interactions and illustrates how an interaction mechanism
of local convergence can generate global polarization (persistence of cultural diversity).
However, culturally polarized states have been shown to be unstable against
cultural drift in a fixed social network. Computational modeling has identified new
mechanisms and conditions that can stabilize the persistence of cultural diversity.
Cultural diversity can be understood as a consequence of co-evolving dynamics of
agents and network [77]. The social network evolves in tandem with the collective
action it makes possible – circumstance makes men as much as men make circumstance.
Cultural diversity is also stabilized when cultural influence is not just exerted
within interpersonal interactions, but when it is rooted in social pressures from local
majorities in actors networks [78]. An additional aspect is that, perhaps surprisingly,
a strong mass media message is known to lead to social polarization, while mass media
is efficient in producing cultural homogeneity in conditions of weak (and local)
broadcast of a message (the power of being subtle, [79]). On the other hand, social
interactions can lead to a cultural globalization in a direction different from the one
broadcasted by mass media, provided that there are long range links in the network
of interactions. Long range links make it possible for collective self-organization to defeat
external messages. An important challenge is the understanding of the effects of
the new ICT mechanisms of spreading and aggregating information, replacing traditional
mass media, on the polarization-globalization issue: The concept of local social
circle has been changed, and people are no longer passive receivers of information
but sources of information immediately available in a global scale, and simultaneously
active players in searching and selecting from a large and distributed number
of information sources.
While Axelrod’s model considers several cultural features at the same level, other
computational studies of cultural dynamics isolate a single cultural aspect such as
language [80]. An open question from the standpoint of computational social sciences
is the development of integrative models that incorporate different cultural features
with different dynamical processes at different scales. Hierarchical and multilevel cultural
models are needed to take into account the interdependence of cultural features
and the interconnection of cultural dynamics with other social processes.
4.3 Cross-methodological research
Well designed experiments constitute another key ingredient of the advancement of
science. In the case of the social sciences, they will prove invaluable when combined
with simulation work [81]. However, when thinking of large-scale simulations and using similarly large-scale data, computational social science is expected to face several
crucial challenges for new experimental work in integration with the modelling
process:
Design of experiments to test inferences from data. Careful analysis of data should
yield intuitions on human interactions and decision-making, but different sets of data
or even different analysis of the same data may lead to incompatible proposals. Experimental
work specifically designed to discriminate between alternatives should allow
choosing among them.
Design of experiments to test simulation predictions, both local and global. Another
use of experiments relates to the model or models themselves. These models
should lead to predictions both at the micro and at the macro level, possibly involving
the corresponding loop. Validation of the models requires more than comparison with
available data: Models should offer insights which are experimentally testable. Note
also that this is something like a second-order test of conclusions obtained on data.
Protocols for large-scale experiments: there is a need for virtual labs with repeatable
procedures and controlled environments. however, going beyond working with
dozens of subjects is by no means trivial. For instance, work on experimental game
theory by Grujic et al. [82], showed that handling 169 individuals simultaneously is
extremely difficult, and this is regarded to be close to what can be done. The design
of large-scale (matching that of the data as far as possible) laboratory experiments
aimed at identifying local and global effects presents a unique set of challenges, whose
resolution requires the development of a commonly agreed methodology and accepted
set of tools, possibly including a virtual laboratory and protocols as to how to handle
large numbers of volunteers interacting through ICT media (not to be confused with
experiments on virtual worlds, [83]).
A word is in order regarding the types of questions such experiments could address.
Aside from human behaviour in a fixed social context, in particular decision making
in contexts like trading, cooperating or coordinating, or the problems of information
aggregation, i.e., how individuals learn and adapt from their social circle, which are
of course interesting aspects, experiments should address the formation of the social
interaction framework (be it a network or otherwise). This poses additional problems
for the experimental design in so far as it may need to be prepared in such a way
that it can alter its own context. How to do this in a controllable manner is far from
clear, more so if one wants to make sure that the experiment addresses the question
one is interested in.
4.4 How much complexity of agents is needed to get a good picture?
The discussion so far points to a number of objectives that require higher-level complexity
than is allowed by ordinary agent based models of social processes.
First, more mental complexity must be modelled to understand what are the
specific mental properties allowing social complexity to be managed and simplified.
For example, what are the specific properties or mental attitudes required to cooperate
and coordinate with others and give help without aiming to obtain benefits in return?
What is needed for people to be willing to participate in social control, thus leading
to more distributed costs of norm enforcement?
Second, social intelligence makes society work better by creating social artefacts,
both material and immaterial (institutions). Of course, properties of social artefacts
are not necessarily linked to individual properties. Therefore the understanding of the
individual is not always needed to understand them. However, social artefacts share
properties of human producers and users (e.g., limited autonomy, see [84]). If we do not understand these properties at the level of human producers and users, we will
not be able to understand them at the level of social artefacts.
Third, the most intelligent use of social power is often not signalled, and not
easily reconstructed. Social intelligence is more than the capacity to represent and
understand the environment. It includes the capacity to represent others’ mental
representations [85], to reason and manipulate these representations and to actively
influence them. Indeed, social power ranges from the exercise of physical strength, to
more subtle and insidious manipulative actions in which humans attempt to modify
one another’s mental states in order to modify one another’s future behaviour. The
manipulative exercise of power consists of keeping one’s intentions hidden to the
target. Consequently, social power, at least its manipulative use, is not signalled.
To understand and predict these fairly complex mechanisms of power, a relatively
complex model of the mind is needed.
Fourth, and consequent, a number of important social phenomena cannot be understood
nor fully taken advantage of, without a more complex model of social intelligence
and social influence. One example is reputation [86–88]. Reputational systems
exist in natural societies since the dawn of mankind. They gave inspiration to a special
technology of reputation extensively exploited in commercial applications on the
Internet. Cross-methodological evidence [89] shows that the inadequate or unsatisfactory
results of the technology of reputation are due to an insufficient understanding
of how reputation systems work in natural societies. Analogous considerations apply
to the case of trust [90].
Fifth, policy modelling often necessitates to investigate the mental mechanisms
which are involved in the behaviours to be modified or strengthened. Consider the
role that overconfidence, according to some economists [91], has played in the last
financial crisis. How to modify such an attitude? Would the recommendation to not
trust banks too much do? And what about bad habits, like drinking, smoking, etc.?
Which policies are more likely to succeed in informing diabetics about the benefits
to quit unhealthy diets, or young people avoid unsafe or irresponsible behaviours?
4.5 Towards a new epistemological foundation: What kind of models
do we need?
4.5.1 Understanding and predicting
Models are often used to make predictions, a practice sometimes called forecasting.
In this case, models represent the properties of actual target systems so that we can
predict what these systems will do in the future. The models used for predictive
purposes often tell us something about mechanisms. They are output-oriented since,
for a given set of initial conditions, they should tell us how the state of the system
will evolve in time.
Models can also be used to explain the behaviour or properties of target systems.
Explanation is a highly controversial notion [92]. Consistent with the scientific literature
on modelling, we hold a view of explaining as “showing how things work” [93].
When we build models for explanatory purposes, we try to make adequate representations
of target systems, similar to the predictive case. However, because there
is a difference in what we use the model for, different properties enable models to
be adequate to their intended use. Unlike predictive uses, which primarily involve
optimizing the models to make their output as accurate and precise as we need it to
be, the explanatory use requires us to learn how the component parts of the target
system give rise to the behaviour of the whole.
There is an additional explanatory use of models, one that is even more remote
from the predictive case. Sometimes we want to understand how hypothetical systems work. There is really no analogue to this in the prediction case because in that context
we are interested in predicting the behaviour of actual target systems. Sometimes in
the course of our trying to explain actual target systems, we make comparisons to
hypothetical systems. Fisher famously said that if we want to explain the prevalence
of two sex organisms, we should start by explaining what the biological world would
be like if there were three sexes [94].
As computational social science is a model-based field of science, aiming to both
explain phenomena of interest and predict their evolution, we need to clearly understand
the respective implications for model building.
4.5.2 Qualitative analysis. Generative models
Generative models are a third way of doing science, an escape from the deductive/inductive
dichotomy. They allow qualitative analysis to be done in a rigorous
and controllable way. A typical computational generative approach, as was proposed
by Epstein, is agent-based simulation, requiring to:
“situate an initial population of autonomous heterogeneous agents (see also [50])
in a relevant special environment; allow them to interact according to simple local
rules, and thereby generate – or “grow” – the macroscopic regularity from the bottom
up” [95].
This passage raises some objections and requires that some caveats be taken into
account.
First, how to find out the simple local rules? How to avoid ad hoc and aribitrary
explanations? As already observed [96], one criterion has often been used, i.e., choose
the conditions that are sufficient to generate a given effect. However, this leads to
a great deal of alternative options, all of which are to some extent arbitrary. The
construction of plausible generative models is a challenge for the new computational
social science.
Second, the notion of rules needs clarification and revision. Possibly, rules should
be replaced by explicit and theory-founded agent models, whch include not only
decision-making mechanisms but also representations, attitudes, strategies, actions,
motivations, and the like.
4.5.3 Integrate heterogeneous models
The analysis and modelling of large-scale social systems should be supported on three
coordinated legs: collection, production and analysis of data; agent-based simulations;
analytical modelling.
Well-motivated models of agents in interactions may generate unexpected collective
effects that call for mathematical understanding. Conversely, collective effects
predicted from models simple enough to be mathematically analyzed call for numerical
tests on more complex (more realistic) multi-agents systems, or empirical validation
through surveys or experiments, whenever this is possible. Modelling requires
to find appropriate compromises between socio-physical realism, and mathematical
simplicity.
With this general goal of understanding and anticipating behaviours of complex
social systems, one needs to develop simplified models, on which the mathematical
analysis can be done. Such models should be able to reproduce the stylized facts
empirically observed. From the analysis of these models, new intuition can be gained,
and more complex models can be studied, both numerically and analytically (see
[97]). Scenarios can be explored and tested by the large-scale simulations, providing
results that can be used for decision making. In the case of collective phenomena (crowds, pedestrian and auto traffic, fashion,
financial or social crisis, opinion formation and epidemic propagation; see [81,98], the
goal of modelling, with analytical and numerical means, is precisely to understand
the global (‘macroscopic’) level from the characteristics of the constitutive elements
(the ‘microscopic’ level) and the social structures they belong to, and also to understand
how the collective level influences the individual behaviour. Such modelling can,
and should, be done with multiple approaches, integrating tools and concepts from
different disciplines: applied mathematics, statistical physics, computer science, theoretical
economics (some relevant keywords would be: partial differential equations,
optimal control theory, information theory, dynamical systems, statistical physics of
disordered systems, graph theory, game theory, numerical simulations, agent-based
systems), and this in close interaction and collaboration with social scientists.
At the interface between data analysis and modelling, the analysis of data may
allow us to reveal robust statistical features that are characteristic of the system under
study. A particular example is the empirical analysis of the network structure, in
different contexts (networks of co-authorships, inter-bank network, road networks...).
A stylized fact observed for a particular system (e.g. a small-world structure) becomes
a guide for the modeller – either this particular network structure is an input to
the model, or one of the goal of the model is to understand how such structure
emerges. Developing new data analysis tools, as well as specific data representation
and visualization tools, is thus an important component of the computational social
approach.
5 Expected impact
5.1 Impact on science
The impact of progress along the above lines on science cannot be overstated. To
begin with, sociology in particular and the social sciences in general would undergo
a dramatic paradigm shift, arising from the incorporation of the scientific method
of physical sciences. Thus, the combination of the computational approach with a
sensible use of experiment will bring the social sciences closer to establishing a wellground
link between theory and empirical facts and research. Such links should inform
all sciences in which human behaviour is the main object of research or interest, and
should solve incompatibilities such as economics relying on the rational actor picture
and sociology and social psychology outright rejecting it; on the other hand, the latter
rely much more on facts (identified from experiments, surveys, etc.) than traditional
economics, based on the strength of purely abstract analytical approaches (for much
more on this, see [99]). Computational social science would be a major factor toward
this paradigm change in the social sciences.
On the other hand, the impact of the research we have described would certainly
go beyond the social sciences, as it would provide new tools and methods that would
be applicable in any instances where BigData are a key ingredient. As we expect
that new analytical approaches will be developed along with algorithms and monitoring
procedures for massive data, the complex systems perspective we propose here,
in which entities and their interactions might be abstract and arise in any context,
would immediately lead to applications of our findings in fields ranging from physics
to ecology through systems biology. In fact, the complex systems approach that transpires
through the entire proposal is a crucial lever towards blurring the dividing
lines among disciplines and creating a truly interdisciplinary, non-compartmental
science [100].5.2 Impact on technology and competitiveness
Computational social science requires an enormous research effort on ICT, the pillar
on which the research directions summarized here rely. Collecting, transmitting,
analysing, simulating and incorporating data to simulations will require computational
and communication capabilities well beyond the current state of the art facilities.
While the EU research community on ICT is very active, a scientific program
like the one discussed in this paper would boost it to take over the US and Japan
advantage on this field, thus increasing the competitiveness of the EU as a result and
advancing technology on a world level.
5.3 Impact on society
The role of computational social science is a leading one in addressing the BigProblems
of society, avoiding crises and threats to its stability and healthy development.
Computational social science will provide many tools towards achieving this goal,
and will play a leading role in the successful execution of the project. The development
of Computational Social Science, as proposed in this manifesto, will make it possible to
model and simulate social processes on a global scale, allowing us to take full account
of the long distance interdependencies that characterise today’s heavily interconnected
world. The output of these simulations will be used to support policy makers in their
decision making, to enable them to efficiently and effectively identify optimal paths
for our society. Similarly, open access to these large scale simulations will support
individuals in their evaluation of different policy options in the light of their personal
needs and goals, greatly enhancing citizen participation in this decision process. These
developments together open the doors to a much safer, more sustainable and fairer
global society.
6 Conclusions
There is an increasing realization of the enormous potential of data-driven computational
social science [9]. The availability of unprecedented amounts of data about
human interactions in different social spheres or environments opens the possibility
of using those data to leverage knowledge about social behaviour beyond research on
the scale of tens of people. The data can be used to check and validate the results of
simulation models and socio-economic theories, but a further step in using them is to
take them into account already at the modelling stage.
At the same time, the relation between social simulation (e.g., agent based modelling)
and theoretical understanding is still problematic. Thus, simulations must be
accompanied by micro-macro-loop theories, i.e., theories of mechanisms at the individual
level that affect the global behavior, and theories of loop-closing downward
effects or second-order emergence.
In view of this, it is clear that naive or brute-force incorporation of large-scale
data into simulation models may not lead to the expected results in terms of achieving
relevant progress in social science. While it is apparent that the analysis of the
data will certainly contribute to the understanding of mechanisms, it is also clear that
further input will often be needed, in particular input obtained from experiments under
controlled parameters or situations. These will shed light on the decision-making
mechanisms that sometimes can be obscured among the midst of the data.
In conclusion, computational social science, as a rapidly developing and successful
field, needs to be aware of the necessity to develop its theoretical premises, and to test them. Much as physical theories and models are tested through incredibly large
experiments (such as the LHC at the CERN), progress in computational models of
social phenomena will only be possible by a sensible combination of data input, experimental
work, and theory devising. Computational social science requires brings
along challenging demands on the experimental side, in terms of design and procedures,
which can only be solved by working together with the computational science
community.
The publication of this work was partially supported by the European Union’s Seventh
Framework Programme (FP7/2007–2013) under grant agreement No. 284709, a
Coordination and Support Action in the Information and Communication Technologies activity
area (‘FuturICT’ FET Flagship Pilot Project). We are grateful to the anonymous
reviewers for the insightful comments.
Understanding the paradigm shift to computational social science in the presence of
big data
“To get value from big data, ‘quants’ or data scientists are becoming
analytic innovators who create tremendous business value within
an organization, quickly exploring and uncovering game-changing
insights from vast volumes of data, as opposed to merely accessing
transactional data for operational reporting.”
[Randy Lee, Vice President, Aster Center for Data Innovation,
Teradata [81]]
“The best way to engage in … data-driven marketing is to gather
more and more specific information about customer preferences,
run experiments and analyses on the new data, and determine ways
of appealing to [casino game] players' interests. We realized that the
information in our database, couple with decision science tools that
enabled us to predict individual customer's theoretical value to us,would allow us to create marketing interventions that profitably
addressed players' unique preferences.”
[Gary Loveman, CEO and President of Caesar's Entertainment [70]]
“Each methodology has its strengths and weaknesses. Each approach
to data has its strengths and weaknesses. Each theoretical
apparatus has its place in scholarship. And one of the biggest challenges
in doing interdisciplinary work is being [able] to account for
these differences, to know what approach works best for what question,
to know what theories speak to what data and can be used in
which ways.”
[Danah Boyd, Senior Researcher, Microsoft; Research Assistant Professor,
New York University [16]]
1. Introduction
With the rapid advances in technology, business interactions involving
consumers and suppliers now generate vast amounts of information,
which make it much easier to implement the kinds of data analytics that
Gary Loveman, current CEO of Caesar's Entertainment, discussed in a
2003 Harvard Business Review article on data mining [70]. Today, this is
referred to as the big data revolution in the popular press, and viewed
as creating challenges and opportunities for business leaders and interdisciplinary
researchers. The world's volume of data doubles every eighteen
months, for example, and enterprise data are predicted to increase
by about 650% over the next few years [45,54]. Today, most firms have
more data than they can handle, and managers recognize the potential
for value, but the promise of big data still has not been realized, according
to the leading academic [35,78] and business media sources [38,79].
3 The
potential arises from the use of data to support the way organizations operate
and serve their stakeholders. A recent article in MIT Sloan Management
Review [62] described the use of big data by an Atlanta-based public
school, for example. High school graduation rates increased due to
better-informed policy decisions that were based on the application of
advanced analytics capabilities to student performance data. Likewise,
organizations now are embedding analytics in their operations to support
data-intensive strategies.
1.1. The emergence of big data
A recent McKinsey report has referred to big data as “data sets whose
size is beyond the ability of typical database software tools to capture,
store, manage, and analyze” [71].
4 Such data come from everywhere:
pictures and videos, online purchase records, and geolocation information
from mobile phones. Big data are not just about sheer volume in
terabytes though. Other important aspects have been emphasized in addition
to volume, including variety, velocity and value [76]. Big data may
be unstructured too: examples are text with social sentiments, audio
and video, click streams, and website log files. Such data may flow in
real-time streams for analysis, which can enable a firm to maximize
business value by supporting business decisions in near to real-time.
This new trend in decision support is evocative of what we saw in the
1990s with the emergence of data mining, and the new emphasis on
data with a large number of dimensions and much higher complexity
(e.g., spatial, multimedia, XML and Internet data). Most of the data
sets were “one off” opportunities, rather than data that had become
available due to systemic and technological advances.
Considerable challenges are present in the quest to capture the full
potential of big data. The shortage of analytics and managerial talent is
a significant and pressing problem, for example. CIO Magazine [72]
and the Corporate Executive Board [79] have reported that it is difficult
for firms to find the right people. The U.S. alone is reported to face a
shortage of 140,000 to 190,000 people with deep analytical skills, as well as 1.5 million managers and analysts to make effective decisions
[71]. (See Fig. 1.)
1.2. Toward computational social science
New perspectives in social science are now tracking the developments
in big data. For example, computational organization science has
broadened researchers' perspectives on social, organizational and policy
systems, by adopting computational models that combine social science,
computer science, and network science [22]. Other related
developments have occurred, including the emergence of computational
social science and e-social science [37,63]. Computational social science
involves interdisciplinary fields that leverage capabilities to
collect and analyze data with an unprecedented breadth, depth, and
scale. Computational modeling approaches now can predict the behavior
of sociotechnical systems, such as human interactions and mobility,
that were previously not studied with one-time snapshots of data for
very many people [83]. We see a paradigm shift in scientific research
methods — and one that prompts new directions for research. A useful
perspective in this context is attributable to Runkel and McGrath [75],
who characterized research methodologies based on three goals: generality,
control and realism. They distinguished between their obtrusiveness
and unobtrusiveness for the subjects of research.
With emerging collection techniques for big data sets, there seem
to be fundamental changes that are occurring related to research
methods, and the ways they can be applied too [58]. In e-business,
for example, the contexts include social networks, blogs, mobile telephony,
and digital entertainment. The new approaches we see are
based on more advantageous costs of data collection, and the new
capabilities that researchers have to create research designs that
were hard to implement before. The research contexts include
human and managerial decision-making, consumer behavior, operational
processes, and market interactions. The result is a change in
our ability to leverage research methodology to achieve control and
precision in measurement, while maintaining realism in application
and generality in theory development.
We will discuss the causes of the paradigm shift, and explore what it
means for decision support and IS research, and more broadly, for the
social sciences. How can we take advantage of big data in our research?
What new perspectives are needed? What will the new research practices
look like? What kinds of scientific insights and business value
can they deliver in comparison to past research? And what research directions
are likely to be especially beneficial for the production of new
knowledge?
Section 2 reviews traditional methods for research and discusses the
key factors that are creating the basis for a paradigm shift. Section 3 describes
the new paradigm in the era of big data, and how it relates to decision
support, IS and social science research. Section 4 assesses how the
research has been changing, through the use of a set of specific comparisons
between research that was conducted before and after the emergence
of new methods associated with big data. Section 5 offers some
new research directions, and section 6 concludes.
2. How are big data supporting a research paradigm shift?
The move to computational social science in the presence of big data
involves a Kuhnian scientific paradigm shift [60]. We will provide a background
on the traditions of research inquiry, and then examine the driving
forces for the paradigm shift, and why access to large stores of data is
speeding the process.
2.1. Traditions of research inquiry
Churchman [27] characterized research with a set of different inquiring
systems. They involve methods, procedures and techniques to
describe and explain behavior, test hypotheses, assess causality, and establish new truths. Runkel and McGrath [75], in contrast, include
theory-building, sample surveys, judgment tasks, lab experiments,
field experiments and studies, and simulations.5 (See Fig. 2.)
2.1.1. Runkel and McGrath's three-horned dilemma has diminished in
intensity in the presence of big data
Today's methodologies are different in terms of how data are collected,
the intrusiveness of the procedures, and the degree to which
each methodology applies to situation-specific versus universallyobservable
behavior. The strengths they provide include achieving
generality, control and realism in the research designs. Runkel and
McGrath [75] argued that these strengths cannot be maximized simultaneously
with any single research methodology though. Instead,
choosing one method takes advantage of a key strength,
while the leverage the others offer may be lost. This is a classic
three-horned dilemma for research methodology. For example, consider
the control of a lab experiment, versus the realism in a field
study or a computer simulation of a process, versus formal theorybuilding
uncontaminated by the complexities of the real world.
The cost and efficacy of using traditional research methodologies
have been affected by the emergence of the Internet. New methods,
such as online experiments from economics, online judgment tasks
from cognitive science, online surveys from sociology, the capture of
full repositories of business transaction data in IS, and data stream filters
from computer science are being used today to answer research questions
that were previously impractical or impossible to answer, due to
the inaccessibility of the data. In addition, since it is relatively easier to
establish subject pools to which multiple control and stimulus conditions
can be applied in online social networks or in mobile phone systems,
research designs can be implemented that allow researchers to
find the data that meet a set of pre-defined experimental conditions –
the “needles in the data haystack” – without ever taking subjects into
a laboratory setting to conduct an experiment [21]. The new paradigm seems to provide a way of addressing, if not entirely resolving Runkel
and McGrath's dilemma.
2.2. The driving forces of change in the paradigm shift
The paradigm shift for computational social science overall, and for
decision support and IS research, is based on several distinct forces.
They include technological changes, the convergence of disciplines,
and the availability of new tools and solution approaches for data analytics.
They cover new business practices driven by senior management
perspectives on the value of data analytics in decision support, changes
in market competition, and what's required for firms to differentiate
themselves. (See Fig. 3.)
2.2.1. Technological change plays a role
IT has improved the efficiency and effectiveness of organizations
since the original implementations of decision support systems (DSS)
in the mid-1960s. Since then, new technologies have been developed,
and new categories of information systems (IS) have emerged, including
data warehouses, enterprise resourcing planning (ERP), and customer
relationship management (CRM) systems. The technologies
have evolved and become commoditized and less expensive, and have
delivered continuous increases in storage and computing power. Moreover,
the Internet revolution has had a drastic impact on business and
society, including the rise of social communication through e-mail, instant
messaging, voice over Internet protocol (VoIP) services, video conferencing,
and other means.
Blogs and wikis have changed the patterns of communication between
organizations and the communities that surround them too.
They have affected how individuals communicate— including in their
homes, in their companies at work, and on vacation. User-generated
content has become a source of creative interpersonal interactions for
people and valuable economic exchange. Analytics can be used to surface
useful information from the raw data that these different settings
and technologies generate, and support decision-making like never before.
A representative example of the new architectures and technologies,
and various tools required for creating business intelligence with
big data is Hadoop. It is the most popular open source software framework
that supports the use of very large data sets spread across distributed
applications in larger systems [71]. Another example is NoSQL
databases, which are used to store very large sets of data in a way that
supports flexible manipulation of unstructured data. Large data stores
often need massively parallel processing so queries can be made effectively across multiple machines simultaneously for faster processing
[42]. They also require in-memory computational capabilities at
the server level, so it's possible to do real-time analysis of streaming
data. Major software vendors are already providing technical solutions,
and advances in large-scale data analytics are tied to advances in the
technologies, and will support innovations in managerial and organizational
decision-making too.
2.2.2. Interdisciplinary convergence intensifies the changes
Yoffie [87] observed another aspect of the recent technological revolution:
previously distinct technologies have converged to unify a variety
of functions that are demanded by people in business and society.
Yoffie called this digital convergence. Examples include wireless phones
and cameras, TVs and hand-held devices, and database and data analytics
tools. Accompanying these changes with technology, there is also a
convergence in the study of leading research problems through interdisciplinary
scientific interactions.
In some of the areas where interdisciplinary convergence is occurring,
there seems to be a special need for new ways to deal with the emergence
of big data. This motivates, for example, convergence among the
disciplines of marketing, economics and computer science, where
fresh insights on consumer behavior and product design provide a
new basis for competitive advantage. Our key observation is this: the
large data sets that describe the inner workings of complex social systems
that researchers would like to investigate cannot be adequately
understood from a single disciplinary perspective. For example, a huge
amount of data is available for the study of group and individual behavior
in sociology, but it cannot be analyzed without data mining and machine
learning methods from computer science. The same can be said of
the centrality of machine learning, econometrics and mechanism design
for advances in keyword search advertising, where creating deep insights
on ad slot valuation and positioning, and the effective prediction
of consumer responses are central to business search advertising success.6 Interdisciplinary convergence offers diverse motivation for
new research questions and theoretical perspectives relevant in all of
the phases of the research process – from problem and theory formulation
– to research model and methods selection – all the way to data
analysis and interpretation of the results.
2.2.3. Competitive strategy and the new data analytics support the change
Data analytics are a matter of competitive strategy for many businesses
and organizations today [30]. More sophisticated analytics solutions
are being developed to solve complex problems based on access to
and capabilities for processing large amounts of data [76].
7 In particular,
improvements in exploratory and predictive analytics capabilities have
made it possible for research to be more data-driven, but in a way that is
highly beneficial and deeply insightful. They no longer carry the “baggage
of the past” — that such approaches are somehow anathema to good practice in social science. The current generation of data analytics
methods provides capabilities for interpreting various types of streaming
data, such as video and GPS data. They also permit managers to
embed analytical capabilities in business processes, which can support
monitoring and analysis of business processes and activities in near
real-time. Another complementary method is data visualization,
which offers managers a chance to make sense of large data sets. They
can discover patterns and visual associations that stand out and provide
information for decision support [85].
2.2.4. New business practices and organizational environments speed the
process
Another driver affecting the rapidity of adoption of business data analytics
in organizations is the creation of new business practices, with
the idea that “technology is the service” [77]. As a result, more firms
are redeveloping their organizational cultures to support new approaches
to understanding their businesses through large data sets.
This requires a firm's senior managers, employees, and its strategic partners
to work together to understand the potential of business data analytics.
This way, they will increase the likelihood of reaping the
advantage that many authors, pundits and observers have suggested
await the market leaders.
3. The new paradigm: computational social science with big data
We next discuss the details of the paradigm shift in research, as a byproduct
of the identified forces.
3.1. Definitions of key terms to describe the paradigm shift
We begin with a series of definitions that support the development
of this discussion. (See Table 1.)
Today, we can offer a more in-depth description of the world, as it
might be viewed from the perspective of computational social science
with the capacity to collect large amounts of data to describe the variety
of activities and events that are occurring within it. The real world is a
complex adaptive system with a collection of entities and their interactions
[23,80]. In such a system, the entities are continuously adapting
to an ever-changing environment.8Within this dynamic system, a series
of events that arise based on the actions and reactions of the entities
occur in different contextual, spatial and temporal settings. These typically
can be observed and quantified, though some aspects may be unobservable due to the limitations of the data capture methods. The
events generate useful data that can be collected on the behavior of
and interactions between entities that an analyst wishes to study. Figuring
out what drives the events that are observed, and developing analytical
approaches to reliably predict them are where new organizational
competencies for decision-making and competitive advantage begin.9
3.2. The macro–meso–micro data spectrum
Thinking about data and research settings this way supports a fuller
understanding of the real world as an action space for much more wellinformed
decision-makers to create value. It prompts the identification
of empirical regularities and the patterns of people in the midst of characteristic
behavior that presage decisions that occur on the value
threshold of their willingness to act or transact. The main difficulty
that many researchers have faced is the shortage of data describing
the real world they wish to study. They have also had to deal with limitations
with the application of the most effective research methods that
aid in the discovery of new knowledge. Data for social science research
now are more plentiful though, so the methods to use for key problems
will offer dramatically more leverage for useful insights.
Advanced technologies now support researchers' efforts to acquire
data to identify interesting empirical regularities in complex realworld
systems to support high value decision support. The data spectrum
available in the new decision support spans the largest scale to the
smallest possible scale. (See Table 2.) One end of the data spectrum –
the most aggregated scale or the macro level – involves tracking patterns
of interactions between nations or economies for different sectors of
industry over a period of time, or tracking inter-regional immigration.
Meso-level societal-scale data can be used to track the behavior of individuals
in a population of a place, such as the mobile communication
records for a single service provider's customers in a city or society.10
Another example of meso-level data that can be collected is from tracking
the communications and behaviors of all users of a social network.
At the other end of the data spectrum, managers can craft new decision
support capabilities from micro-level data related to a set of individuals'
interactions with their relatives in shared phone plans, or the
patterns of textual expression in blog posts over a period of time.
Even more detailed nano-level or atomic-level data might arise in the assessment of human emotional reactions based on physiological proxies
when affective computing assessments are used.11 These are essentially
indicative of the lowest level, most disaggregated data in which
measurement is relevant for a given analysis setting. In neuroscience
and physioanalytics, for example, data collection approaches include
magnetic resonance imaging and brain scans for human decisionmaking
under uncertainty [34], and the measurement of affective responses
in new survey research designs [84]. The data sets are likely
to be of similar orders of magnitude, even though they exhibit different
degrees of granularity related to phenomena that researchers are
studying.
We also consider attribute relevance and attribute completeness to describe
the entities in a research setting. The typical attitudes that we see
expressed among managers and researchers are that “more attributes
are better,” but also that “capturing much more information becomes
prohibitively costly.” These attitudes may or may not be tied in with
business value, but current data acquisition methods will make the
costs of not getting this right much less expensive. With greater access
to data and the attributes of the relevant entities, researchers will be
able to leverage data stratification cheaply for multidimensional analysis,
beyond the standard data hyper-cubes in online analytic processing
(OLAP) in data warehousing.
Another characteristic of data for computational social science research
is whether it comes in stocks or flows. A data stock usually can
be acquired as a data set from an existing database, and it may be updated
over different periods of time. Quarterly corporate earnings performance,
or retirement investment portfolio values changes year to year are good
examples. When the updating is continuous and occurs in real-time or
near to real-time it is a data flow. Clickstreams from a two-way cable TV
set-top box that tracks viewer use of the remote control handset, and sequences
of advertising views when an Internet user is conducting a
search are examples. Choosing which approach – data stocks or data
flows – for acquiring data gives researchers the agility to study complex real-world systems in thoughtful ways that can be tailored more carefully
to match the critical desiderata for effective decision-making.
3.3. The interdisciplinary research approach
When conducting empirical research with data available now from
online auctions, social network communications, mobile phone networks,
gaming platforms, search engines, and blogs, researchers need
to be more open to exploration from the viewpoints of multiple disciplines.
Extracting meaningful information from a large data set may require
methodological and interdisciplinary flexibility to create business
intelligence for decision support from it. Someone who has an expertise
in sociology and psychology may be unaware of the insight-surfacing opportunities
that exist to work with the same data using data mining and
machine learning techniques, for example. Different knowledge from
statistics and economics will be useful too. The data may no longer exhibit
the small sample properties that many people in the social sciences
have been trained to work with. Also user behavior in social networks
may be correlated, which requires new methods to make sense of the
causal relationships that are present in the observed stimulus–response
loops. Thus, it is appropriate to embrace methodology-focused collaboration
to develop an interdisciplinary, multi-perspective approach that
will yield the most interesting results for the new decision support and
e-commerce issues, and other social science problems of interest.12
Another consideration involves the kinds of theoretical explanations
of interest. Extensive data sets that cover consumer demographics,
product and service usage, and managerial decisions to redesign product
bundles [8,9,41], for example, are open to theoretical and modeling
assessment from the perspective of multiple disciplines. The impetus
might be to understand the behavioral economics of consumer reactions
to service prices or market share competition among firms. Or it may come from trying to discover the efficacy of new product designs
intended to capture the new revenues from customers without promoting
cross-product revenue cannibalization. To bring these diverse concerns
together requires theory-focused collaboration involving expertise
in marketing, psychology, design and economics too.13
A third reason is the problems themselves and the measurement innovation
collaborations that they motivate. The study of organizational
and human communications and interactions in social networks is a
case in point — apart from any application area that the communications
relate to (seller reputations, service quality, etc.). Relevant theory
comes from sociology, communications, cognitive science and social
psychology. But how the connections among participants are formed
is just as much a matter of network economics, graph-theoretic operations
research, computer science and accounting. So much of what is
happening can be described by making relevant measurements in settings
that can be abstracted as graphs, or understood through new metrics
dashboards.14
But what measurements? And with what basis in theory? In social
network-related research, we can distinguish among many different
things that need to be measured so they are understood better. These
include the strength of human reactions and communications, the position
and distances between the entities in a network, the costs of communicating,
changes in social sentiments, changing interactions among
corporations and people in the marketplace, and so on. For every different
measurement problem, there may be different disciplines of measurement
knowledge to leverage. So there needs to be an effort to
bring together people whose capabilities complement one another,
and can bring new measurement approaches to support the creation
of surprising discoveries and deep new insights.
3.4. Research methodology
We next will discuss how the new paradigm has been affecting the
three-horned dilemma of research methods. We also will link our experience
to the current era with what occurred in the 1980s and 1990s, as
computers and data analytics were beginning to mature. The idea is that
each method has a different strength in terms of achieving the three research
goals of generality, control, and realism. So it isn't possible in
principle to maximize the power of all three simultaneously: tradeoffs
are involved.
But to what extent is this still true? The new approaches suggest we
can now study similar phenomena to what was studied in the past, and
new phenomena that have not been fully described in published research,
with much greater control than before in more realistic settings.
We also can do this without creating any disturbances in the processes
under observation. Interdisciplinary big data-focused research methods
are making it possible to address all of the dimensions in a simultaneously
more effective way.3.4.1. Big data blur the distinction between field experiments and laboratory
experiments
With the emerging capabilities to collect data sets from a variety of
real world contexts, the field itself has become the researcher's new behavioral
research lab. We no longer have to architect artificial settings in
the lab to affect control or forgo realism by mimicking the real world in
a synthesized setting. Instead, we can capture information that represents
the fundamental elements of human actions and interactions –
clickstreams, tweets, user opinions, auction bids, consumer choices,
and social network exchanges – directly from the real world as digital
traces of human behavior. By using appropriate instrumentation for
data collection, we can take advantage of situations that are naturally
experimental or create randomized treatments to distill the conditions
under which certain behaviors and unique outcomes are observed.
3.4.2. Societal-scale data support greater realism in survey and experimental
research
Runkel and McGrath [75] suggested that judgment tasks and sample
surveys support higher generalizability by decontextualizing real-world
settings. This goal can be accomplished through the use of big data. Survey
data often involve subjective responses, and they will complement
what we can learn from objective factual data that are available in empirical
research designs. What we cannot learn from objective data we
may be able to gain insights on with participants' subjective responses.
Further, we are no longer restricted by the number of subjects in surveys
and experiments due to the widespread use of online social
media, mobile phones, e-markets, and other new contexts. We can include
many more participants and population-level data collection,
which support greater generality through decontextualization.
3.4.3. The new methods affect how we can work best with theory
Social science research has emphasized development of theoretical
models and testable hypotheses. Research with data at terabyte or
petabyte scale challenges how we work with assumptions and theories.
In a 2008 article in Wired magazine, Anderson [1] boldly suggested that
big data might lead to the “death of theory.” Although his statement is
clearly hyperbole and begs a reality check, consider this: Google has
mostly depended only on numbers and applied mathematics in
matching ads to user search contents. Its Adwords has supported a remarkable
level of profitability that doesn't require a full understanding
of the underlying theory.
Anderson's argument seems to be that we no longer have to build
theoretical models, since no semantic relationships or causal analyses
are required any longer. Instead, research methodologies in the new
paradigm can put more emphasis on better data and better analytical
tools, rather than better and more elaborate theories. We think this is
similar to technical analysis and fundamental analysis in the financial
market context. The former looks for patterns to establish price trends
to predict valuation; the latter seeks underlying structure and causes
to provide explanations for why some level of valuation is appropriate.
They work hand in hand to help financial market investors gain more
knowledge of their world.
Theory, in our view, still ought to remain central in the development
of even more robust and interesting research insights. One possible way
to create theory more effectively is by iterating between the data discovery
and theory-building approaches. We think of this as a sort of
grounded theory development approach, only it is supported by big
data, and it involves the use of predictive analytics [74].
3.4.4. Greater control of research designs is no longer intrusive
Traditional lab experiments attempt to implement research designs
that can eliminate noise and establish causality to achieve for more
meaningful results. In many cases, the experiments are designed to
have subjects choose one of two possible alternates, or to ask them to
provide information on their preferences. This intrusiveness can lead
to lower precision in a controlled experiment, as exemplified by the effect of forced choices in most academic and industry studies of consumer
preferences and decision-making [33]. The computational social
science paradigm with big data can diminish the intrusiveness by using
a subset of the data that are available. For example, researchers are now
able to understand subjects' preferences from the digital traces of their
behavior without directly asking them questions. Thus, an important
consideration in research design is choosing which data to use in view
of changing contextual conditions. The new approaches support greater
control for the experimenter and greater precision in data collection,
and thus will be able to produce much richer and valuable capabilities
for decision support.
The reader should not falsely conclude, however, that big data are a
panacea for resolving all aspects of the three-horned dilemma. There is a
lot of noise in real-world environments that may make it difficult to
work with very large data sets. Researchers often point out that being
successful involves figuring out some “tricks” to work with big data
also. We also are not indemnified against modeling and research design
errors. These things require more attention, and we will discuss them
more fully later in this article.
3.5. Strategies for observation
A unique aspect of the new paradigm is that researchers now have
more control over the timing of observations they can leverage in social
science. Traditionally, one round of a survey might cost between US
$5000 and US$25,000 to reach a subject pool of sufficient size for a
given research design. Much traditional research in social science has
been cross-sectional in its coverage of a unit of analysis at a single
point in time, single units of analysis with a sequence of observations
over time, and many targeted subjects or entities at a given level of analysis
in longitudinal observational designs.
3.5.1. More frequent data collection is possible
The real world often creates situation in which the researcher is able
to take advantage of changing conditions: the acceleration of online
game competition, electronic market demand changes, new service delivery
mechanisms in mobile telephony, and so on. As a result, it may be
appropriate to collect data frequently, including following up with online
survey response requests after every customer transaction is made
with a firm.
Empirical studies for which it was only possible to capture data on a
one-time basis at a high cost can now be redesigned so it's possible to
harvest the same kinds of data repeatedly – and even quite frequently –
with greater fidelity and richness at a fraction of the prior cost. This
opens up the study of a range of geospatial and spatio-temporal contexts
that involve the spatial movement of consumers in stores, medical
patients in their homes and workplaces, delivery vehicles and public
transit buses, and so on.
3.5.2. Appropriate data granularity must be defined by the research
Depending on the questions, a macro-, meso- or micro-level of data
granularity may be appropriate. In most cases, the lowest level – microlevel
or atomic-level data – may be desired to create the highest resolution
for understanding unique contextual behavior in a research setting.
This kind of data may represent a behavioral time-series of human actions,
a more precise way of describing their digital traces. In Internet
surfing, for example, clickstreams provide evidence for what a person
is doing online; and in TV viewing, detailed viewership records of specific
channels with timestamps can be collected. Micro-data can be aggregated,
and this will support the study of higher-level issues too.
3.5.3. Iterative research designs create new power
Research designs include one-time to daily or even hour-by-hour experiments
in environments where the patterns of human micro-behavior
are relevant. This requires a pre-specified experimental approach in a
natural context for the study of the phenomenon of interest — the technological, individual, organizational, network, systemic or market
context. A researcher can perform iterative collection of data in chosen
time intervals, and then apply different metrics over time. The process
can be further refined in fresh ways to take advantage of the changing
empirical regularities or trends that are observed in the system that has
produced the data. It also creates the opportunity to conduct different
kinds of sensitivity analysis through sequential variation in controlled experiments,
as well as repeating, cyclical and even fractal patterns that are
not easily discerned in other ways.
Overall, these strategies for observation support pre-theoretical
analysis and explanation of the empirical regularities that arise for different
phenomena in different settings. These span theory and modelbased
hypothesis testing in experiments, to secondary data via empirical
designs, to the use of online surveys, simulations and assessments
embedded in real-world product and service settings.
4. A comparison of examples of traditional and new
paradigm research
A telltale indicator of the changes that are occurring is when we can
identify research that introduces fresh research questions associated
with longstanding problems and issues that can be studied in ways
that were not possible before, and with new depth and insight from
the findings. We next explore three representative areas of research
that now involve the use of big data and analytics for business, consumer
and social insights: Internet-based selling and pricing; social media
and social networks; and electronic market and auction mechanisms.
We will make comparisons between two representative studies and
some related works for each. For the representative studies, one uses
more traditional research approaches and does not use big datadriven
designs versus another that reflects more current research that
fits into the computational social science paradigm. In addition to considering
the contracts between the business problems and research
questions asked, we also consider the differences with the data and research
methods, and the new kinds of findings that result. (See Table 3.)
4.1. Internet-based selling and pricing
We have seen a lot of interest expressed by IS researchers and their interdisciplinary
collaborators in the area of Internet-based selling and pricing
[66]. For example, research on the airline industry [24,25] has evolved
because more detailed data at various levels of analysis have been able to
be obtained through the Internet, supplementing traditional sources. One
such source is the U.S. Department of Transportation's OD1A 10% sample
of all airline tickets written for all of the origin–destination city-pairs in
the United States. To illustrate the contrast between more traditional
but still sophisticated research approaches and current work, we consider
Duliba et al. [36], who used an extensive but not historically large data set
before web-based online airline booking systems became available. We
compare it with the work of Granados et al. [48], who used airline ticket
booking micro-data for the online and offline channels.
The scope of research problems that can be addressed in the presence
of newly available big data has expanded to include issues and
questions that were not able to be studied with data available in the
1980s and 1990s. Of interest then, for example, was to estimate the
business value benefits of computerized reservation systems (CRS)
and their effects on air carrier market shares at the city-pair level and
revenue and load factor performance at the aggregate national level.
More recently, the newly-available data have permitted analysis of
much more detailed price elasticity dynamics across the online and
offline channels, blending the multiple market levels of analysis (citypair
and regional comparisons), and the impacts of different product attributes,
especially price premium sensitivity for service upgrades
[24,28,49,50]. In addition to the OD1A Department of Transportation
data used by Duliba et al. [36] in the earlier research, the authors were
able to gain access to proprietary data on the locations and density of CRS terminals in travel agencies in city-pairs. In contrast, Granados et al.
[48] gained direct access to an airline industry global distribution system
(GDS) that brought together information on ticket bookings from
multiple airlines. This allowed them to collect millions of data points
from the online and offline channels.
Another contrast between the studies involved the data analysis
methods used. Duliba et al. [36] applied a multinomial logit market
share model for city-pair analysis and an industry production model
for the national level analysis. They showed that the presence of agency
CRS terminals was associated with greater airline city-pair market share
and also with higher levels of aggregate airline performance. The authors
used relatively indirect measures to assess the impact of IT investments
on the airlines' corporate performance and produce meaningful results.
Today, it is possible to employ better proxy variables and more direct
measures of the effects that are central to the research inquiry through
the use of big GDS data. Granados et al. [49–51] used sales data to assess
the effects on price elasticity of demand in the presence of digital intermediaries
and intensifying city-pair route competition, and how the effects
varied across the online versus offline markets. The authors used
a natural experiment in which it was possible to do matched assessments
of differently intermediated competitive settings. To deal with
endogeneity issues arising from city-pair route price-setting and the associated
service production costs that are private for most competitive
firms, the authors were able to construct instrumental variables in
their econometric analysis from publicly-available data sources.
The argument that we made earlier regarding the impacts of the
new paradigm for computational social science research is borne out
in the airline industry price elasticity of demand research: the strength
of Runkel and McGrath's [75] three-horned dilemma seems to have diminished.
Our capability to capture so much data with so little intrusion
on operations has enhanced the efficacy of the research methods that
can be used to come closer to the truth without the traditional burden
of the trade-offs in power.
4.2. Social media and social networks
The burgeoning research on social media and social network during
the past five years reflects some of the most exciting new impacts of IT
on relationships among people in society, as well as significant innovations
for how new methodologies and interdisciplinary perspectives can
be used [4,31,32,44,46,47]. In the time before the emergence of social
networking services and web-based interactions became popular, it
was not easy to directly measure the strength of the relationship between
two individuals. Yet a related research question that has been
of interest to marketers and sociologists over the years is: “How do
strong ties or homophily (similarities between people) impact wordof-mouth
processes?” A traditional approach to answering this question,
demonstrated in the research of Brown and Reingen [17], is to interview
individuals in a network. In contrast, as shown by Aral and
Walker [3], observation of the behavior of individuals in an online social
network such as Facebook now allows the above question to be answered
without directly asking individuals. New data analytics methods
also have created the capability to separate correlation from causation
to measure peer influence in social networks much better.
While Brown and Reingen [17] performed hypothesis tests using twophase
methods involving cross-checks on their interview results for 118
participants in a social network, Aral and Walker [3] used a randomized
field experiment with 9167 experimental users and 1.4 million friends
on Facebook.15 They also assessed social network effects when viral elements were embedded into product marketing campaigns on the Internet,
and found that passive broadcast viral features caused a greater increase
in peer influence and social contagion than active personalized
viral features.
Again, we can see the changes in the research related to word-ofmouth
effects made possible by the emergence of social networking
systems. We now have the available means for direct observations of
how users behave in social networks, the capacity to attack new research
questions with new interdisciplinary research designs and approaches,
such as graph-theoretic models and the analysis of events,
to understand hidden user behavior [13,88,89].
4.3. Electronic market and auction mechanisms
Research on markets and auctions has received considerable attention
from researchers over the years. Economists have modeled and estimated
consumer demand and surplus in traditional markets with mathematical
models and econometrics analysis. New kinds of data on the behavior of
e-market participants are now more visible on the Internet, and able to
be collected through the use of screen-scraping and agent-based data collection
approaches [10,11,56,57], which has opened up many new opportunities
for the study of electronic markets and auctions mechanisms.
Sites such as uBid and eBay have been the most popular and widely
used, and consequently the most studied with computational social science
methods.
An example of research that estimated the level of consumer surplus
in traditional markets is by Hausman [52], who applied quasi-indirect
utility and quasi-expenditure functions, and used market demand
data to parameterize the unobserved compensated demand curve. In
the more current eBay context, Bapna et al. [12] asked a different, but
related question: How much consumer surplus can eBay auction participants
extract per transaction and in aggregate? They also assessed the
extent to which the estimated auction surplus might be skewed, due to
issues with the data and the assumptions applied to it. Their research
involved 4,514 eBay auctions from 2004, and used experimental controls
for their data selection. Their analysis suggested a conservative estimate
of average surplus per auction was on the order of US$4, and
more than US$7 billion in aggregate consumer surplus. They also were
able to check the robustness of the results against 2005 data, an extraordinary
step made possible by the changed costs of data collection. This
shows how big data research design supports more refined results
and insights.
Similar to the other research examples we have discussed, research
on e-markets and online auctions now can take advantage of Internetbased
and other sources for large data sets. From such sources, researchers
can bring together representative sets of observations to identify
and probe different kinds of “needle-in-the-haystack” conditions
that can be analyzed to produce significant statistical results and support
a more complete understanding with more meaningful insights
than were possible before.
5. Research guidelines and practical considerations
We next discuss several new directions for research that have become
possible.
5.1. Research directions with the new paradigm
Contextual information is a key aspect of the new kinds of information
available due to the big data revolution. We propose:
• Research Direction 1. Explore the impacts of knowledge about
contextual awareness. Newly available contextual information can be
collected inexpensively, and supports the analysis of new levels of contextual
awareness on the part of consumers in many commercial settings.
New research questions can be explored that have not been considered before, which creates the potential for new knowledge in the business,
consumer and social insights arena.
Contextual information provides important clues to understanding
why different outcomes occur in different settings. For example, it may
be relevant to explore a spatio-temporal setting (e.g., a consumer leaving
a coffee shop in a mall) or a sequence of settings over time (e.g., mobile
location information to show the shops a consumer visited in a shopping
mall on Saturday morning versus Wednesday afternoon). These provide
especially rich settings in which to discover consumer insights that have
never been identified before. Traditionally, seasonal demand effects offered
useful contextual information in aggregate. Today though, information
can be collected at the level of the individual (e.g., customers visiting
an online shop or a bricks-and-mortar seller). We also can study consumers'
instantaneous reactions to time and location-specific purchase
incentives, or responses to tweets in a social network context involving
political topics, candidates and elections, or user responses to instant
deals in group-buying. New research can focus on contextual factors
and triggers that create consumer decision responses.
To exploit data on contextual awareness, researchers should
consider how such information can support extreme personalization
or hyperdifferentiation in product and service relationships. We
propose:
• Research Direction 2. Reassess the business value of personalization
and extend the study to consumer and firm co-informedness and
hyperdifferentiation. The extraordinary amount of data that can be
captured on consumers today creates dramatic changes in the basis for
value maximization from the consumer's and the vendor's standpoints.
Innovative experimental approaches create new means for vendors to
tease out the new dimensions for product and service designs, and new
product and service bundling designs that are likely to emerge as a result.
Affective computing-based data collection, decision monitoring, locationbased
data, and sequential response data can support new kinds of research
in these areas.
The era of terabytes and petabytes of data also affords inexpensive
collection of micro-level data to yield insights on human behavior.
One outcome is that individual-level information has the potential to
transform the quantitative study of consumer decision-making, which
will offer new directions for software and machine-based decision support.
We expect to see new and more in-depth research on preadoption
and post-adoption usage behavior for all kinds of things — social
media, software-based decision aids, websites, data storage services,
and so on. Researchers will be able to move beyond purchase intention to
purchase observation research in the presence of sales patterns for
hyperdifferentiated products and services, and produce more valuable
research findings in the process. New levels of consumer informedness
are likely to affect consumer web surfing behavior and the action sequences
that lead to purchases. Recent approaches involving neuroimaging
and affective computing offer other new ways to explore individual
diversity and the empirical regularities of emotional reactions
through big data [34,82].
A third aspect is related to data that are produced when events occur
in various settings. Some examples are: transactions in electronic markets;
exchanges in social and mobile phone networks; first consumer
contacts with a company's website; viewing initiation in digital entertainment
settings; and so on. We suggest the following research direction
on how big data can support this kind of research:
• Research Direction 3. Pursue big data studies that focus on
patterns, explanations and predictions of events in business,
consumer and social insight settings. New means for digital sensing
of events in various settings are available through technological advances
that can produce behavioral and economic information at the individual,
group, network, firm and societal levels. Event-focused analysis opens up
the possibility of the application of new research methods to discover
knowledge that may otherwise not be acquired also.Events in different business, consumer and social settings are indicative
of critical transitions that are often worthwhile to study closely. For
example, these occur when a consumer's online search results in a purchase
transaction, or when the occurrence of social events lead to
changes in social sentiments, or when pricing changes result in the
changed sales performance of a firm. Decisions have been made. The
study of events that have occurred in the past or are likely to happen
motivates natural and controlled experiments. We also can obtain an
understanding of where events occur; for example: where in a social
network; where in a physical network of stores; where in the geospatial
patterns of people's usage of mobile phones; or where a Twitter user
originates a message to her followers. These kinds of observations
open up the use of methods for analysis that involve interdisciplinary
knowledge. These include: event history analysis to understand the
genesis, likelihood and timing of observable events [15,59]; spatial
econometrics and data mining to assess how space interacts with time
in the daily patterns of consumer purchase behavior [2,14]; and count
data modeling and stochastic counting processes to bring greater analytical
sophistication to our assessment of the simplest elements of observable
phenomena in data, such as the count, frequency and variation
in density of events [20,86].
• Research Direction 4. Explore market and societal-level impacts
with new data-sensing and social media capabilities. New data
collection and data-sensing mechanisms in market and social media contexts
open up opportunities for conducting research to observe and understand
the impacts of human behavior in more aggregate market and
societal settings. Examples are group-buying and social commerce, electronic
auctions and digital markets, credit card transaction volumes,
transportation and traffic flows, and social sentiments.
During the past few years, there has been much research conducted
using social media websites, especially Twitter and Facebook, but also
job posting websites, such as Monster, LinkedIn, Indeed, CareerBuilder,
and SimplyHired. From these kinds of websites, there are many opportunities
to collect data that permit the assessment of more aggregate
level issues in society and in the economy. The social media sites report
on social sentiments among people in online communities, as well as
their preferences, choices and social relationships. The job posting
sites provide social clues on the state of labor and unemployment, and
reflect aggregate-level information in terms of secular shifts in demand
for a variety of skills and human capital in the marketplace. In addition,
it may be possible to glean information from such sources as credit card
services providers, phone services vendors, and digital entertainment
providers on basic societal-level information, including people's spending
behavior, creditworthiness, and sensitivity to changes in the economy.
Research on aggregate behavior – the market and society levels –
will provide a new basis with societal-scale data sets to identify how
to sense the need for and improve social services, such as municipal
rapid transit, highway traffic problems, and the quality of urban housing
and public services. Governments will play an important role in fostering
the disclosure of large data sets in the pursuit of open innovation,
and as a means to improve social welfare for the general public.
Underlying all of the other research directions that we have mentioned
is the issue of information privacy, which has become more critical,
as people suffer from the negative impacts of clickstream data
mining, mobile phone location tracking, social network connection observation,
and the general loss of privacy associated with digital life.
We propose:
• Research Direction 5. Conduct complementary research on
information privacy issues related to big data sets in parallel
with investigations of business, consumer and social insights. The
rise of big data and the new vulnerabilities that it creates prompt the exploration
of information privacy issues that affect people as individuals, in
groups, in relation to private and public organizations, and with respect
to government and society as a whole.There are multiple directions that such research can take. Important
research, for example, can be done on the creation of a new codex for appropriate
processes in research projects that involve the capture of extensive
data on individual behavior, communication, transactions and
other kinds of social interactions. As it stands today, in many research
settings that we are familiar with, the individual – as a social network
participant, or a consumer, or in some other role – is vulnerable to the
capture of data that she doesn't know is being targeted. Sometimes this
is occurring with the best of intentions. For example, a credit card company
tracks a consumer's purchases to identify her merchants and shopping
patterns so as to be able to shut down her account in an instant if
there are hints that fraudulent transactions are being made. Universitybased
institutional review boards for research with human subjects still
have not come to grips with the technological complexities and implications
of data collection from mobile phones, social networks, and other
online settings in which micro-level data can be obtained though.
5.2. Practical considerations
To appropriate the benefits of using very large data sets for data
analytics research, additional effort is required, and there also are practical
issues to deal with that involve limitations and feasibility issues.
5.2.1. Difficulties with data collection, cleaning and reliability
Collecting and cleaning large data sets is never easy. Advanced, scalable
data processing and programming skills for analytics software are
required (e.g., DB SQL, SAS, Matlab, Stata, R, etc.). Data come from multiple
sources and data scientists must match and integrate large tables
of data to make them useful. Real-world business data tend to be
messy, with inconsistencies, built-in biases, and errors. So analysts
need to use special care and a lot of efforts to prepare and understand
the contents of the data. Summary statistics offer useful clues to check
whether the data are consistent and logical. The data are likely to be
noisy as well, so it is appropriate to identify the appropriate scope and
volume of data that are needed to support the estimation or computation
of various models and deliver meaningful statistical results.
5.2.2. Organizational relationship development
Data collected in business contexts typically are proprietary, and subject
to non-disclosure agreements. Fortunately, organizations today
seem to have become aware of the strategic importance of investing in
business, consumer and data insight analytics-based decision-making.
Changing attitudes toward the practicality of working with large data
stores have encouraged academic researchers and industry practitioners
to join forces to understand people and the organizations, communities,
and markets in which they live and work. The new imperative in data analytics
is value co-creation [90], which encourages many organizations to
bring people with different skills together to jointly pursue a high-value
agenda of digital innovation for leading-edge information strategy.
5.2.3. Data acquisition and information security
The acquisition of large data sets for joint academic and industry research
from a sponsor is a process that requires care and foresight to ensure
that nothing goes wrong. Porting data between an organizational
sponsor and a university-based research center gives rise to a range of
issues. Data need to be anonymized or aggregated so that no
personally-identifying information is made available. They have to be
sent from one institution to another with full controls to minimize risk
and errors. Also, even after data have been ported to secure servers in
a research institution, project researchers still must be careful to prevent
any data leakages, inappropriate uses, or losses of control. To implement
the necessary protections is a costly and challenging job,
which requires attention to data handling process design, the specification
and implementation of training and procedures so employees
know their responsibilities, periodic auditing to ensure the research
and data server environments are properly protected, and problem-identification processes that identify the potential for information security
events to arise. Our experience with this process suggests best practice
is not a matter of how individuals work with data. Instead, it is how
their organization encourages and structures the work of its research
faculty, staff, students and other collaborators to ensure safe data and
information handling. Organizational competencies support the development
of staff competencies and competitive advantage in developing
skills with data analytics, as well as data acquisition and information security
safeguards for data at scale. These things are likely to become the
new hallmarks of the capabilities of leading research universities.
6. Conclusion
The excitement is high around the new opportunities that big data
make available in research. We have emphasized the importance in
the role it plays to diminish the three-horned dilemma in computational
social science research. This change is a paradigm shift that enables us to
study a wider range of issues in time and context with unprecedented
control and new real-world insights. Still, the challenges for conducting
this kind of research are significant, as our discussion of practical considerations
suggests.
Beyond the new research approach that we have described remain
the same kinds of issues that make creativity in research a neverending
pursuit, and a distant image on the horizon of new knowledge.
Big data offer us new ways to proceed, but the sharp thinking that leading
researchers seem to demonstrate about how to craft insightful experiments,
distill useful explanations and rework theoretical interpretations
from intuition to knowledge is often nothing short of miraculous. As
Jonah Lehrer [64,65], author of the “Frontal Cortex” blog at Wired magazine,
has written, a key quality of the kinds of people who are successful
with creativity in the range of activities they pursue is the psychological
trait called “grit.” Grit is about persistence, work and iteration much
more than it is about discovery, since it describes the effort and the process
that we must engage in to be successful in research. With big data
and the methods that can be used to leverage it, nothing will come for
free. We also should not confuse changes in the costs of methodology
for discovery with the typically high expenses of achieving insights.
We must remind the reader that results with data alone rarely tell the
full story that managers need to hear. For managers to successfully interpret
what the data have to say based on sophisticated analytics often requires
relatively simple background information. This might include, for
example: what a firm's competitors were doing when a firm introduced
new prices for its digital telephony and entertainment services; how ebanking
transaction patterns changed at physical automated teller machine
locations, when a bank introduced new functionality through its
Internet banking or mobile phone banking channels; or how the creation
of Wikipedia material changed in the face of endogenous and exogenous
shocks.
Big data, in our view, are powerful – powerful to an extreme in some
ways – but they also complement simple and unsophisticated managerial
knowledge about what is actually going on in the real-world setting.
We advocate “walkabout” observation used in social science research
for identifying key informants in the research contexts that we study
with big data.We need to have their help so we can understand the simple
aspects and complexities of the research settings, and key events
that have occurred or are due to happen that make it possible to conduct
insightful natural and fully-specified experiments.
Although we have focused on how the presence of big data changes
the paradigm for computational social science research, the insights
obtained from the new approaches are key to more effective datadriven
decision-making by managers. Brynjolfsson et al. [18] have
pointed out that firms that emphasize decision-making based on business
analytics have higher performance in productivity, asset utilization,
return on equity and market value. Since big data are now
everywhere and most firms can acquire it, the key to competitive advantage
is to accelerate managerial decision-making by providing managers with implementable guidelines for the application of data analytics
skills in their business processes. This takes us beyond the “diamonds
in the data mine” to the rigor of best practices in data analytics
for the business world [70]. Humans still will design the processes,
and insights can be best discovered with a combination of machinebased
data analysis and the intuition of people [19].
Acknowledgments
The ideas in this article were presented as they were developed at:
the 2011 Workshop on E-Business in Shanghai, China; the 2012 International
Conference on Information Management in Kaohsiung, Taiwan;
the 2012 Wuhan International Conference on E-Business in China; the
China University of Geoscience; and included as discussion papers for
the 2012 Workshop on Analytics for Business, Consumer and Social Insights,
and the 2012 International Conference on Electronic Commerce
in Singapore. We acknowledge the people who provided us with comments
at the conferences, workshop and universities we visited, and
for ongoing discussions with colleagues Darren Choo, Peter Cook, Pedro
Ferreira, Steve Fienberg, Kim Huat Goh, Timothy Goh, Jia Jia, Kwansoo
Kim, Youngsoo Kim, Adrian Koh, Ramayya Krishnan, Prakash Kumar,
Hoong Chuin Lau, Dongwon Lee, Ee-Peng Lim, Dan Ma, Rakesh Menon,
Steve Miller, Archan Mishra, Andrew Ngai, Srini Reddy, Mike Sherman,
Jialie Shen, Yesha Sivan, Sushila Shivpuri, Priyanka Singh, Insoo Son, Raj
Srivastava, Rahul Telang, Chuck Wood, Irene Xu, Dongsong Zhang,
Michael Zhang, Peiran Zhang, Jing Zhao, Yinping Yang, Alejandro
Zentner, Vladimir Zwass and the doctoral students in IS 705 on Research
Foundations for IS and Management Research at the School of Information
Systems at Singapore Management University. Interactions with
faculty, research colleagues and senior administration colleagues related
to the “Analytics for Business, Consumer and Social Insights” interdisciplinary
Area of Excellence at Singapore Management University were
helpful too. This research was supported by the Singapore National
Research Foundation under the International Research Centre @
Singapore Funding Initiative, administered by the Interactive Digital
Media Program Office (IDMPO).
Computational social science: Obstacles and opportunities
T
he field of computational social science
(CSS) has exploded in prominence
over the past decade, with
thousands of papers published using
observational data, experimental
designs, and large-scale simulations
that were once unfeasible or unavailable
to researchers. These studies have greatly
improved our understanding of important
phenomena, ranging from social inequality
to the spread of infectious diseases. The
institutions supporting CSS in the academy
have also grown substantially, as evidenced
by the proliferation of conferences,
workshops, and summer schools across the
globe, across disciplines, and across sources
of data. But the field has also fallen short in
important ways. Many institutional structures
around the field—including research
ethics, pedagogy, and data infrastructure—
are still nascent. We suggest opportunities
to address these issues, especially in improving
the alignment between the organization
of the 20th-century university and
the intellectual requirements of the field.
We define CSS as the development and
application of computational methods to
complex, typically large-scale, human (sometimes
simulated) behavioral data (1). Its intellectual
antecedents include research on
spatial data, social networks, and human
coding of text and images. Whereas traditional
quantitative social science has focused
on rows of cases and columns of variables,
typically with assumptions of independence
among observations, CSS encompasses language,
location and movement, networks,
images, and video, with the application of
statistical models that capture multifarious dependencies within data. A loosely connected
intellectual community of social scientists,
computer scientists, statistical physicists,
and others has coalesced under this
umbrella phrase.
MISALIGNMENT OF UNIVERSITIES
Generally, incentives and structures at most
universities are poorly aligned for this kind
of multidisciplinary endeavor. Training
tends to be siloed. Integrating computational
training directly into social science (e.g.,
teaching social scientists how to code) and
social science into computational disciplines
(e.g., teaching computer scientists research
design) has been slow. Collaboration is often
not encouraged, and too often is discouraged.
Computational researchers and social
scientists tend to be in different units in
distinct corners of the university, and there
are few mechanisms to bring them together.
Decentralized budgeting models discourage
collaboration across units, often producing
inefficient duplication.
Research evaluation exercises such as
the United Kingdom’s Research Excellence
Framework, which allocate research funding,
typically focus within disciplines, meaning
that multidisciplinary research may be
less well recognized and rewarded. Similarly,
university promotion procedures tend to
underappreciate multidisciplinary scholars.
Computational research infrastructures at
universities too often cannot fully support
analysis of large-scale, sensitive data sets,
with the requirements of security, access to
a large number of researchers, and requisite
computational power. To the extent these issues
have been partially resolved in the academy
(e.g., with genomic data), lessons have
not fully made their way into practice in CSS.
INADEQUATE DATA-SHARING PARADIGMS
Current paradigms for sharing the kinds of
large-scale, sensitive data used in CSS offer a
mixed bag. There have been successes built
on partnerships with government, especially in economics, from the study of inequality
(2) to the dynamics of labor markets (3).
There are emerging, well-resourced models
of administrative data research facilities
serving as platforms for analyzing microlevel
data while preserving privacy (4). These offer
important lessons for potential collaboration
with private companies, including the development
of methodologies to keep sensitive
data secure, yet accessible for analyses (e.g.,
innovations in differential privacy).
The value proposition for private companies
is different and there has been predictably
less progress. Data possessed by
government agencies are held in trust for
the public, whereas data held by companies
are typically seen as a key proprietary asset.
Public accountability inherent in sharing
data is likely seen as a positive for the
relevant stakeholders for government agencies,
but generally, far less so for shareholders
for private companies. Access to data
from private companies is thus rarely available
to academics, and when it is, it is typically
granted through a patchwork system
in which some data are available through
public application programming interfaces
(APIs), other data only by working with (and
often physically in) the company in question,
and still other data through personal
connections and one-off arrangements, often
governed by nondisclosure agreements
and subject to potential conflicts of interest.
An alternative has been to use proprietary
data collected for market research (e.g.,
Comscore, Nielsen), with methods that are
sometimes opaque and a pricing structure
that is prohibitive to most researchers.
We believe that this approach is no longer
acceptable as the mainstay of CSS, as pragmatic
as it might seem in light of the apparent
abundance of such data and limited
resources available to a research community
in its infancy. We have two broad concerns
about data availability and access.
First, many companies have been steadily
cutting back data that can be pulled from
their platforms (5). This is sometimes
for good reasons—regulatory mandates
(e.g., the European Union General Data
Protection Regulation), corporate scandal
(Cambridge Analytica and Facebook)—however,
a side effect is often to shut down avenues
of potentially valuable research. The
susceptibility of data availability to arbitrary
and unpredictable changes by private
actors, whose cooperation with scientists is
strictly voluntary, renders this system intrinsically
unreliable and potentially biased
in the science it produces.Second, data generated by consumer products
and platforms are imperfectly suited for
research purposes (6). Users of online platforms
and services may be unrepresentative
of the general population, and their behavior
may be biased in unknown ways. Because
the platforms were never designed to answer
research questions, the data of greatest
relevance may not have been collected (e.g.,
researchers interested in information diffusion
count retweets because that is what is
recorded), or may be collected in a way that
is confounded by other elements of the system
(e.g., inferences about user preferences
are confounded by the influence of the company’s
ranking and recommendation algorithms).
The design, features, data recording,
and data access strategy of platforms may
change at any time because platform owners
are not incentivized to maintain instrumentation
consistency for the benefit of research.
For these reasons, research derived from
such “found” data is inevitably subject to
concerns about its internal and external validity,
and platform-based data, in particular,
may suffer from rapid depreciation as those
platforms change (7). Moreover, the raw data
are often unavailable to the research community
owing to privacy and intellectual
property concerns, or may become unavailable
in the future, thereby impeding the reproducibility
and replication of results.
INADEQUATE RULES
Finally, there has been a failure to develop
“rules of the road” for scientific research.
Despite prior calls to develop such guidance,
and despite major lapses that undermined
public trust, the field has failed to
fully articulate clear principles and mechanisms
for collecting and analyzing digital
data about people while minimizing the
potential for harm. Few universities provide
technical, legal, regulatory, or ethical
guidance to properly contain and manage
sensitive data. Institutional Review Boards
are still generally not attuned, and consistent
in their response, to the distinct ethical
challenges around digital trace data. The
recent modification of the Common Rule in
the United States, which concerns ethics of
human subjects research, did not fully address
these problems.
For example, in a networked world, how
should we deal with the fact that sharing
information about oneself intrinsically provides
signals about those with whom one is
connected? The challenges around consent
highlight the importance of managing security
of sensitive data and also of reimagining
institutional review processes and ethical
norms; yet few universities integrate infrastructure
and oversight processes to minimize
the risks of security lapses.
Resources and rules,
incentives and innovations
Strengthen collaboration
• Develop enforceable guidelines in
collaborations with industry around
research ethics, transparency, researcher
autonomy, and replicability.
• Develop secure data centers
supplemented by an administrative
infrastructure for granting access,
monitoring outputs, and enforcing
privacy and ethics rules.
New data infrastructures
• Develop large-scale, secure, privacypreserving,
shared infrastructures
driven by citizen contributions of time
and/or data. Capture the metadata
that describe the collection process.
• Develop infrastructure to capture the
dynamic, algorithm-driven behavior of
the major platforms over time.
• Promote legal frameworks that allow
and mandate ethical data access
and collection about individuals and
rigorous auditing of platforms.
Ethical, legal, and social implications
• Professional associations should help
develop ethical guidelines.
• Large investments are needed to
develop regulatory frameworks and
ethical guidance for researchers.
Reorganize the university
• Develop structures that connect
researchers having shared interests
in computational approaches.
• Fundamentally reconceive graduate
and undergraduate curricula.
• Reward collaboration across silos.
• Appoint faculty with multi-unit
affiliations
• Physically collocate faculty from
different fields
• Allocate internal funding to support
multidisciplinary collaboration.
• Empower and enforce ethical research
practices—e.g., centrally coordinated,
secure data infrastructures.
Cambridge Analytica, and other, similar,
events, have engendered an impassioned
debate around data sovereignty. Battle lines
have been drawn between privacy advocates
and companies, where the former seek
to minimize the collection and analysis of
all individual data, whereas the latter seek
to justify their collection strategies on the
grounds of providing value to consumers.
Often missing in public debates are voices
for policies that would encourage or mandate
the ethical use of private data that preserves
public values like privacy, autonomy,
security, human dignity, justice, and balance
of power to achieve important public
goals—whether to predict the spread of disease,
shine a light on societal issues of equity
and access, or the collapse of the economy.
Also often missing are investments in infrastructures
in the academy that could power
knowledge production and maintain privacy.
RECOMMENDATIONS
In response to these problems, we make five
recommendations.
Strengthen collaboration
Despite the limitations noted above, data
collected by private companies are too important,
too expensive to collect by any
other means, and too pervasive to remain
inaccessible to the public and unavailable
for publicly funded research (8). Rather than
eschewing collaboration with industry, the
research community should develop enforceable
guidelines around research ethics, transparency,
researcher autonomy, and replicability.
We anticipate that many approaches will
emerge in coming years that will be incentive
compatible for involved stakeholders.
The most widespread and longest-standing
model is open, aggregated data such as
Census data. The aforementioned models
developed to share government data, with
an emphasis on security and privacy, offer
promise in working with corporate data. The
United Nations Sustainable Development
Goals call for partnerships on public-private
data sources to provide a wide variety of new,
very rich neighborhood-by-neighborhood
measures across the entire world (9), and
national statistical offices in every corner of
the world are quietly working on producing
such products, but progress is slow owing to
lack of funding. The development of secure
administrative data centers supplemented by
an administrative infrastructure for granting
access, monitoring outputs, and enforcing
compliance with privacy and ethics rules offers
one model for moving forward. As noted
above, this model has already been demonstrated
in the domain of government administrative
data; as well as in a few cases, by
telecommunications and banking companies.
Similar models are rare—but becoming
more common—for academic research.
The Open Data Infrastructure for Social
Science and Economic Innovations in the
Netherlands is one example. Facebook has
iterated through multiple models for collaboration
with academics. In its early years, it
focused on one-off collaborations, largely in formally negotiated. After the 2016 election,
it launched Social Science One, providing
access to aggregate data of news consumption,
which, despite being well resourced, has
faced challenges in providing data (10).
Coronavirus disease 2019 (COVID-19) has
played a particular role in creating partnerships
between researchers and companies to
produce insights regarding the trajectory of
the disease. (COVID-19 has, in many countries,
including the United States, also illuminated
the fractured and politically contingent
nature of much public data regarding
the disease.) Twitter has provided a streaming
API regarding COVID-19 for approved
researchers. Similarly,   location data  companies
such as Cuebiq have provided access
to anonymized mobility data. There remain
open questions as to what extent these datasharing
efforts will continue after the disease
settles into the history books and, if so, how
to robustly align them with critical research
norms in academia, such as transparency, reproducibility,
replication, and consent.
The election examples with respect to
Facebook highlight the potentially adversarial
role between researchers and corporations.
A central contemporary question for
the field of CSS (as discussed below) is in
what ways particular sociotechnical systems
are playing positive and negative roles in
society. This tension may partially (but not
entirely) be resolved if companies feel that it
is in their long-term interest to transparently
study and anticipate these issues. Even in the
most optimistic scenario, however, there will
be a disjuncture between the public interest
in the insights that research could produce,
and corporate interests.
Academia, more generally, needs to provide
carefully developed guidelines for professional
practice. What control can companies
have over the research process? It clearly
is not acceptable for a company to have veto
power over the content of a paper; but the
reality of any data-sharing agreement is that
there are negotiated domains of inquiry.
What are the requirements for providing
data for replication? What are the needs of
researchers for access to a company’s internal
data management and curation processes?
New data infrastructures
Privacy-preserving, shared data infrastructures,
designed to support scientific research
on societally important challenges, could
collect scientifically motivated digital traces
from diverse populations in their natural environments,
as well as enroll massive panels
of individuals to participate in designed experiments
in large-scale virtual labs. These
infrastructures could be driven by citizen
contributions of their data and/or their time
to support the public good, or in exchange forexplicit compensation. These infrastructures
should use state-of-the-art security, with an
escalation checklist of security measures depending
on the sensitivity of the data. These
efforts need to occur at both the university
and cross-university levels. Finally, these infrastructures
should capture and document
the metadata that describe the data collection
process and incorporate sound ethical
principles for data collection and use. The
Secure Data Center at the GESIS Leibniz
Institute for the Social Sciences is an example
of shared infrastructure for research
on sensitive data. Further, it is important to
capture the algorithm-driven behavior of the
major platforms over time (11, 12), both because
algorithmic behavior is of increasing
importance and because algorithmic changes
create enormous artifacts in platform-based
data collection. It is critical that legal frameworks
allow and mandate ethical data access
and collection about individuals and rigorous
auditing of platforms.
Ethical, legal, and social implications
We need to develop ethical frameworks
commensurate with scientific opportunities
and emergent risks of the 21st century.
Social science can help us understand the
structural inequalities of society, and CSS
needs to open up the black box of the datadriven
algorithms that make so many consequential
decisions, but which can also embed
biases (13). The Human Genome Project
devoted more than $300 million as part of
its Ethical, Legal, and Social Implications
program “to ensure that society learns to
use the information only in beneficial ways”
(14). There are no off-the-shelf solutions on
ethical research. Professional associations
need to work on the development of new
ethical guidelines—the guidelines developed
by the Association of Internet Researchers
offer one example of an effort to address a
slice of the issue. Large investments, by public
funders as well as private foundations,
are needed to develop informed regulatory
frameworks and ethical guidance for researchers,
and to guide practice in the field
in government and organizations.
Reorganize the university
Computation is adjacent to an increasing
number of fields—from astronomy to the humanities.
There needs to be innovation in the
organization of typically siloed universities to
reflect this, with the development of structures
that connect diverse researchers, where
collaborating across silos is professionally rewarded.
Successful examples of institutional
practice include the appointment of faculty
with multi-unit affiliations (e.g., across computer
science and social science disciplines)
and of research centers that physically collocate faculty from different fields, as well as
allocation of internal funding to support multidisciplinary
collaboration. There needs to
be a fundamental reconceiving of the development
of undergraduate and graduate curricula
for training a new generation of scientists.
There must be pervasive efforts within
the university to empower and enforce ethical
research practices—e.g., centrally coordinated,
secure data infrastructures.
Solve real-world problems
The preceding recommendations will require
resources, from public and private sources,
that are extraordinary by current standards
of social science funding. To justify such an
outsized investment, computational social
scientists must make the case that the result
will be more than the publication of
journal articles of interest primarily to other
researchers. They must articulate how the
combination of academic, industrial, and
governmental collaboration and dedicated
scientific infrastructure will solve important
problems for society—saving lives; improving
national security; enhancing economic
prosperity; nurturing inclusion, diversity,
equity, and access ; bolstering democracy;
etc. Current applications of CSS in the global
response to the pandemic are emblematic
of the broader potential of the field. Beyond
generating results that are meaningful outside
of academia, the pursuit of this objective
may also lead to more replicable, cumulative,
and coherent science (15).
We live life in the network. We check our e-mails
regularly, make mobile phone calls from almost
any location … make purchases with credit cards
… [and] maintain friendships through online
social networks. … These transactions leave
digital traces that can be compiled into comprehensive
pictures of both individual and group
behavior, with the potential to transform our
understanding of our lives, organizations, and
societies.
—Lazer et al. (2009, 721).
Powerful computational resources combined
with the availability of massive social media datasets
has given rise to a growing body of work that
uses a combination of machine learning, natural
language processing, network analysis, and statistics
for the measurement of population structure
and human behavior at unprecedented scale.
However, mounting evidence suggests that many
of the forecasts and analyses being produced
misrepresent the real world.
—Ruths and Pfeffer (2014, 1063)
The exponential growth in “the volume,
velocity and variability” (Dumbill 2012, 2)
of structured and unstructured social data has
confronted fields such as political science, sociology,
psychology, information systems, public
health, public policy, and communication with
a unique challenge: how can scientists best use
computational tools to analyze such data, problematical
as they may be, with the goal of
understanding individuals and their interactions
within social systems? The unprecedented
availability of information on discrete behavDhavan
V. Shah is the Louis A. & Mary E. MaierBascom
Professor at the University of Wisconsin–
Madison, where he is director of the Mass
Communication Research Center. His work focuses on
framing effects on social judgments, digital media influence
on civic engagement, and the impact of health
ICTs.iors, social expressions, personal connections, and social alignments provides
insights on a range of phenomena and influence processes—from personality
traits to political behaviors; from public opinion to relationship formation—
despite issues of representativeness and uniformity. That is, even though data
from social media may not represent the entirety of a population, that does not
mean they are without research value for understanding that population. And the
challenges of interpreting these sorts of social data are not limited to population
biases and tailored content (Pariser 2011); they extend to the ethics of research
practice and personal privacy, the value of theory and reasoning in relation to
prediction and engineering, and, of course, the application of appropriate and
rigorous modes of inference. This introduction, to a volume considering these
possibilities and perils, explores some of the key issues confronting researchers
who pursue computational social science in the age of big data.
At the outset, we should explain what we mean by computational social science
as a specific subcategory of work on big data. It is an approach to social inquiry
defined by (1) the use of large, complex datasets, often—though not always—
measured in terabytes or petabytes; (2) the frequent involvement of “naturally
occurring” social and digital media sources and other electronic databases; (3) the
use of computational or algorithmic solutions to generate patterns and inferences
from these data; and (4) the applicability to social theory in a variety of domains
from the study of mass opinion to public health, from examinations of political
events to social movements.
We emphasize that the phrase “naturally occurring” in the above definition is
of special importance. Surveys and experiments, the traditional work horses of
the social sciences, by their nature involve the intervention of researchers into
social processes, engaging unavoidably in various types of experimenter effects
(e.g., unintentionally treating experimental and control group subjects differently
in ways that shape their responses) and self-report/social desirability biases (e.g.,
survey respondents tendency to overreport good qualities and behaviors, while
underreporting less desirable ones). Computational analyses of big data offer a
welcome counterpoint and potential triangulation of multimethod confirmation
of key findings in concert with experiments and surveys (Campbell and Fiske
1959). Nonetheless, working with such data remains challenging, not least
because of the issues of generalizability, ethics, and theory noted above, but also
because of the acquisition, archiving, and analysis of these types of data, which
are not easily processed using conventional database applications.
Yet recent increases in storage capacity, boosts in processing power, and the
availability of analytic systems have fundamentally expanded the ability of social scientists to collect and utilize these sorts of data. What previously required
access to networked computing cores in a dedicated facility can now be handled
by a small server cluster housed in a corner of an office or, alternatively, “in the
cloud,” through a distributed computing system. In this volume, social and electronic
media sources are being used by psychologists, epidemiologists, and political
and communication scientists to (1) code content and sentiment to infer
subjective well-being and personality traits (Schwartz and Ungar, this volume),
score the emotionality of news content (Soroka et al., this volume), and trace
signals of public opinion (González-Bailón and Paltoglou, this volume); (2) cluster
and map networks to understand political alignments (Bode et al., this volume;
Freelon et al., this volume), and predict the emergence of online
relationships (Welles and Contractor, this volume); and (3) examine dynamics
between conventional and social media during presidential debates (Shah et al.,
this volume) and school shootings (Guggenheim et al., this volume).
Commercial ventures and academic researchers are also deploying large-scale
data interventions in social media. This approach introduces changes in the
online environments of social media users and observes the consequences in
online behavior. For example, Kramer, Guillory, and Hancock (forthcoming)
obtained evidence of text-based emotional contagion resulting from small
linguistic changes in news stories forwarded to Facebook users. Work of this sort
has produced significant and robust, though small, changes in observed behaviors
but also provoked outrage from those who have been (or might have been) the
targets of the intervention without prior consent. High-profile events such as
this may have long-term consequences for how computational social science is
undertaken.
Equally ambitious, these sorts of tools and techniques are also deployed to
examine entire social networks on a longitudinal basis (Resnick et al., this volume;
Han et al. 2011), connect natural language processing with neuroimaging
to understand message transmission (O’Donnell and Falk, this volume), and
generate more effective health messages through the use of automated filtering
systems (Cappella et al., this volume). Such holistic and personalized information
collection also speaks to the growing set of conceptual and ethical questions concerning
data use and its limits. The acquisition and archiving of complex data
systems—let alone their manipulation—often involve collecting personally identifiable
information. This forces some reflection on issues of data privacy and
de-identification, especially in an era of increased tracking of expression and
action, especially regarding physical and mental health (Crosas et al., this volume).
Such concerns must be weighed against the value of scholarly understanding,
with appropriate steps taken to protect individual privacy and honor the
principle of informed consent.
Computational Research and Data Science
On a fundamental level, scholars across the social sciences are questioning the
role of big data in relation to conventional methods, theory building, and formal scientific reasoning. Hybrid methods that combine or compare established
approaches, such as manual content coding or conventional survey research, with
computational systems, like machine learning or network mapping, are gaining
ground (Burscher et al., this volume; Park et al., this volume; Zamith and Lewis,
this volume). Some scholars integrate while others contrast methods to highlight
the strengths of each approach, though both camps tend to stress their complementarity.
This suggests the need for scholars who can “employ an interdisciplinary
skill set that draws from traditional social sciences, statistics, and computer
science” (Miller 2011, 1815), and is in sharp contrast to those who suggest abandoning
established methods in favor of data science.
Indeed, some have gone so far as to suggest that “with massive data, this
approach to science—hypothesize, model, test—is becoming obsolete” (Anderson
2008, 108). Rich data and algorithmic approaches such as machine learning permit
more accurate forecasts in many areas (Hindman, this volume), though often
absent theoretical justification. Some laud these approaches for allowing the
engineering of “useful computational artifacts,” even though they may not serve
the goal of producing deeper social scientific understanding (Lin, this volume).
This is not a perspective we fully share, nor do we advocate the notion that big
data will replace and make surveys, lab experiments, clinical trials, and content
analyses irrelevant (Anderson 2008).
Nonetheless, from a policy perspective, these systematic predictive and analytic
techniques can provide insight into, if not directly solve, significant social
problems. The availability of large amounts of social communication about
daily life—real-time reactions to media, political, environmental, and social
events—and evaluation of those data by the groups producing the content
raises the possibility of immediate access to cultural (and subgroup) discourse.
For example, we can mine data for insight into environmental pollution patterns
by “sniffing social media” (Mei et al. 2014) and understand the spread of
contagious diseases as traced through symptom posting (Khoury and Ioannidis
2014). Mining social data can also be used to enable health interventions to
better target affected subpopulations (Barrett et al. 2013). In addition, the push
toward electronic health records creates opportunities to cull available data to
test “the effectiveness of the intervention among real patients in real settings,
the safety and side effects of the intervention, and determining for whom the
intervention may be most effective” (Hesse et al., this volume).
Computational approaches, in other words, have the capacity to gather and
process large quantities of information quickly to serve the public good and
examine the public agenda. In doing so, researchers must contend with content
intended to mislead citizens and consumers through the deployment of “spam
bots” generating comments on everything from political candidates’ policy briefs
to hotel accommodations’ service quality. It is not surprising, then, that in
expressing concerns about representativeness of social data, Ruths and Pfeffer
(2014) also note the flaws and distortions that emerge as a consequence of nonhuman
accounts. Others have sought to find indicators of real and planted commentary
in the online environment (Ott et al. 2011). These features can introduce
serious distortions into research and must be addressed to ensure data validity.While we recognize that a sample of tweets or even a universe of tweets, for
example, is not representative of or projectable to a universe of individuals as
implied by the term public opinion (Hargittai, this volume), that is not our primary
concern here. Indeed, such collections are not even a reliable sample of
content circulating within social media more broadly defined (Driscoll and
Thorson, this volume). But as an indicator of sentiment or behavior or diffusion
in relation to a particular topic at a particular time, it is an increasingly prominent
and important indicator of what is occurring in the public sphere, now accessible
to systematic and real-time analysis. To manage and analyze these complex datasets,
social scientists are forging connections with specialists in mathematics,
statistics, engineering, computer science, information systems, and high-throughput
computing and are using tools often developed by industry and government,
a step toward transdisciplinary work.
Organizing Themes and Vexing Issues
It is with these issues in mind that we organized this volume around five main
themes that represent frictions in this field space and reflect the cutting edge of
research:
(1) Reflections on tools for collaborative research and computational modeling,
with particular attention to prediction, privacy, and sampling biases;
(2) Examinations of language and discourse as indictors of traits, cognitions,
and behaviors, as tapped through automated text coding and machine
learning;
(3) Studies about social connections in the form of network ties, information
flows, and social clustering that define interpersonal connections and
political action;
(4) Research considering influences of and on social media as understood in
relation to overtime changes in external factors such as traditional media
content; and
(5) Advances into complementary procedures, including large-scale data management,
recommendation systems, neuroimaging, and hybrid approaches.
As noted above, a number of additional themes cut across these sections and
studies. In particular, several articles consider (1) the role of collection systems in
computational social science, (2) the need to attend to multiple platforms when
sampling content, (3) the tension between conventional and computational
methods, (4) the need for team science and transdisciplinary research, and (5)
the relation between theory and big data. The richness of this volume rests on
these intersections and the core themes that we have selected, which are critical
issues for social scientists to confront.
Yet there are other, less obvious themes and issues that arise across these
entries. One such emergent issue is the use of inferential statistics in a de factocensus, albeit one that involves a draw from a larger corpus, or a massive social
media sample. A census of every tweet generated last Tuesday could be seen as
a “sample” of weekly data, but not likely a meaningful random sample if it is
intended to define a broader universe. Given the extraordinarily large sample
sizes in much of this research, nearly everything is statistically significant in big
data analytics (Lohr 2012). As such, researchers must use inferential statistics
carefully, recognizing the risk of “false discoveries”—Type I errors, or the assertion
of a relationship that is not present. Indeed, the issue of huge samples making
insignificant findings seem meaningful because they achieve conventional
thresholds of statistical significance may be a more problematic issue than the
question of what constitutes a true census of social data.
At times when the risk of a Type I error looms large, theory can provide essential
guidance. In the work by Kramer, Guillory, and Hancock (forthcoming) on
emotional contagion, for example, the effects observed in that study are tiny but
statistically significant, partly a function of the size of the sample involved.
Nonetheless, the authors argue that the effect is meaningful and consequential
in part because extant theories driving and explaining processes of emotional
contagion are well established. So rather than seeing their results as capitalizing
on chance or simply being inconsequential, they argue that they have another
manifestation of a core social interactive process—emotional contagion—in a
totally unique social media context. In this respect, we see psychological and
social theory as critical to interpreting computational findings.
A related concern centers on representativeness of social data relative to population
parameters. We contend that the discussion of bias begs the question about
what the population of interest is and what can be done to deal with bias. The
latter is especially important in an arena where sentiment analysis from social
media is seeking to replace or supplement more representative public opinion
work. For example, the Twitter “fire hose” is the actual universe of tweets, so there
is no bias there if the scholarly aim is to speak to the dynamics in that social space.
In contrast, a low-response-rate survey is a biased sample of public opinion,
despite the fact that it aims to represent the population. Even a high-responserate
survey represents a biased sample of the public to an extent. In both cases,
there are methods to correct for distortion. Scholars must incorporate these, or
they must at least acknowledge the ways their data limit inference.
Two additional issues reoccur across the articles collected here and emanate
from these smaller issues. One is big data versus theory. Some have argued that
big data mean the end of theory, while others assert that theory will be generated
through a combination of inductive and deductive approaches (Anderson
2008; boyd and Crawford 2012). There is a sharp tension between these positions,
with one camp advancing the view that data science and algorithmic systems
can produce faster, deeper, and more accurate and actionable results than
scientific specialists incrementally building knowledge, while the other camp
argues for the centrality of the interpreter of data and the essential role of theory
in big data analytics. Given that some big data collections render statistical
hypothesis testing essentially irrelevant because of the overwhelming levels of
power, we contend that making sense of findings requires good theory to offer clear a priori predictions and sensible explanations of what are otherwise uninterpretable
statistical tests (Kramer, Guillory, and Hancock, forthcoming). That
is, we tend to fall on the side of the latter position but can see value in the former,
especially the inductive approach. Others in this volume have addressed this
same issue (e.g., Lin, this volume).
Tied to this is the issue of prediction versus explanation. Many approaches in
the world of big data are primarily oriented toward building predictive models
that solve a problem, whether commercial or social or political. Is there a role for
explanation in heavily prediction-oriented modeling associated with some
approaches to big data? A number of our contributors talk about prediction-oriented
approaches and seem to suggest that prediction for its own sake is just fine
and that explanation—specifically causal explanation—can catch up later, if necessary.
We are of the opinion that the outcomes of some prediction-only
approaches can provide the grist for explanatory approaches, melding two of the
key components of computational social science—successful predictions and
explanatory models. For example, as the magnitude of data available allows prediction
models on smaller and smaller sets of cases—the individual case in the
extreme—then variation in the parameters of those prediction models themselves
become interesting objects of explanation and theorizing. The two orientations
do not need to be seen as either-or choices but rather as complementary at
least or even as opening new avenues of research and theory previously unavailable
because of a surfeit of data at the individual level.
Computational Communication Science
It is clear that the era of data science is reshaping the fields of communication,
political science, psychology, sociology, and public health. Computational social
science, with its focus on large-scale data and social media data, will precipitate
other shifts in the commitments and training of researchers, some obvious and
some less so. Much of the data of computational social science are and will be
textual, and require honing skills in natural language processing. Quantitative
social scientists have been accustomed to numerical data, collected either
through self-reported responses on scales or the assessment of formal instruments
(e.g., skin conductance).
With much of the core social data now in textual form, changing in central
ways how data are acquired and reduced, scholars will need to come to new
agreements on what constitutes reliable and valid descriptions of the data; the
categories used to organize those data; and the tools necessary to access, process,
and structure those data. Although communication researchers are well positioned
to move into these domains because of their long history of careful assessment
of the content of communication, some retooling will be inevitable as will
the need for collaborative research with computer scientists and engineers, and
the redirection of training for graduate students into the latest and most powerful
techniques for analyzing textual materials in electronic form. Visual materials still remain a challenge for computational analysis by researchers, but one that is
receiving increasing attention (Shah et al., this volume).
The focus on “text-as-data” in much of the work in computational social science
places the field of communication at the center of this evolving domain,
suggesting the rise of computational communication science. Attention to the
content of communications—how they are produced and how they are responded
to—is central to work in the field. As the articles in this volume show, communication
researchers are at the forefront of confronting the challenges of computational
social science and integrating insights and approaches from different
disciplines to answer the questions posed.

Computational social science: Making the links

Jon Kleinberg's early work was not for the mathematically faint of heart. His first publication1, in 1992, was a computer-science paper with contents as dense as its title: 'On dynamic Voronoi diagrams and the minimum Hausdorff distance for point sets under Euclidean motion in the plane'.

That was before the World-Wide Web exploded across the planet, driven by millions of individual users making independent decisions about who and what to link to. And it was before Kleinberg began to study the vast array of digital by-products generated by life in the modern world, from e-mails, mobile phone calls and credit-card purchases to Internet searches and social networks. Today, as a computer scientist at Cornell University in Ithaca, New York, Kleinberg uses these data to write papers such as 'How bad is forming your own opinion?'2 and 'You had me at hello: how phrasing affects memorability'3 — titles that would be at home in a social-science journal.

“I realized that computer science is not just about technology,” he explains. “It is also a human topic.”

Kleinberg is not alone. The emerging field of computational social science is attracting mathematically inclined scientists in ever-increasing numbers. This, in turn, is spurring the creation of academic departments and prompting companies such as the social-network giant Facebook, based in Menlo Park, California, to establish research teams to understand the structure of their networks and how information spreads across them.

“It's been really transformative,” says Michael Macy, a social scientist at Cornell and one of 15 co-authors of a 2009 manifesto4 seeking to raise the profile of the new discipline. “We were limited before to surveys, which are retrospective, and lab experiments, which are almost always done on small numbers of college sophomores.” Now, he says, the digital data-streams promise a portrait of individual and group behaviour at unprecedented scales and levels of detail. They also offer plenty of challenges — notably privacy issues, and the problem that the data sets may not truly be reflective of the population at large.

Nonetheless, says Macy, “I liken the opportunities to the changes in physics brought about by the particle accelerator, and in neuroscience by functional magnetic resonance imaging”.

Social calls

An early example of large-scale digital data being used on a social-science issue was a study in 2002 by Kleinberg and David Liben-Nowell, a computer scientist at Carleton College in Northfield, Minnesota. They looked at a mechanism that social scientists believed helped drive the formation of personal relationships: people tend to become friends with the friends of their friends. Although well established, the idea had never been tested on networks of more than a few tens or hundreds of people.

Kleinberg and Liben-Nowell studied the relationships formed in scientific collaborations. They looked at the thousands of physicists who uploaded papers to the arXiv preprint server during 1994–96. By writing software to automatically extract names from the papers, the pair built up a digital network several orders of magnitude larger than any that had been examined before, with each link representing two researchers who had collaborated. By following how the network changed over time, the researchers identified several measures of closeness among the researchers that could be used to forecast future collaborations5.
As expected, the results showed that new collaborations tended to spring from researchers whose spheres of existing collaborators overlapped — the research analogue of 'friends of friends'. But the mathematical sophistication of the predictions has allowed them to be used on even larger networks. Kleinberg's former PhD student, Lars Backstrom, also worked on the connection-prediction problem — experience that he has put to good use now that he works at Facebook, where he designed the social network's current friend-recommendation system.

Another long-standing social-science idea affirmed by computational researchers is the importance of 'weak ties' — relationships with distant acquaintances who are encountered relatively rarely. In 1973, Mark Granovetter, a social scientist now at Stanford University in Stanford, California, argued that weak ties form bridges between social cliques and so are important to the spread of information and to economic mobility6. In the pre-digital era it was almost impossible to verify his ideas at scale. But in 2007, a team led by Jukka-Pekka Onnela, a network scientist now at Harvard University in Cambridge, Massachusetts, used data on 4 million mobile-phone users to confirm that weak ties do indeed act as societal bridges7 (see 'The power of weak ties').

In 2010, a second group, which included Macy, showed that Granovetter was also right about the connection between economic mobility and weak ties. Using data from 65 million landlines and mobile phones in the United Kingdom, together with national census data, they revealed a powerful correlation between the diversity of individuals' relationships and economic development: the richer and more varied their connections, the richer their communities8 (see 'The economic link'). “We didn't imagine in the 1970s that we could work with data on this scale,” says Granovetter.

Infectious ideas

In some instances, big data have showed that long-standing ideas are wrong. This year, Kleinberg and his colleagues used data from the roughly 900 million users of Facebook to study contagion in social networks — a process that describes the spread of ideas such as fads, political opinions, new technologies and financial decisions. Almost all theories had assumed that the process mirrors viral contagion: the chance of a person adopting a new idea increases with the number of believers to which he or she is exposed.
Kleinberg's student Johan Ugander found that there is more to it than that: people's decision to join Facebook varies not with the total number of friends who are already using the site, but with the number of distinct social groups those friends occupy9. In other words, finding that Facebook is being used by people from, say, your work, your sports club and your close friends makes more of an impression than finding that friends from only one group use it. The conclusion — that the spread of ideas depends on the variety of people that hold them — could be important for marketing and public-health campaigns.

As computational social-science studies have proliferated, so have ideas about practical applications. At the Massachusetts Institute of Technology in Cambridge, computer scientist Alex Pentland's group uses smartphone apps and wearable recording devices to collect fine-grained data on subjects' daily movements and communications. By combining the data with surveys of emotional and physical health, the team has learned how to spot the emergence of health problems such as depression10. “We see groups that never call out,” says Pentland. “Being able to see isolation is really important when it comes to reaching people who need to be reached.” Ginger.io, a spin-off company in Cambridge, Massachusetts, led by Pentland's former student Anmol Madan, is now developing a smartphone app that notifies health-care providers when it spots a pattern in the data that may indicate a health problem.

Other companies are exploiting the more than 400 million messages that are sent every day on Twitter. Several research groups have developed software to analyse the sentiments expressed in tweets to predict real-world outcomes such as box-office revenues for films or election results11. Although the accuracy of such predictions is still a matter of debate12, Twitter began in August to post a daily political index for the US presidential election based on just such methods (election.twitter.com). At Indiana University in Bloomington, meanwhile, Johan Bollen and his colleagues have used similar software to search for correlations between public mood, as expressed on Twitter, and stock-market fluctuations13. Their results have been powerful enough for Derwent Capital, a London-based investment firm, to license Bollen's techniques.

Message received

When such Twitter-based polls began to appear around two years ago, critics wondered whether the service's relative popularity among specific demographic groups, such as young people, would skew the results. A similar debate revolves around all of the new data sets. Facebook, for example, now has close to a billion users, yet young people are still overrepresented among them. There are also differences between online and real-world communication, and it is not clear whether results from one sphere will apply in the other. “We often extrapolate from how one technology is used by one group to how humans in general interact,” notes Samuel Arbesman, a network scientist at Harvard University. But that, he says, “might not necessarily be reasonable”.

Proponents counter that these are not new problems. Almost all survey data contain some amount of demographic skew, and social scientists have developed a variety of weighting methods to redress the balance. If the bias in a particular data set, such as an excess of one group or another on Facebook, is understood, the results can be adjusted to account for it.

“We didn't imagine in the 1970s that we could work with data on this scale.”

Services such as Facebook and Twitter are also becoming increasingly widely used, reducing the bias. And even if the bias remains, it is arguably less severe than that in other data sets such as those for psychology and human behaviour, where most work is done on university students from Western, educated, industrialized, rich and democratic societies (often denoted WEIRD).

Granovetter has a more philosophical reservation about the influx of big data into his field. He says he is “very interested” in the new methods, but fears that the focus on data detracts from the need to get a better theoretical grasp on social systems. “Even the very best of these computational articles are largely focused on existing theories,” he says. “That's valuable, but it is only one piece of what needs to be done.” Granovetter's weak-ties paper6, for example, remains highly cited almost 40 years later. Yet it was “more or less data-free”, he says. “It didn't result from data analyses, it resulted from thinking about other studies. That is a separate activity and we need to have people doing that.”

The new breed of social scientists are also wrestling with the issue of data access. “Many of the emerging 'big data' come from private sources that are inaccessible to other researchers,” Bernardo Huberman, a computer scientist at HP Labs in Palo Alto, wrote in February14. “The data source may be hidden, compounding problems of verification, as well as concerns about the generality of the results.”

A prime example is Facebook's in-house research team, which routinely uses data about the interactions among the network's 900 million users for its own studies, including a re-evaluation of the famous claim that any two people on Earth are just six introductions apart. (It puts the figure at five15.) But the group publishes only the conclusions, not the raw data, in part because of privacy concerns. In July, Facebook announced that it was exploring a plan that would give external researchers the chance to check the in-house group's published conclusions against aggregated, anonymized data — but only for a limited time, and only if the outsiders first travelled to Facebook headquarters16.

In the short term, computational social scientists are more concerned about cultural problems in their discipline. Several institutions, including Harvard, have created programmes in the new field, but the power of academic boundaries is such that there is often little traffic between different departments. At Columbia University in New York, social scientist and network theorist Duncan Watts recalls a recent scheduling error that forced him to combine meetings with graduate students in computer science and sociology. “It was abundantly clear that these two groups could really use each other: the computer-science students had much better methodological chops than their sociology counterparts, but the sociologists had much more interesting questions,” he says. “And yet they'd never heard of each other, nor had it ever occurred to any of them to walk over to the other's department.”

Many researchers remain unaware of the power of the new data, agrees David Lazer, a social scientist at Northeastern University in Boston, Massachusetts, and lead author on the 2009 manifesto. Little data-driven work is making it into top social-science journals. And computer-science conferences that focus on social issues, such as the Conference on Weblogs and Social Media, held in Dublin in June, attract few social scientists.

Nonetheless, says Lazer, with landmark papers appearing in leading journals and data sets on societal-wide behaviours available for the first time, those barriers are steadily breaking down. “The changes are more in front of us than behind us,” he says.

Certainly that is Kleinberg's perception. “I think of myself as a computer scientist who is interested in social questions,” he says. “But these boundaries are becoming hard to discern.”

Computational Social Science and Sociology
INTRODUCTION
The rise of the Internet and the mass digitization of administrative records and historical archives
have unleashed an unprecedented amount of digital data in recent years. Unlike conventional
datasets collected by social scientists, these new digital sources often provide rich detail about the
evolution of social relationships across large populations as they unfold (Bail 2014, Golder & Macy
2011, Lazer et al. 2009, Salganik 2018). Meanwhile, a variety of new techniques are now available
to analyze these large, complex datasets. These include various forms of automated text analysis,
online field experiments, mass collaboration, and many others inspired by machine learning (Evans
& Aceves 2016, Molina & Garip 2019, Nelson 2017, Salganik 2018). The rapid increase in digital
data—alongside new methods to analyze them—has given birth to a new interdisciplinary field
called computational social science.
The term computational social science emerged in the final quarter of the twentieth century
within social science disciplines as well as science, technology, engineering, and mathematics
(STEM) disciplines. Within social science, the term originally described agent-based modeling—
or the use of computer programs to simulate human behavior within artificial populations (Bruch
& Atwell 2015, Macy & Willer 2002). This work led to fundamental theoretical advances in
the study of social psychology, network analysis, and many other subjects (e.g., Baldassarri &
Bearman 2007, Centola & Macy 2007, Watts 1999). Within STEM fields, by contrast, any
study that employs large datasets that describe human behavior is often described as computational
social science (e.g., Helbing et al. 2000, Pentland 2015). Although many of these studies
applied elegant theories from physics and mathematics to analyze collective dynamics such as
crowd behavior, they were largely disconnected from social science theory (see McFarland et al.
2015)—despite early efforts to synchronize them (e.g., Carley 1991, Macy & Willer 2002).
Acknowledging its diverse origins across multiple disciplines (Lazer et al. 2009), we provide the
following definition of the field for this review: Computational social science is an interdisciplinary
field that advances theories of human behavior by applying computational techniques to large
datasets from social media sites, the Internet, or other digitized archives such as administrative
records. Our definition forefronts sociological theory because we believe the future of the field
within sociology not only depends on novel data sources and methods, but also its capacity to
produce new theories of human behavior or elaborate on existing explanations of the social world.
Although we applaud recent calls for solution-oriented social science that aims to predict human
behavior for practical purposes (Macy 2016, Watts 2017), our review examines only studies that
also aim to explain human behavior to advance social science theory.1 Readers should take note,
however, that ours is not a consensus view among computational social scientists—and particularly
those outside of sociology. Such a consensus might not be possible, however, given the remarkably
rapid growth of the field across so many disciplines.
Another defining feature of our review is our focus on the evolution of computational social
science within the discipline of sociology. Because of space limitations, we are unable to provide a
comprehensive overview of computational social science across parallel social science disciplines
such as political science or economics that may be of interest to sociologists. Although earlier reviews
have examined the growth of new data sources or methods within the field of computational
social science (Bail 2014, Evans & Aceves 2016, Golder & Macy 2014, Molina & Garip 2019), our
goal is to map how such tools are currently being applied in empirical settings across sociology.
Using a combination of bibliometric methods and in-depth analyses of individual studies, our review
surveys work that is addressing longstanding questions about human behavior that predatethe field, as well as new questions about human behavior that have emerged alongside the rapid
integration of digital data into nearly every quarter of our lives.
Our principal conclusion is that computational social science is spreading rapidly into many
different subfields within sociology. Our review, however, focuses on seven substantive areas where
at least five studies have been published that meet our definition of the field. These include: (a) social
networks and group formation; (b) collective behavior and political sociology; (c) the sociology
of knowledge; (d) cultural sociology, social psychology, and emotions; (e) the production of culture;
( f ) economic sociology and organizations; and (g) demography and population studies. Work in
these areas appears in flagship journals, inspires panels at major conferences, and helps communicate
the public value of sociology as well. Nevertheless, our review also identifies numerous new
challenges that range from ethics to the increasing opacity of data production in digital spaces.We
conclude by identifying promising lines of inquiry for future studies, more effectively integrating
sociological theory into the agenda of computational social science, and calling for increased engagement
with other social science fields to ensure the continued spread of computational social
science into mainstream sociology.
MAPPING THE FIELD
One of the many domains where data digitization has exploded in recent years is scholarly archives.
This, combined with recent innovations in automated text analysis, inspired us to use the tools of
computational social science to map the field itself. Our analysis is multi-faceted—combining data
from a large bibliometric database, data from popular conferences in the field, and multiple stages
of human coding—to identify all articles by sociologists that meet the definition of computational
social science we proposed above.
We began by querying the Web of Science, the largest database of scholarly publications currently
available.2 Unfortunately, this resource omits most books and does not cover all journals—
particularly some conference proceedings, where a considerable amount of computational social
science research appears. Still, it provides perhaps the best possible point of departure currently
available. Our sampling strategy began by searching for all mentions of the terms “computational
social science” or “big data” (a neologism that was used in the early days of the field) in the title,
abstract, or keywords of articles designated with a social science classification by the database.3
Because computational social science scholarship often appears in interdisciplinary journals that
might not receive a social science designation, we also searched for our keywords in five additional
publications that feature prominent research in the field: Science, Nature, Proceedings of the National
Academy of Sciences, Science Advances, and Nature Human Behavior. To further improve the breadth
of our sample, we collected the names of all scholars who presented publications or posters at the
largest conference in the field: the International Conference in Social Science (2018). Next, we
collected all articles written by these individuals within the Web of Science database and added
them to our sample. Finally—to capture influential articles that emerged before our keywords
became popular—we inspected the 300 most-cited articles within our database and identified 79
additional articles that met our definition of the field.
Before we present our results for the discipline of sociology, we provide an overview of the
evolution of computational social science across a broader set of fields. Figure 1 is a time series
graph that describes the number of publications within five scholarly disciplines where scholarship
mentioned the terms computational social science or big data between 2000 and 2016. This
figure should be taken as a rough approximation of the field, given that individual articles were
not reviewed by human coders to confirm that they are on the subject of computational social
science. Still, several things are noteworthy about this figure. First, there has been a remarkable
explosion of work in computational social science across many different fields since 2012. Second,
the disciplines in which this research agenda is most active are clearly business, psychology, and
education. Although computational social science is growing more slowly in sociology according
to this broad view, there has been an exponential increase in work by sociologists since 2010 as
well.
Next, we produced a citation network of all articles by discipline (see Figure 2). Each node
describes a paper in our corpus, and edges between papers indicate that they cite each other.4 We
identified 24 areas of scholarship using the Louvain community detection algorithm. Four large
communities can be identified at the core of this network. The first large community connects
communications, sociology, and political science. The second large community is primarily comprised
of work in geography and communications. The third sizeable community ties together
business and library science. A fourth, notable community connects business, finance, and law. Although
sociology is prominently featured in the center of this visualization, it also appears within
another community connected to anthropology and business management. We highlight with
pink shading in Figure 2 communities in which sociology is a prominent member.
Although the figures above provide a useful panorama of computational social science, all
keyword-based sampling procedures suffer from both false positives and false negatives. Because
our focus in the remainder of this review is on sociology, we took additional steps to ensure the
accuracy of our sample within this discipline. First, we obtained a crowd-sourced list of scholars
who work in the field of computational social science produced by participants of the Summer
Institutes in Computational Social Science (SICSS)—a major training event in the field funded
by the Russell Sage Foundation and the Alfred P. Sloan Foundation. We then identified all sociologists
on this list, collected their curricula vitae, and added to our database any articles that met
our definition of the field. Second, we hand-coded a refined list of articles in our database classified
as sociology to remove false positives, which yielded a total of 248 articles. The remainder of
our article discusses the seven subfields of sociology where we were able to identify at least five
articles within this database that employ both digital data and computational methods to develop
theories of human behavior.
SOCIAL NETWORKS AND GROUP FORMATION
One of the first areas where computational social science emerged in sociology is the study of social
networks and group formation. This is not surprising given that conventional research techniques
such as surveys struggle to capture the evolution of social relationships in situ. Although
many early studies employed agent-based models for these reasons, data from Internet sites, social
media platforms, and telecommunications inspired some of the first population-level research in
the field. Watts, for example, employed email data to demonstrate and elaborate on core social
science theories in digital spaces such as the principle of six degrees of separation (Dodds et al.
2003, Watts 2004), network dynamics and equilibrium (Kossinets & Watts 2006), and opinion
leadership (Watts & Dodds 2007). Similarly, Macy and colleagues used telecommunications data
to demonstrate a strong relationship between network diversity and economic development and
various other theories of group formation (e.g., Eagle et al. 2010).
More recent studies employ digital data sources to examine the diffusion of complex contagions
in social networks. Centola (2010), for example, created an online community where he
could control the topology of social networks. This work shows that diffusions were much more
likely to reach individuals in tightly clustered networks than those that were organized at random.
Other research showed that network topology can also shape the adoption of social conventions
(Centola & Baronchelli 2015). Social media and telecommunications data also enable analyses of
networks with unprecedented scale. Bail et al. (2017) employ Facebook data to identify synergy
between the diffusion of emotional and rational communicative styles in a large network of people
discussing public health issues. In another study, Bail et al. (2019) use Google Search data
to track the diffusion of cultural products across global networks. Together, these studies show
that microlevel interactions between individuals can generate macrolevel diffusion patterns anticipated
by early social science theorists such as Gabriel de Tarde. Park et al. (2018) study extremely
long-range ties using data produced on Twitter and via international phone calls. Their findings
challenge the long-held belief that social relationships that connect clusters of individuals within
networks are usually weak (i.e., based on acquaintances rather than close friends). In contrast, they
show that very long-range network connections—although rare—are often as strong as those that
connect a close circle of friends.
Another line of research examines network dynamics via online games. Shirado & Christakis
(2017) recruited participants to play a game that required them to coordinate colors within a matrix.
By introducing bots that simulated human agents playing the game poorly, they were able to improve coordination among human respondents in the study. In another experiment where respondents
occupied hypothetical neighborhoods and were asked to share Wi-Fi with each other,
Shirado et al. (2019) examined how network brokerage shapes inequality. They find that wellconnected
individuals can suffer when too many others come to depend on them for services.
Online games have also been influential in studying the wisdom of crowds and political beliefs.
Guilbeault et al. (2018), for example, show that diverse political groups produce more accurate
estimates of political facts if they are anonymous to each other, but less accurate estimates if their
political identities are revealed to each other. Becker et al. (2019) show that the wisdom of crowds
also improves estimation in politically homogeneous settings. Finally, games have been used to
study collective dynamics more broadly. Centola et al. (2018) created a game where groups of respondents
were asked to determine the appropriate names for avatars. They showed that network
properties enable small factions of study participants to overturn pre-existing consensus among
prevailing majorities.
COLLECTIVE BEHAVIOR AND POLITICAL SOCIOLOGY
Computational social science has inspired considerable work on collective behavior and politics.
Data from social media and other communication platforms have advanced the study of collective
behavior in particular. The Egyptian Revolution, the Indignados movement, and Occupy Wall
Street, for example, show that digital tools now play a central role in protests and that data generated
by these tools can inform studies of collective political efforts (Tufekci & Wilson 2012).
Twitter has become a focus of research in this area because it provides large datasets that can
be used to study how information spreads across networks. Case studies by González-Bailón and
colleagues indicate peripheral users in movement networks can generate large cascades of information,
but that leadership and hierarchy create larger information flows (Barberá et al. 2015,
González-Bailón et al. 2013, González-Bailón & Wang 2016). However, some scholars have asked
whether such findings can be generalized to other settings. Analyzing data from a Facebook application,
Lewis et al. (2014) find that online supporters are less committed to movements than
activists on the ground. Still, others find strong correspondence between online and offline activity
among social movements (Abul-Fottouh & Fetner 2018, Hanna 2013). Even if online data
do not capture all offline processes, they often provide a useful supplement to conventional data
sources—and for an increasing number of movements may be the only source of information
available (Zhang & Pan 2019).
Computational research has helped develop theories related to mobilization and behavior
change in social movements as well. Vasi et al. (2015) use data from social media and Google
to show how both online discussions about hydraulic fracturing and mobilizations against the
practice increased in communities where screenings of a documentary took place. This illustrates
how cultural artifacts might be used in social movements to motivate activists and shape public
opinion. Online experiments have also proven valuable for testing existing theories about mobilization.
Centola and colleagues’ network experiments, described above, highlight the importance
of critical masses for spreading new social conventions (Centola et al. 2018). van de Rijt’s and others’
studies leverage Change.org, an online petitioning platform, to conduct field experiments in
which samples of petitions are assigned different quantities of signatures as treatments (Vaillant
et al. 2015; van de Rijt et al. 2014, 2016). These experiments demonstrate the importance of cumulative
advantage effects in mobilization (van de Rijt et al. 2016) and the potential for initially
unpopular campaigns to encounter unexpected revivals (Vaillant et al. 2015).
Elsewhere, the proliferation of text-based data has inspired new work on political discourse.
Recent work by Bail (2015) and Bonikowski & Gidron (2016) demonstrates how fringe discourse can enter the mainstream by analyzing the social position of actors within discursive fields and
the emotional valence of their language. Scholars also use text data to examine how certain discursive
styles prove more effective than others for advocacy groups (Bail 2015, 2016a; Bail et al.
2017), as we discuss in additional detail below. Other studies examine the interaction between political
discourse and policy change. Flores (2017), for example, uses text data from Twitter to show
how anti-immigration laws hardened public opinion against immigration in Arizona. Text-based
research has also examined how elites shape political discourse—both across elite fields (e.g., journalism,
politics, and celebrities) and across countries (AlMaghlouth et al. 2015, Wells et al. 2016).
Finally, many studies examine political polarization and persuasion using online data, automated
text analysis, experiments, and agent-based modeling. These studies investigate how
political tribes form and highlight the role of “echo chambers” that create selective exposure to
information. Network-based research suggests that polarization arises from homophily among
conservatives and extremists (Boutyline & Willer 2017) as well as more general homophily and
peer influence processes (DellaPosta et al. 2015). Research using other methods notes that patterns
in donations to candidates have grown more polarized in recent decades (Heerwig 2017) and that
donations from elites drive belief polarization (Farrell 2016a). Computational sociologists have
also performed experiments to learn how partisans can change their beliefs. As noted above, Becker
et al. (2019) find that politically homogeneous groups make better decisions in online games. At
the same time, other research reveals that exposure to opposing views can create backfire effects.
Bail et al. (2018b) paid Twitter users to follow bots that exposed them to opposing political views
and found this treatment increased partisanship. Still, other studies indicate that minimizing signals
of partisan identity (Guilbeault et al. 2018), using specific moral language (Feinberg & Willer
2015), and matching the linguistic styles (Romero et al. 2015) may help reduce polarization.
SOCIOLOGY OF KNOWLEDGE
Computational social science has become a central part of the sociology of knowledge. One strand
of research employs citation data to study consensus formation within science. Building on Shwed’s
& Bearman’s (2010) influential study, Light & Adams (2015) analyze citation networks to determine
how consensus about the outcomes of children with same-sex parents arises among scholars.
Focusing on temporal patterns, they identify the point when the scientific consensus emerged that
children of same-sex parents show no marked differences in their sexual orientation compared to
those from other parental configurations. Bruggeman et al. (2012) demonstrate the importance of
differentiating between citations that signal agreement and citations that signal disagreement. Using
simulations, they find that small proportions of citations that are contentious have significant
effects on whether citation networks signal consensus.
Now that bibliographic data have become easier to collect and analyze at scale, scholars can
map and model processes that shape scientific fields as a whole. The Metaknowledge program of
University of Chicago sociologist James Evans exemplifies such research (e.g., Evans & Foster
2011). This includes studies that trace the increasing dominance of teamwork in science. For example,
Wuchty et al. (2007) show that, in the social sciences, the propensity to work in teams has
more than doubled over the past five decades. Others show that small teams tend to produce work
that introduces novel and disruptive ideas in science and technology, whereas large teams tend to
develop existing ideas further (Wu et al. 2019). Advances in text analysis have enabled scholars to
shed new light on similarities and differences between disciplines as well (e.g., Evans et al. 2016,
McMahan & Evans 2018, Vilhena et al. 2014). For example, McMahan & Evans (2018) develop
a measure to capture the ambiguity of language within scientific articles. They find that expressions
are used most consistently in the biological and chemical sciences and least consistently in the humanities, law, and environmental sciences. Ambiguous language also produces more integrated
citation streams, which stimulates more involved academic debate. Vilhena et al. (2014)
draw on the concept of cultural holes to map differences in disciplinary jargon within and across
fields. Although these language-based gaps do not map neatly onto structural holes within citation
networks, they nevertheless inhibit efficient communication between scientists. Shi et al. (2015)
go a step further and model the generative process underlying an entire field. They show that
biomedical research can be conceptualized as a dynamic network that evolves according to the
ways scientists link theory and methods across time.
Another strand of literature examines how academic work gains impact and prestige. Uzzi et al.
(2013) for example, show that high-impact publications build on prior work yet simultaneously
feature unusual and novel combinations. Within sociology, Leahey & Moody (2014) show that
articles that span subfields garner more citations and are more likely to appear in prestigious journals.
Still others have created large databases on academic prizes (e.g., Li et al. 2019). Using such
data, Ma & Uzzi (2018) study the worldwide network of academic prize-winners over 100 years.
They show that a relatively small number of prizes and scholars push the boundaries of science,
whereas the increasing number of prizes remains concentrated in a small circle of scientific elites.
Computational approaches also reveal how scientific discoveries are driven by the choices of
individual scientists and their organizational contexts. Rzhetsky et al. (2015), for example, identify
the strategies that scientists in biomedicine follow in choosing which relationships between
molecules to study. They show these strategies reflect personal career choices and demonstrate
that increased risk-taking and publishing of research failures could improve scientific discovery
in this field as a whole (see also Foster et al. 2015). Others focus on organizational context. For
example, Rawlings et al. (2015) and Rawlings & McFarland (2011) identify how the organizational
structure of one prominent university shapes peer influence in academic funding proposals, noting
the increasing dominance of a few faculty members in the flow of knowledge as measured in
terms of their citation histories.
Another strand of work in this area employs computational techniques to study the public
face of science and how science interfaces with industry. For example, Shwed (2015) shows how
attempts by the tobacco industry to hamper scientific inquiry paradoxically enabled scientists to
discover the perils of smoking. Farrell (2016a,b) examines how private-sector influence shaped the
debate about anthropogenic climate change. His analysis reveals how corporate funding influences
both the content and prevalence of polarizing themes in this debate over time. Still others combine
network analysis with text analysis to better understand how scientists position themselves
in public debates. For example, Edelmann et al. (2017) study the controversy about the use of
potentially pandemic pathogens in biomedical research. They find that peer effects and research
specializations shape the positions scientists publicly support in this debate. Scholars have also
explored the public consumption of science by turning toward online purchasing data. Studying
millions of online book purchases, for example, Shi et al. (2017) reveal partisan interests in science.
Consumers of liberal-leaning political books prefer basic science, whereas customers of conservative
books tend to prefer applied, commercial science. This implies that science might both bridge
and reinforce political differences within the general public.
Finally, computational work reveals gender asymmetries and inequalities within science and
the representation of knowledge online more generally. For example, drawing on data from Jstor
(one of the largest digital repositories of academic journals), West et al. (2013) trace gender inequalities
in scholarly authorship across the natural sciences, social sciences, and humanities since
1545. Even when women and men appear to have similar publication counts, this study shows that
men continue to dominate single-authored papers or prestigious first or last author positions. In
another study, King et al. (2017) show that men are more likely to cite themselves than women—a tendency that increased over the past two decades. Finally, Wagner et al. (2016) focus on the representation
of women on Wikipedia. They find that ceiling effects curb the entry of women and
identify linguistic differences in how achievements of women and men are described—as well as
differences in meta-data that imply asymmetries in the wider reception of articles written by men
and women online.
CULTURAL SOCIOLOGY, SOCIAL PSYCHOLOGY, AND EMOTIONS
Computational social science also appears in the study of cultural sociology, social psychology, and
emotions. Within cultural sociology, several studies examine broad processes of cultural change.
Bail (2015) shows how automated text analysis can be used to examine the transformation of discursive
fields during what Ann Swidler calls “unsettled times” in order to develop a systematic
theory of resonance—or why certain cultural messages have a natural advantage over others. In
follow-up work, Bail (2016b) identifies a “carrying capacity” within discursive fields: Messaging
strategies attract more attention if they cover a diverse range of subjects, but messages that combine
too many themes are less impactful. Bail (2016a) also develops a theory of “cultural bridging”
which indicates organizations that adopt brokerage positions within discursive fields are more
likely to attract large audiences. Other studies document similar cultural processes in scientific
fields, markets, and corporations (Goldberg et al. 2016a,b, Vilhena et al. 2014), as we discuss elsewhere
in this article. Finally, Kozlowski et al. (2019) use word embeddings to examine large-scale
shifts in the meaning of terms associated with class and gender in the United States and United
Kingdom over time.
The growth of text-based data has also opened new lines of inquiry about how cultural messages
are expressed. Much of this research involves data collected from Facebook or Twitter. For
example, Golder & Macy (2011) track the frequency of language associated with different types
of emotions and discovered evidence of diurnal rhythms. Other work reveals emotional contagion
in public discussions about public health issues on Facebook, or the likelihood that exposure to
emotional language makes social media users more likely to become emotional themselves—and
thus more likely to interact with emotional content as well (Bail 2016c). Bail et al. (2017) identify
what they call “cognitive-emotional currents” in such discussions, or alternating surges of rational
and emotional styles of language. This work thus feeds into broader discussions about the role of
“hot” and “cold” processing within the human brain and explains how social context can shape
how expressive styles spread across social networks over time.
In addition to text analysis, scholars are pioneering the use of virtual reality to further examine
core questions in social psychology. For example, van Loon et al. (2018) paid university students to
perform collaborative tasks, but randomized half of them into a treatment condition where they
used virtual reality to take the perspective of the other research participants. This intervention
increased prosocial behavior. Another line of research contributes to theories of phenomenology
and small-group processes using virtual reality (Schroeder 2010). Although virtual reality-based
research is exploding in adjacent social science fields such as psychology, sociology has been slow
to integrate this new technology. This is surprising not only because virtual reality holds great
promise for studying how people respond to social settings within tightly controlled experiments,
but also because sociologists conducted some of the earliest research on virtual communities (e.g.,
Gamson & Peppers 1966).
PRODUCTION OF CULTURE
Computational approaches are often used to study how people evaluate cultural products such as
music, art, or films. In an influential study, Salganik et al. (2006) created an online “music lab”
to study how peer influence shapes musical preferences of new artists. In their experiment, participants
rated music from obscure bands. In the control condition, participants received no information
about its popularity, whereas those in the treatment condition were shown how many
times songs were downloaded. Treated respondents were more likely to listen to songs with high
download rates and to rate such songs more positively. Although most songs in the treatment
condition followed a self-fulfilling prophecy—where false popularity became real over time—a
follow-up study revealed the highest-quality songs recovered popularity over time (Salganik &
Watts 2008, van de Rijt 2019). Other studies explore the dynamics of critical acclaim. For example,
previous work finds the age of music or film, its genre, and sponsorship by a major production
company increase the likelihood of industry accolades for both music and film (Light & Odden
2017, Rossman & Schilke 2014).
Computational work also measures how combinations of themes in cultural products shape
their reception. Using data from Spotify and Billboard, Askin & Mauskapf (2017) find songs perform
best when they sound similar to songs that were on the chart the year before but include
minor diversions from such themes as well. Other research suggests tastes for atypicality vary
across demographics, such as class, race, and geographical location. Some people prefer products—
such as food and film—that fit cleanly into a single category, whereas others prefer novel combinations
of themes from multiple categories. Those that prefer items from single categories at a
time represent the traditional omnivore hypothesis. They have a taste for authenticity in multiple
categories—no categorical mixing—which signals high status (Goldberg 2016a). Yet studies also
indicate tastes are stratified by regional differences, as interest in certain ingredients and recipes
is influenced by location (Wagner et al. 2014). By considering the characteristics of products and
consumers in tandem, researchers are developing a more comprehensive theory of how taste and
consumption patterns align.
Another line of studies examines how social networks between individuals shape the creation
of cultural objects—often with a focus on novelty and innovation. For instance, de Vaan et al.
(2015) use a large database of video-game production credits to demonstrate that cross-cutting
teams of developers create products that are both more creative and popular. Analyzing networks
of Hollywood actors, Rossman et al. (2010) show how status is transferred between people who
work together. In this case, an actor is more likely to receive an Oscar nomination—an important
consecration event in the film industry—when they appear in a film alongside a high-status actor.
Finally, recent studies examine how gender, class, and political identities shape the production
of cultural products. Shor et al. (2015), for example, find men are featured more often in the media
than women because of journalists’ focus on high-status topics that typically focus on men. A largescale
historical analysis of Google Books data shows that mentions of social class, class struggle, and
other terms that describe social class increase alongside the economic misery index—a measure of
inflation and unemployment (Chen & Yan 2016). Finally, Hoffman (2019) uses a combination of
text analysis and network analysis to study how reading patterns shape political ideology and vice
versa using records from a large public library.
ECONOMIC SOCIOLOGY AND ORGANIZATIONS
The intersection of computational and network methods is also flourishing within economic sociology.
Email and instant messaging archives create large, dynamic networks of individuals in
organizations—a significant improvement on data generated via self-reports. Early exchanges
among individuals and organizations—for instance, patient transfers between hospitals—shape
the evolution of social networks and the allocation of resources (Horvát et al. 2015, Kitts et al.
2017). Elsewhere, instant messaging data reveal that social balance theory predicts streaks of high performance among day traders (Askarisichani et al. 2019). Messaging data also allow researchers
to measure the level of coordination among employees in economic decision making and show
that synchronous communication increases the profitability of trades (Saavedra et al. 2011). Finally,
network data have improved the study of social capital and career outcomes. For example,
studies indicate certain network compositions benefit women differently than men (Lutter 2015).
For example, women are more likely to succeed if they are part of an inner circle within a network
that is predominantly female (Yang et al. 2019). Network data are not only useful for studying
exchange and career outcomes, but also organizational culture. Goldberg et al. (2016b) analyze
email messages within a large corporation to measure whether employees’ communications fit
within prevailing norms and behavior. Employees who integrate themselves into organizational
culture receive a variety of rewards and are less likely to be fired (Srivastava et al. 2018).
In addition to network data, digitized records of textual communications provide a richer understanding
of the role of culture and emotions in the marketplace. Using text analysis, studies
reveal how market volatility influences whether traders are discussing current or future conditions,
and how these shifts shape trading behavior (Saavedra et al. 2011). Other studies indicate
individuals who exhibit moderate levels of emotions in their communication with other employees
make the most profitable stock trades (Liu et al. 2016). Finally, Schnable (2016) combines qualitative
methods with automated text analysis to study how religion provides grassroots NGOs with
cultural frames that allow them to claim legitimacy, build social networks, and secure funding and
resources.
DEMOGRAPHY AND POPULATION STUDIES
Computational social science has emerged within demography only relatively recently, but it is
rapidly gaining popularity (Cesare et al. 2018). Unsurprisingly, computational approaches are most
often used to produce high-quality population estimates. Mobile phone data, for example, are
employed to produce more dynamic population estimates—particularly in areas where national
statistics are not reliable (Cesare et al. 2018, Eagle et al. 2010, Palmer et al. 2013). Others use
Google Street View and deep learning to estimate demographic characteristics of neighborhoods
(Gebru et al. 2017), crowd-sourcing techniques to measure demographics of social media populations
(McCormick et al. 2017), websites to map large-scale genealogy trees (Kaplanis et al. 2018),
and online image data to predict age (Helleringer et al. 2019). Computational approaches are also
being developed to study the “holy trinity” of demography: fertility, mortality, and migration. To
study fertility and mortality, studies employ Google Search data and Facebook data (Hobbs et al.
2016, Kashyap & Villavicencio 2016, Ojala et al. 2017). Other studies use data from Twitter and
LinkedIn to develop more accurate estimates of migration within and between countries (Palmer
et al. 2013, State et al. 2014, Zagheni et al. 2017).
Demographers also use computational approaches to probe the microdynamics of population
processes such as dating and marriage. Several recent studies use data from a large Internet dating
platform to examine how relationships form across racial and ethnic divides. Lewis (2013) finds
strong evidence of racial homogamy within Internet dating, but also shows that if people are
invited on a date by someone they do not know, they are more likely to go on the date if the
person is from another racial or ethnic group. Lin & Lundquist (2013), however, find that women
are more likely to respond to dating invitations from members of dominant racial and ethnic
groups, regardless of social distance. Outside of the US context, Potârca & Mills (2015) identify ˘
very strong racial and ethnic homogamy across European countries—and particularly those with
ethnically homogeneous populations. More recent work by Bruch and colleagues (Bruch et al.
2016, Bruch & Newman 2018) examines how population-level factors—such as the number of single men or women in the city—shape individual decision-making processes above and beyond
physical attraction.
A final strand of computational social science research by demographers uses digital data
sources to study socially undesirable health behaviors and enumerate populations that are difficult
to access. For example, Kashyap & Villavicencio (2016) employ Google Search data to study
the prevalence of selective abortion in India. Bail et al. (2018a) use Google Search data to examine
how demographic factors contribute to violent radicalization. Moreno et al. (2012) use Facebook
data linked to surveys to measure the prevalence of binge drinking in US colleges. Chakrabarti &
Frye (2017) use text analysis of diary data to study AIDS prevention. Araujo et al. (2017) track the
prevalence of lifestyle diseases in 47 countries using Facebook’s advertising tools. Other studies
leverage these same data to revisit core questions in demography about migration, fertility, and
gender segregation (Fatehkia et al. 2018, Rampazzo et al. 2018, Stewart et al. 2019). Stewart et al.
(2019), for example, study the cultural assimilation of undocumented immigrants in the southern
United States by tracking the size of audiences interested in non-US soccer teams.
Other scholars have developed entirely new models for population research inspired by conventions
within the field of data science. Within these fields, it is common to create a competition
wherein multiple teams compete to build models for a cash prize. A popular example is the socalled
Netflix Prize, for which teams of data scientists competed to build a model that provides the
most high-quality recommendations of what Netflix users might like to watch. Princeton sociologist
Matthew Salganik coordinated the Fragile Families Challenge using a similar model that staggered
the release of a new wave of the Fragile Families and Child Wellbeing Study—a multi-wave
study of inequality and the family. He invited teams of researchers to develop models based on a
subsample of the new wave of the dataset—commonly referred to as a training dataset—to predict
outcomes of interest on the full dataset that was released later (Salganik et al. 2020, Lundberg et al.
2018). Although this effort did not produce major advances over extant models, it provided a critical
litmus test for machine learning within population science—and social science more broadly.
CONCLUSION
As this review illustrates, the field of computational social science is expanding in many exciting
new directions—indeed, it is expanding so rapidly that any review of the literature will become
outmoded within short order. Still, this preliminary review indicates that computational social
science has become a central part of research across many different subfields. Perhaps more importantly,
the field has moved far beyond the descriptive social media research that characterized
much early work in the field. Indeed, sociologists have developed a range of hybrid methodologies
that combine computational approaches with more conventional techniques to take advantage of
the promise of digital data sources while addressing their limitations as well. Yet sociologists are
also rapidly pursuing the cutting edge of new technologies as well. Although machine learning has
yet to have its watershed moment within sociology, artificial intelligence, bots, and virtual reality
already offer sociologists a powerful new suite of techniques to study how social relationships form
and evolve across contexts.
Using Computational Social Science to Build Theories of Human Behavior
Importantly, our review also indicates that sociologists are not only contributing new methods
for computational social science, but also major theoretical advances as well—which we view as
key to the continued expansion of the field into mainstream sociology. There are multiple ways in
which theoretical advances can be made via computational social science. First, many studies are
using new types of data and methods to revisit old sociological questions that were once thought impossible to study. Examples include new macrolevel theories of social networks and cultural
change as well as microlevel theories of human decision making. Yet such work only scratches
the surface of the full potential of computational social science to advance theory development
across multiple levels of analysis. In our view, the most influential work within computational
social science within the coming years will be the type that is able to link macro levels of theories
about topics such as cultural change to microlevel processes of decision making. Such efforts will
most likely not be possible with social media data, administrative records, or other new sources
of digital data alone. Rather, they will require creative hybrid methods that illuminate the space
between different levels of analysis.
A second way that computational social science can advance the theoretical progress of sociology
is to develop new theories of the social terra incognita created by the rise of digital technology
itself. These include novel theories of digital protest, consumption of cultural products,
and the spread of knowledge online—to name but a few. Many of the studies above have examined
these new spaces—particularly within the realm of politics, networks, and communication. But the
number of new social spaces—and new forms of social relationships they enable—are currently
outpacing the evolution of sociological theory. Perhaps the most obvious example is the manner
in which machine learning and artificial intelligence are currently recasting the way we communicate
with each other—which may ultimately create new forms of social segregation that shape the
evolution of our networks themselves. Sociologists have also left mostly untheorized the ways that
interaction between machines themselves can shape patterns of human behavior and social organization
as well. If sociologists do not rise to these new challenges, other fields will. Indeed, the
majority of theorizing in the broader field of computational social science outside of sociology is
either inattentive to sociological theory or focuses on a handful of influential ideas.We believe that
sociologists need to insert themselves into such conversations more proactively by demonstrating
how core tenets of sociological theory such as the social construction of reality or self-fulfilling
prophecies can advance the study of the many new social spaces created in recent decades.
Finally, sociologists can use the tools of computational social science to develop new ways of
creating theory itself. Although it is becoming increasingly clear that machine learning will not
easily allow us to reverse-engineer collective behavior, culture, or human decision making, we can
systematically use such tools to identify new dimensions of social behavior or test the robustness of
our existing accounts. This will probably proceed most naturally by comparing the gap between
the predictions produced by such models and the underlying reality—either on a variable-byvariable
basis or by systematically testing how permutations of different variables might enable
us to identify unforeseen complexity of human behavior or otherwise identify our blind spots.
Finally, scholars are also creating theories by interfacing with other fields. Comparing theories of
human behavior, computing, and artificial intelligence, for example, opens exciting new lines of
inquiry about the nature of social life (e.g., Foster 2018). Taken to the extreme, we can also use
this line of theoretical development to compare precisely how differences between artificial and
human intelligence help us develop deeper understanding of social processes—particularly those
that are not easily articulated or may be difficult for a machine to observe.
Outstanding Challenges and Directions for Future Research
Despite the tremendous potential of computational social science to advance sociological theory,
it also creates numerous pressing challenges. Many of the most valuable data sources in the
field—such as data from social media companies—are either inaccessible to researchers or only
accessible by those with high status. Although a variety of initiatives are underway to address this
problem (King & Persily 2019), the ongoing controversy about privacy and data within the public domain will make such collaborations difficult for years to come (and perhaps appropriately so).
This is because the field of computational social science also creates a range of complex ethical
issues (Salganik 2018). Among others, these include balancing the principles of open science and
protection of human subjects, linking information across datasets without violating confidentiality,
and a broader set of questions about how deeply researchers should probe into the online lives
of their subjects—particularly in cases where people may not know that the data they generate
online could be analyzed by others.
Another set of challenges concerns access to training in computational social science. Coding
in open-source software, embedding field experiments in online platforms, and dealing with unconventional
data structures is not part of regular training within most sociology departments.
Although numerous online resources offer training in such techniques, they nevertheless require
extensive domain knowledge and are not geared toward social scientists who aim to repurpose
digital data for the study of human behavior. The SICSS are designed to fill this gap, by providing
training to graduate students, postdoctoral fellows, and junior faculty in a range of techniques
including web scraping, automated text analysis, digital experimentation, ethics, mass collaboration,
and a range of other subjects. The SICSS also seed interdisciplinary research by connecting
young scholars in a range of social science and STEM fields in more than 11 locations around the
world and encouraging them to start research projects together. Although these events provide
a temporary solution to the lack of training in computational social science, we hope that more
departments will consider including such material within their core curricula.
Perhaps because of the issues of data access, ethics, and training just described, computational
social science has not yet been broadly integrated into many of the largest subfields in sociology.
These include race and ethnicity, criminology, education, inequality and stratification, religion,
sex and gender, law, medical sociology, historical sociology, religion, and many other subfields. It
is possible that computational social science has not yet permeated these subfields because the
initial wave of data created by new communication technologies were more conducive to pressing
questions in the fields we reviewed above. Yet there is tremendous potential to link such new
data sources—alongside the widespread digitization of historical and administrative archives—to
address longstanding questions outside of these fields as well. To give only a few examples, the
digitization of medical records might enable a host of new questions within medical sociology,
and new data on policing and crime creates fertile ground for new theorizing within criminology
as well (see Brayne 2020). In such cases, computational data and methods might be fruitfully
combined with the panoply of existing methods—from conventional surveys to ethnography.
Just as we hope future research will span the subfields of sociology, we also call for even greater
engagement with other disciplines where computational social science continues to blossom. As
the bibliometric data we presented at the outset of this article showed, research is exploding in
other social science disciplines as well as computer science and engineering. Sociologists are fortunate
to occupy a central position—if not the most central position—within this interdisciplinary
conversation. We believe that sociologists must continue to capitalize on this position—not only
because computational social science offers so much to sociology, but also because many of the
most pressing problems studied by computational social scientists are inherently sociological—
from the role of social networks in the spread of misinformation or the emergence of echo chambers
to the ways that algorithms create or reproduce social inequality.
DISCLOSURE STATEMENT
The authors are not aware of any affiliations, memberships, funding, or financial holdings that
might be perceived as affecting the objectivity of this review.ACKNOWLEDGMENTS
We thank Elizabeth Bruch, Nicolo Cavalli, Karen Cook, James Evans, Ridhi Kashyap, Matt
Salganik, and Emilio Zagheni for helpful comments on previous drafts. We thank Joshua Becker
and Alanna Lazarowich for providing data about the International Conference on Computational
Social Science.
Making Computational Social Science Effective
Recent advances in computational science and, in particular, the growing popularity of
agent-based modeling (ABM) are creating noteworthy excitement in the social sciences.
ABM is a significant advance over previously available modeling formalisms in that it
is an appropriate ontology for representing much of the knowledge and data that is available
about social actors and social systems. The agents that populate an ABM can be based on
individuals, groups, or institutions occurring in the natural system. The decision algorithms
used in these agents can be based on knowledge and data available regarding the decision
behavior of the associated individuals, groups, or institutions. Similarly, knowledge about
relationships between such agents can readily be fit within the agent-based mechanism.
Environmental processes and effects that are not inherently agent based in character can be
included in hybrid computational models with significant agent-based components. In contrast
to competing modeling frameworks, suchas numerical models based on systems of differential
equations, or symbolic logic, the agent-based approachprovides muchgreater
facility for capturing the information that is available, and executing the resulting simulations
can be used to infer the dynamic implications of the combination of knowledge and
assumption that is incorporated in the model.
In response to these technological innovations, there has been an explosion of modelbuilding
activity spanning many fields, including economics, sociology, anthropology, political
science, and game theory. Nearly all of the early studies have focused on the representational
aspects of their problems. The design of models that capture aspects of the structure
and behavior of their target systems has been emphasized. Demonstrations that a model produces emergent behavior similar to that of the actual system are typically used to suggest the
promise of this approach. However, most studies to date must be labeled as preliminary, and
there are few examples so far of specific theories being propounded based on such modeling
or of concrete policy recommendations being made.
To take the next steps, beyond preliminary modeling studies to research that leads directly
to new theory or to policy recommendations, innovations beyond representational adequacy
are going to be required. In this article, we discuss three related issues and approaches we
have developed to address them. We believe these are the needed secondary innovations
required for computational social science to become truly effective. The first issue is that of
computational epistemology. Here, we will suggest the requirements for research based on
computer modeling that must be met if we are to credibly learn things about the world by this
means. Second is the related question of research methodology. How should a research program
investigating a specific question set about finding an answer through computer modeling?
Finally, we describe software technology we are building to make it easier to pursue the
approaches described here.
COMPUTATIONAL EPISTEMOLOGY
Computational epistemology deals with the question, “How can we learn things about the
world by performing computations?” In a scientific context, we can ask the more specific
question, “What are the standards of rigor that should be applied to research based on computer
modeling and that should be met if we are to believe the conclusions that research suggests?”
In a policy context, the equivalent question is, “What arguments for specific decisions
based on the results of computer modeling should be accepted, or at least viewed, as
credible?” The more model-centric version of these questions is, “What properties must a
model have that allow it to be useful in answering a scientific or policy question?”
Focusing on the last version of the epistemological question, we can distinguish two very
different images of modeling that can be observed in the thinking of computational scientists.
In the first image, a good computer model is a veridical model of the system, a mirror of
the world sufficiently correct that we can learn about the world by peering into it. Mirrorworld
computer models have been very powerful tools for many problems in science, engineering,
and business. Suchmodels can be validated by comparing model outputs to data
from the actual system or physical experiments conducted to check model performance.
Models that reliably predict detailed behavior of systems of interest to within some wellcharacterized
error process are powerful artifacts, and when they can be achieved, the issue
of epistemology is solved. However, many problems remain difficult or intractable to solve
through predictive modeling. Predictive modeling can only be applied to closed systems that
are well enough understood that the uncertainties are manageable and linear enough that
whatever uncertainty does exist does not overwhelm the knowledge that we do have about
the system. The most successful domains for applying such an approach have been engineering
applications, though clearly there are social systems whose behavior can be predicted,
perhaps probabilistically.
Unfortunately, many very interesting scientific and policy problems have properties that
do not really allow the mirror-world approach. Problems that combine significant complexity
withuncertainty can make the classical model-building strategy of representing accurately
the important details and neglecting or simplifying others difficult to employ. For
deeply uncertain problems, no matter how detailed a model is constructed, we cannot be confident
that the model’s predictions about the behavior of the real system can be relied on.
Many policy problems are characterized by open boundaries, so that no forecast is going to be reliable, and eventually being surprised is the one thing that is assured. For these hard
problems, any model we construct is a flawed mirror, which has the potential to deceive as
well as illuminate.
A very different framework for thinking about computer modeling is rooted in the phrase
computational experiment. Any computation we might perform can be thought of as a computational
experiment if it has the possibility of surprising us and the potential to inform us
through its outcome, surprising or not. A computer simulation can be a platform for performing
computational experiments, mapping the inputs that specify a case into the outputs that
measure the resulting system behavior. From this point of view, a computer model can be
useful if it can be used to perform computational experiments whose outcomes are useful in
constructing a credible argument. In this framework, a model is not used as a mirror but
rather serves the role of laboratory equipment. And a good model is not necessarily one that
is as perfect an image of the actual system as is possible. Instead, a good model is one that can
be used to perform crucial experiments that are useful in the context of an argument or problem.
Sucha model may in fact be purposefully unrealistic. To best support an a fortiori argument,
for example, some model attributes must be reliably greater or less than those of the
actual system to make the a fortiori logic work. Thus, an economic model that demonstrates
highly efficient market outcomes using agents with very limited reasoning abilities can be
used to support an argument that efficient outcomes may be an emergent effect not requiring
any particular intelligence or wisdom on the part of the market’s participants. A model with
more realistically intelligent agents would be less useful to this argument than the clearly
unrealistic one withunreasonably stupid agents.
This second approach to modeling that uses models as platforms for computational experiments
has been called “exploratory modeling” (Bankes, 1993, 1996; Dewar et al., 1996).
The differences between exploratory and predictive modeling are large and not widely
appreciated. When building a model intended to predict outcomes, there is a strong motivation
to limit the explicit uncertainties, sometimes to the detriment of the scientific value of
the result. When building laboratory equipment, on the other hand, adding additional knobs
and switches serves to increase the utility of the instrument. And it can be observed that
inside an exploratory modeling context, model builders tend to declare a larger number of
dimensions of uncertainty, as this gives their products more rather than less potential value.
Predictive modeling comes from the context of theoretical science, with a bias toward
deductive reasoning and a resulting preference for validity as a standard of researchquality.
Exploratory modeling treats the use of computer models as experimental science. Standards
of rigor for experimental science are based on inductive and abductive logic and, thus, are
defined more in terms of falsifiability and reproducibility (Popper, 1979; Radnitzky &
Bartley, 1987). To the extent it is sensible to speak of validity when performing computational
experiments, one must validate arguments, not pieces of lab equipment. Lab equipment
may be calibrated or checked for errors, of course, but properly calibrated lab equipment
hardly guarantees credible scientific results. The epistemology of experimental science
has received a great deal of attention, and the realization that much of computational social
science is experimental science presents an answer to the epistemological question. Except
for those rare cases where agent-based models can be experimentally validated to predict
outcomes, the standards of rigor for computational social science should be framed in terms
of evolutionary epistemology (Popper, 1979; Radnitzky & Bartley, 1987) and not on logical
correctness, validity, or other positivist concepts.
A single model that accurately predicts the outcomes of future events or experiments is
clearly a powerful tool (clear mirror) that can provide substantial support for reasoning and
decision making. But the widely held belief that only through predictive accuracy can a model be made useful has been a substantial barrier to effective computational social science.
Most ABM activities do not produce models that predict future measurements. In
defense of their work, modelers are frequently drawn to claim predictive accuracy that has
not really been demonstrated or to label their work as preliminary, suggesting to their colleagues
and sponsors that predictivity may eventually be achieved. But for many problems,
predictive accuracy may never be possible, and models that do not predict can still serve as
the basis for doing good science and informing good policy. The question of how to do so is
addressed next.
METHOD: EFFECTIVE REASONING WITHOUT PREDICTION
Over the past decade, we and our colleagues have developed and used exploratory
modeling–based methods to address numerous problems, including modeling cognition,
global climate policy, electronic commerce, technology diffusion, and product planning.
Although individual studies have employed diverse methodological tricks, we can synopsize
these techniques into four fundamental methodological principles: (a) conceive and employ
ensembles of alternatives rather than single models, policies, or scenarios; (b) use principles
of invariance or robustness to devise recommended conclusions or policies; (c) use adaptive
methods to meet the challenge of deep uncertainty; and (d) iteratively seek improved conclusions
through interactive mechanisms that combine human and machine capabilities. In this
section, we describe eachof these principles in turn.
Using Ensembles
The large amounts of computer power that are now available make possible new
approaches to computational problem solving. For uncertain, nonlinear problems, an ensemble
of alternative models can contain more information than any single model. By conducting
large numbers of modeling experiments, it is possible to derive insights through the
exploration of the properties of an ensemble of alternative models that no single computer
model could reveal.
Instead of only constructing and running the model that we believe best combines simplicity
with accuracy and using that as a mirror for the real world, an ensemble containing
large numbers of alternative models can be described. Suchan ensemble can be represented
in the machine either explicitly by constructing each member of a (finite) ensemble or generatively,
where a process can be used to construct any member of a (potentially infinite)
ensemble driven by a data structure that specifies that member. To use this approach to computational
reasoning, we need not demonstrate that any single model correctly captures all
important properties of the system under study but only that the ensemble is sufficiently
diverse, given available knowledge, that the behavior of the system of interest should be contained
within the ensemble. By looking in many mirrors, each flawed in different ways, it is
possible to see truths that no single mirror can reveal.
Once suchan ensemble is defined, researchconsists of performing computational experiments
by drawing examples from the ensembles of models (or, equivalently, the ensemble of
possible computational experiments or the ensemble of possible futures). Properties of the
entire ensemble can be (inductively) inferred from the experiments that are performed. As
with physical experiments, there is no guarantee that some experiment not yet performed
may eventually refute conclusions that we reach. Such nondeductive reasoning strategies
may seem alien to some theorists, but this is normal experimental science.Conclusions From Invariance and Robustness
When a single model suffices to capture all we know about a system, inference consists of
running the model, and policy recommendations can be formulated by optimizing some cost
function against that model over the range of possible policies. Where single predictive models
can be discovered, this approach to reasoning is sound. However, the desire to be able to
use computers in reasoning about social systems has caused many models to be developed
and used even where prediction is not possible. Treating such a model as though it predicts is
invalid reasoning (regardless of whether that model is viewed as having been “validated” by
some means other than predictive accuracy). And the desire to advance science by having a
single predictive model has driven social science to avoid important problems, to craft models
withunreasonable assumptions, and to formulate “efficient” or “optimal” policy recommendations
that may be fragile to uncertainty or surprise. Other problems include valuing
those things that can be measured and quantified over those things that cannot, a tendency to
make inappropriate assumptions to render cost functions convex (to make global optimization
tractable), and a culture that values efficiency over robustness.
When knowledge is best captured with an ensemble of models, the only inferences that
can credibly be made are inferences regarding properties of the ensemble. Properties of single
models, and results of single computational experiments, are of little consequence except
where they contribute to that larger goal. By discovering properties of an ensemble of models
that includes the actual system, we can learn new facts, even if no model tested is an accurate
representation of the world.
Properties of ensembles that can be applied to all the members of the ensemble can be said
to be invariant. Thus, the foundation of any methodology for dealing with complex social
systems can be described as representing available knowledge as an ensemble of possible
models and using a series of computational experiments to (inductively) infer invariant properties
of that ensemble. Invariant properties may be simple statements that are true of all
members of the ensemble or more complex and conditional statements that imply different
conclusions for different models depending on their attributes (see below). The nature of the
invariant that a researcher is trying to establish can influence the search or sampling strategy
employed to try to demonstrate (or discover) that invariant property.
In policy analysis settings, the invariant of greatest value is a policy recommendation that
can be made for any possible model of the system. It is reasonable to speak of such a property
as robust. A robust policy is one that has satisfactory outcomes for any model that could be
drawn from the ensemble. In policy settings, we desire robustness in several different senses.
A good policy is one that is robust to uncertainty, differences in values, exogenous shocks,
and surprises.
Robust policy recommendations prove to be a more natural formulation for interacting
with human decision makers than decisions asserted to be optimal, to be efficient, or to have
maximal expected utility. Humans often use robustness and the desire to avoid regret as the
basis for decision making. By employing similar methods in computer-based decision support,
the machines can become contributors to human deliberations, rather than a “take it or
leave it” alternative.
Many different techniques can be employed to discover invariant properties or robust policies.
Some are described in subsequent sections. A quite direct one is to reuse the mechanisms
for representing ensembles that are already needed to deal with uncertainty. The computational
mechanisms used to make manageable the use of (potentially infinite) ensembles
of alternative models can be further used to represent ensembles of objective functions, system
states, or candidate policies. Thus, for example, we can characterize satisfactory policies for a particular model as those achieving better than a specified threshold in some utility
function. Robust policies, then, are those that lie in the intersection of these “level sets”
across all possible models.
Dealing With Uncertainty Through Adaptivity
An ensemble of models does not need to be an unordered assortment. Individual models
can be characterized by a variety of attributes, economic growth rates, network topologies,
learning rates, and cultural norms. And social theories need not be simple facts that must be
proved true for any model. Theories can be parameterized by the attributes that index models
in the ensemble and can depend on these attributes in arbitrarily complex ways. As a consequence,
our ability to craft effective theories of social systems need not be reduced in admitting
to limits in our knowledge and our ability to predict. Computational methods can be used
to jointly search universes of possible models and possible theories. Tools allowing human
and machine collaboration in such searches and in the reframing of problem statements to
allow the discovery of effective theories have a great potential to revolutionize the conduct of
social science.
In a policy setting, a very important type of solution complexity is the use of adaptive policies
to achieve robustness. One of the main ways humans seek robustness in the face of a
complex and uncertain world is by being adaptive. We temporize, postponing choice as long
as we can manage, and create policies and institutions designed to respond to changing circumstances.
The modern business literature is full of exhortations for managers and organizations
to be agile, responsive, and “learning organizations.” But there is scant advice on how
to adapt, what sort of mechanisms to implement to be responsive, or how to quantitatively
evaluate alternative approaches to organizational agility. This is to a great extent due to the
tunnel vision created by our analytic heritage.
When we base our analytic techniques on best estimate models (either deterministic or
stochastic) and then either evaluate or devise policies using optimal performance as a criterion,
the result can never be an adaptive policy, if adaptation has any associated cost whatsoever.
Devising adaptive mechanisms requires a challenge set of possible futures.
Existing techniques for decision support encode information about the problem in a static
fashion and produce recommendations that amount to static timetables. Thus, many businesses
forecast prices and demand far into the future and construct business plans from static
strategies of many years’ duration. Most municipal governments have transportation plans,
development plans, and infrastructure plans, all static and covering many decades of future
events. Perhaps the most extreme example of this approach are global climate treaties that
commit nations to emission abatement plans that are static timetables covering a century or
more of future activity. All of these plans are taken seriously, even though everyone knows
that their implementation cannot be guaranteed beyond the next election.
For problems withsignificant uncertainties, adaptive policies can easily outperform any
static competitor. An adaptive policy is in effect a computer algorithm that adjusts actions as
information is gained with the passage of time. The problem of finding good (robust) adaptive
policies is often intractable when framed mathematically. However, computational
methods can be used to discover such policies. We have done so in a number of studies over
the past decade. Adaptive policies are a better construct for creating policy recommendations
incorporating the pragmatics of real policy contexts. They also have several useful features
for supporting policy research. In explicitly representing sensing and responding, adaptive
policies are useful researchconstructs for computing the value of information and trading off
research, investment in infrastructure, and immediate action in confronting long-term social problems (Lempert, Popper, & Bankes, 2002; Lempert, Popper, Bankes, & Fonkych, forthcoming;
Lempert & Schlesinger, 2000).
Interactive Collaboration Between Humans and Machines
A model may embody knowledge we have about the world, but computational research is
conducted not witha model but throughmodeling. A well-crafted algorithm may deploy
computational resources to perform useful inference, but knowledge discovery is best
accomplished by combining the strengths of humans and machines. Computational experimentation
is best accomplished with lab equipment that allows for agile interaction between
computer software and researchers. Computers are much better at reliably pursuing long
chains of inference, combining diverse facts to reveal their implications. But humans are
vastly superior at perceiving patterns in the outcomes of experiments and possess pragmatic
and tacit knowledge that resists easy encoding.
Classical analytic researchmethodologies conceive of calculation in support of reasoning
as a data-driven, feed-forward process. The process begins by making bounding assumptions,
arraying all relevant information, choosing the appropriate mathematical encoding of
that information, and then using the computer to mechanically solve the problem. In reality,
research is highly iterative, takes many unexpected turns, and often leads to destinations that
were not initially anticipated. Infrastructure for computational social science must provide
support for this entire process; otherwise, integration between knowledge resident in the
computer and that resident in humans cannot be achieved.
The need for rich interactivity applies to all the methodological goals discussed above.
Researchers may define an initial ensemble of possible experiments, but as insight grows
through the process of research, the need to alter the bounds of the analysis will frequently
emerge. An ensemble may be expanded by relaxing assumptions, contracted by the discovery
of new knowledge that further constrains the range of plausibility. Or the framing of the
entire problem may be altered, mutating the syntax of the ensemble and its supporting
mechanisms.
The search for invariance and robustness can benefit from human-machine collaboration.
Once a framework of possible conclusions is defined, the process of validating or refuting
these hypotheses can be automated. But the results of such an automated process can easily
prompt the researcher to make new guesses, to advance hypotheses of new or revolutionary
form. Indeed, the complexity of hypotheses may incrementally evolve as the researcher redefines
the terms of investigation in response to preliminary failure.
None of the methodology described here could be accomplished without the computational
power that only recently has become available. And enormous labor would still be
required to aggressively pursue this approach without appropriate software tools. In the final
section, we describe the design of tools we have implemented to meet this need.
TOOLS FOR EXPLORATORY MODELING
We are developing a prototype software environment that supports research using the
methodology described above. Known as CARs (computer-assisted reasoning system), it
provides a wide range of services allowing users to incorporate modeling platforms of
diverse type, define ensembles of possible experiments based on these platforms, perform
large numbers of computational experiments drawn from these ensembles, and interactively
visualize the outcomes of experiments through graphical displays. CARs has already been
used on a number of researchefforts, and a version available to a larger researchcommunity is now entering alpha testing (see www.evolvinglogic.com). In this final section, we
describe the general design features of CARs and how they can be used to support the methodology
we are advocating.
Experimental Contexts
The fundamental concept of the CARs environment is that of computational experimental
contexts. An experimental context is a software mechanism that encapsulates a platform for
performing computational experiments and provides a variety of services relevant to conducting
a series of those experiments. In particular, it implements the syntax for a data structure,
instances of which serve as specifications for individual experiments, and the syntax for
the data structure that contains the results from experiments. The context’s platform for performing
experiments can be viewed as providing a service that maps input data structures to
result data structures. The experimental context can be thought of as representing the ensemble
of possible computational experiments. This representation is generative in nature. An
infinite ensemble can be represented by an experimental context in the sense that any particular
instance of the ensemble can be generated on demand by submitting the data structure to
the computational experiment platform. To understand the properties of the ensemble, a
method for sampling from the ensemble must be selected, this method used to generate a
series of computational experiments, and the pattern of outcomes thus revealed used to infer
the properties of ensemble generally. This inference is not deductive and therefore could be
falsified by additional computational experiments. But credible inference is none the less
possible.
By encapsulating a computer model and using it as a platform for computational experiments,
one can create an atomic experimental context and use this context as a basis for
exploring the properties of the ensemble of possible experiments the context represents.
CARs provides various services to facilitate this restricted variety of exploratory modeling,
which some authors have referred to as exploratory analysis (Brooks, Bennett, & Bankes,
1999; Davis & Carrillo, 1997). However, the specification for a computational experiment
can include information beyond that which is directly used as input to an external model. The
specification for an experiment can also be used to generate the details of the model to be
used, and experimental contexts can thus support exploration over structural as well as parametric
uncertainties (see below for additional details).
CARs provides a basis for exercising models that is richly interactive. By exploring alternative
graphics types, choices of axes, and slider bar settings, the user is able to develop a
growing understanding of the input-output properties of the underlying computational
model. This interaction can be used to explore a database of stored values of experiments run
offline. But user inputs can also drive case generation on the fly, allowing users to dynamically
explore landscapes of experimental outcomes and to develop incrementally a understanding
of the properties of the overall ensemble.
The process of exploratory modeling frequently leads researchers to vary not only specifications
within a given context (that is, instances within a fixed ensemble of possible experiments)
but also to explore across alternative definitions of ensembles of interest. In CARs,
this could be accomplished as it is in most computational science, by revising the code for the
model, which is often better thought of as the platform for performing experiments. But
CARs also provides significant support for exploring across alternative experimental contexts
through a mechanism known as derivation operators, whose use does not require revising
code, or any human labor at all. Derived Contexts
CARs supports two variants of experimental contexts, atomic and derived. A derived
experimental context is indistinguishable from an atomic context except in the implementation
of the platform for performing computational experiments. (In CARs technical jargon,
suchplatforms are known as computational experiment virtual machines [CEVMs], or
machines for short.) In an atomic context, computational experiments are performed by
using the specification for the experiment to create the inputs for an external computation
(frequently a model) that is being used as a CEVM. In a derived context, the specification for
an experiment is translated to create specifications in one or more foundation contexts, typically
different than the original derived context. The results from the experiments performed
in these foundation experiments are then also translated back into the syntax and semantics
of the derived context.
A particular means for creating sucha derived context could be custom built for an individual
research effort. However, CARs supports a library of derivation methods, which can
be used to create new derived contexts from one or more foundation contexts and are useful
across many different applications. Derivation methods can be recursively applied to arbitrary
depth. Ultimately, specifications in derived contexts must be translated, possibly
through a complex series of derivation steps, to create one or more cases to be run in atomic
contexts. These derivation mechanisms allow users to explore across alternative reframings
of the “language” for describing computational experiments, without making changes to the
code. Because derivation may be recursively applied, this allows us to provide automated
tools for assisting in explorations across alternative contexts, and over time, researchers can
move to a progressively higher level of abstraction in the search for frameworks that provide
insights into candidate theories or policies. In our system, ideally, the code for a model
should include only the known “physics” of the system, that is, the aspects of phenomenology
that are well understood and universally agreed on. All analytical devices should be
applied outside of the model using the CARs reasoning tool, allowing the option of computerized
checking of analytic results and tentative conclusions.
CARs supports a wide variety of derivation methods, not all of which are described here.
To better understand how derivation methods are used, it is useful to first consider a restricted
subclass, known as simple derivations. Simple derivations create a derived context from a
single foundation by applying translation operators that map single specifications in the
derived context to single specifications in the foundation context and translate single result
objects in the foundation context back to become single result objects in the derived context.
An extremely useful example of a simple derivation method is the freeze derivation,
which is used in CARs to restrict exploration to a subspace of the foundation context’s
ensemble of experiments. A freeze derivation eliminates some number of inputs from the
foundation context’s input space by giving them fixed values. These values are specified at
derived context creation time. When a case is requested of the CEVM in the derived context,
the derived context’s specification is translated into a specification for the foundation context
by “filling in” the missing inputs using the fixed values given at the creation of the derived
context.
The freeze derivation allows us to create models with huge numbers of inputs but to focus
on a smaller subset of inputs when manually exploring the model. Frequently, assumptions
get made in model writing that cannot really be justified by the facts. Such assumptions seldom
get revisited, as doing so would require modifying the code. And frequently, these
assumptions are poorly documented, and the “faceplate” of the model provides no hint that
they have a role in the model’s “wiring.” The availability of the freeze derivation encourages and allows model writers to represent the full range of uncertainty as model inputs, without
paying a price through the huge dimensionality of the input space at analysis time. The decision
to simplify the analysis by making some number of assumptions can then be delayed
until after preliminary exercise of the model. So, for example, an analysis of variance can
reveal those inputs that have the greatest impact on the issue at hand, allowing the researcher
to focus attention on inputs most likely to be relevant and to make assumptions primarily on
issues that are less likely to be important. Similarly, once tentative conclusions have been
reached by an analysis conducted in the limited subspace, these conclusions can be checked
using automated methods, without significant researcher labor or recoding. So, for example,
one can launch a “weak method” search, such as a genetic algorithm, across the entire
ensemble of the foundation context, searching for counterexamples to the tentative conclusions.
(To see an example of sucha researchmethodology applied to a real problem, see
Robalino & Lempert, 2000.)
Many other simple derivation methods exist in CARs. New inputs or outputs can be created
from mathematical combinations of inputs and outputs in the foundation context. Inputs
and outputs can be eliminated through additional simplification mechanisms such as deriving
multiple inputs in the foundation context from simple inputs in the derived context. These
methods provide significant power to reframe the terms of the analysis and to discover new
problem languages in which better theories can be described, while leaving model code an
unchanged description of known facts. However, an even more powerful mechanism is provided
by derivation methods that create large numbers of experiments in foundation contexts
from a single specification in a derived context.
Compound Computational Experiments
CARs supports a number of derivation methods where a single experiment in a derived
context causes a large number of experiments to be conducted in its foundation context(s).
Thus, it is possible to create compound computational experiments, where single acts by the
user of the software environment cause large numbers of atomic experiments to be conducted
in one or more atomic contexts, and their results are summarized for maximal insight
generation.
As the degree of compounding increases, compound computational experiments can be
seen to have infinite parallelism. The capability in CARs to distribute experiments over networks
of machines, either across a local network or over the Internet, thus can provide
researchers with a substantial “surge” capability in meeting their computational needs.
An example of a compound computational experiment is a Monte Carlo averaging of various
outcome measures over probability distributions on various inputs in a foundation context.
Consider a foundation context where choices for input values deterministically produce
outcome values. In a derived context, a single experiment that specifies the subset of inputs
that are not being viewed probabilistically can result in a Monte Carlo averaging over the
ones that are to produce averages and variances as results. Among other advantages, this
keeps the modeling of the physics separate from the Monte Carlo generation and assumptions
about probability distributions. As a consequence, robustness testing where one
searches for the assumptions on priors that would be required to falsify conclusions reached
using nominal probability distributions can be routinely conducted.
A type of compound experiment that is particularly powerful is the use of search to invert
the map that is computed in the foundation context. In CARs, there are derivation methods
that allow such inverse mappings to be computed for any model, without the modeler needing
to get involved in the implementation of search methods employed. One possibility is to search for the maximum or minimum of one of the outputs of the foundation. In a typical derivation,
a subset of inputs of the foundation context would be bound. The derived context
would then only have the unbound inputs, and for each single experiment in the derived context,
a compound experiment would be conducted in the foundation context, searching over
the bound inputs for the appropriate extremal point in one of the context’s outputs. The
derived context would map from specifications for eachof the unbound variables to a result
that includes the values of all results in the foundation context at the discovered “best point,”
plus the value of all bound variables required to achieve that maximum or minimum.
The semantics of this derived context varies depending on the nature of the map in the
foundation (whether it is one to one, in particular). Syntactically, it has the interesting property
of allowing us to move entries from the input to the output side of the mapping relationship.
A somewhat different derivation takes as input a target value for an output in the foundation
context’s results and searches for values of bound inputs that achieve that target value
in the foundation. This has the effect of moving entries from the output side to the input side
of the map. Together with a sufficiently rich set of simple derivations, these derivation methods
form a “spanning set”; any syntactic variation on the input/output map of an atomic context
can be achieved by the appropriate series of derivations. With this property, we can see
the promise of CARs developing into a tool that allows a strict separation of representation.
In the ideal situation, only phenomenology should be represented in model code, with all
analytic manipulation being provided by the software environment in which computational
experiments are conducted.
Other examples of compound derivation methods include those that conduct a sensitivity
analysis on some of the inputs of a foundation context, or a regret derivation that for a particular
policy choice and a list of guesses for exogenous uncertainties discovers through search
how much worse the chosen policy does in that context than the best possible (the regret), and
those that create contexts that integrate over some of the inputs in the foundation. All of these
can be useful tools in searching for conclusions or policies that can be reached in spite of
uncertainty. And used together (by conducting sensitivity analyses of regret, for example),
they provide major pieces needed to efficiently automate the search for robust adaptive
policies.
SUMMARY
To fulfill its potential, the ontological innovation of ABM must be augmented with further
innovations in epistemology, methodology, and technology. In this article, we have provided
an epistemological framework with methodological implementations that has a demonstrated
capability to use ABMs to solve real problems. Prototype software technology shows
the potential to make the application of these methods widespread and routine. Although
much remains to be done, the promise of this approach is now apparent.

On agent-based modeling and computational social
science
1. INTRODUCTION
The two decades around the turn of the millennium have seen the
rapid advent, and perhaps the premature decline, of a paradigmatic
shift in science, represented by agent-based modeling
(ABM) and simulation. In this section, after shortly defining what
we mean with ABM, we present a short account of its history.
1.1. WHAT AGENT-BASED MODELING IS
What is meant by Agent Based Modeling? Often, this is defined
in opposition to Equation-Based Modeling (see for example
Dyke Parunak et al., 1998; Cecconi et al., 2010). More specifically,
ABM arises at the intersection between agent theory, systems,
and architectures, on one hand, and the social sciences, on the
other hand. Agents are usually defined (see Conte, 2009) as
autonomous systems that operate transitions between states of
the world, based on mechanisms and representations somehow
incorporated into them.
Under this general definition, the field of agents shows a
tremendous variability. Agents vary indeed on several dimensions,
which include whether and to what extent they are
autonomous, self-interested, sociable, and capable to learn from
experience and/or observation. Agents also differ in their level
of complexity: according to a classic distinction introduced by
Wooldridge and Jennings in their influential work (Wooldridge
and Jennings, 1995), agents in a “strong” sense are capable to
manipulate and reason upon mental representations; otherwise
they are considered agents in a “weak” sense. Another important
distinction concerns the way in which mental representations
are incorporated: symbolic representations allow an agent to
mentally manipulate them in order to reason, plan, take decision,
communicate. Sub-symbolic representations are unaware,
implicit, based for example on network-like configurations representing
the structure of relationships among neurons in cerebral areas, and not liable to purposive manipulation on the side of the
agent. Finally, agents vary according to the philosophical or metatheoretical
view their description is based upon. One example is
the attempt to model agents on the basis of a personal utility function,
on which much work on agents has been done over the past
30–40 years or so, and that has also been criticized as for its micro
plausibility (Antunes and Coelho, 2004).
The practice of ABM however did represent a substantial
under-exploitation of such wide spectrum of possibilities. De
facto, much of the agent models worked out and simulated are
totally ad-hoc, based on very simple local rules (Epstein, 2006),
more or less arbitrarily implemented on a program running on
a computer (Gilbert and Troitzsch, 2005). When the program is
run, macroscopic effects of the local rules can be observed on the
screen, and then be stored, analyzed and possibly visualized in
search for emergent phenomena. We will return to the problem
of ad-hoc rules in section 2.3 below. Such a practice of modeling
lends itself well to observe and experiment upon multi-agent
worlds or agent societies. These are meant to either reproduce
some real-world setting or phenomenon [a typical example is the
Anasazi culture simulation (Axtell et al., 2002)], or to build up
and observe would-be worlds (Casti, 1997). Such models allow
novel theories about abstract social phenomena to be formulated,
operationalized, and tested. Examples of this application of ABM
abound and are among the best cited works so far worked out in
this field.
1.2. AGENT-BASED MODELING IN A HISTORICAL PERSPECTIVE
Conference proceedings, dedicated to the new methodology
of ABM and its multiple applications within the social and
behavioral areas of science, started to appear in Europe since
the early nineties (Gilbert and Doran, 1994; Gilbert and
Conte, 1995; Conte et al., 1997). Agent-based models of social phenomena trace back to as early as 1971, when the famous
(Schelling, 1971) model of segregation was published in the
Journal of Mathematical Sociology. In 2002, the field obtained
a major institutional acknowledgment, when the proceedings
of a Sackler Colloqium of the National Academy of Sciences,
under the title “Adaptive Agents, Intelligence, and Emergent
Human Organization: Capturing Complexity through AgentBased
Modeling,” held in October 2001, were published on PNAS
(Bonabeau, 2002). In that circumstance, ABM was proclaimed
as the leading field—we might say the flagship to use a trendy
tag - in the renewal of the social, behavioral, and complexity
science, which was expected to take place in the years to
come. Consecrated by the US scientific institutions, the field was
already intensely practiced also, if not primarily, in Europe, where
ABM had given rise to a new journal, the Journal of Artificial
Societies and Social Simulation (JASSS, founded in 1998), to the
first scientific association (ESSA, The European Social Simulation
Association, created in 2003), and was at the center of a variety
of promotional activities. Soon enough its range of influence
extended beyond the two sides of the Atlantic, reaching out to
the Pacific area, and giving rise to the PAAA association. At the
same time, the NAACSOS association was founded in the US.
After some years of fruitful competition, the associations joined
in the first World Conference on Social Simulation held in Kyoto
in 2006.
At the end of the first decade, however, the ABM leadership
seems to be challenged if not decisively weakened by the
(re)appearance of a more sober, more encompassing, and less
innovative tag, that of Computational Social Science (CSS), of
which ABM is a component (see Bankes et al., 2002) for an early
insight), and which now candidates itself to replace ABM in the
same leading position for the next decade. Evidence of a change
of leadership and of a possible coming era for bare CSS, rather
than for the more inspiring Generative Social Science proposed by
Epstein in 2006, can easily be found in some position papers (e.g.,
Lazer et al., 2009; Cioffi-Revilla, 2010), books recently appeared
(Gilbert, 2010), a new regional association—the CSS Society of
the Americas, born on the ashes of the short-lived NAACSOS, and
the relative conference held in 2011—and, finally, the objectives
of the unsuccessful but groundbreaking EU FET flagship pilot
FuturIct (www.futurict.eu).
If history is instructive, the study of signaling is fun. In
the era of information overflow, distributed content production,
collaborative filtering, crowd sourcing, and so on, emblems
are decisive. Tags have a far-reaching but short life. Under the
tyranny of PageRank, contents compete in terms of lookups,
and these most certainly depend on familiarity, and possibly
also on tags appeal. Science makes no difference. It is somewhat
surprising when a paradigm shift is signaled by a flat
combination of two traditional scientific areas: social sciences
and computational science. What is the meaning conveyed by
this signal? Does the new label correspond to a new paradigm
shift in the social and behavioral sciences, or does it simply
meet a kind of marketing need for periodical renewal of
names?
This paper presents an attempt to weigh up the impact of ABM
and answer the question whether this field is undergoing or not a real decline; whether or not his replacement was timely, necessary,
and effective. Next, some current variants of CSS will be compared.
Finally, some important requirements for achieving real
progresses in the computational study of social phenomena will
be identified and discussed.
2. AGENT-BASED MODELING: A BALANCE
Rather than a detailed survey of ABM (for a good example, see
Helbing and Balietti, 2011a) this paper presents an attempt to
draw a balance of this field, pointing to its main weaknesses and
strengths.
2.1. STRENGTH OF AGENT-BASED MODELING
One may wonder what ABM is good for and what are its major
strong points. The tricky questions as to when ABM is really
needed, whether agent-based models can or cannot be converted
into an analytical, equation-based model and to what extent this
can be done has been debated at length elsewhere (see for example
Epstein, 2006; Cecconi et al., 2010). Nonetheless, ABM remains
the only known approach apt to model and reproduce sets of
heterogeneous agents interacting and communicating in different
ways.
Of course, ABM can only provide a sufficient explanation of
the phenomenon of interest, not a necessary one. This feature,
which (Epstein, 2006) extensively clarifies and discusses, is also
known as multi-realizability (Sawyer, 2005), and it is an outstanding
property of multilevel systems. A macro-level phenomenon in
whatever domain social, natural, mental, etc. of reality, is multirealizable
when it can be implemented in different ways on the
lower levels. Inevitably, ABM generates the higher-level effect by
following one of the possible generating paths. Even if as many
models as generating paths were actually implemented, it would
still be difficult, if not impossible, to assess which one among
them is effectively implemented in the real world. But, interestingly,
this is true also of the target phenomena: an organization
can perform its mission independently of the internal structure
(consider as an example a project-based structure against a functional
one). Social conformity is achieved through a variety of
internal mechanisms, e.g., imitation or norm compliance. It is still
unclear how disapproval works as a sanction, whether it affects
people’s decision-making because it activates an expected associated
material punishment, or violates the goal of a good (self)
esteem. Actually, multi-realizability is a property not only of ABM
but also of the real world. In this sense, multi-realizability differs
from the more general issue of model underdetermination,
as it connects it directly with possible generative paths in reality,
an analogy that makes ABM particularly apt to study the equivalence,
or possible lack thereof, of structure and mechanisms inside
intermediate levels, in the sense of the examples above.
To implement sets of heterogeneous agents in interaction
brings about a series of second order advantages: agent societies
are (1) operational platforms where theories get converted into
falsifiable hypotheses; (2) experimental laboratories where theories
get gradually and thoroughly controlled; (3) multilevel worlds
where the level of individual units, the agent, is clearly distinct
from the macro-level, the system level and unforeseen effects and
emergent properties of interaction can be observed. In short, ABM is an in nuce society, which unfolds and actualizes
when the model is implemented on a computer program
and this runs. In some cases, the effects observed in the computer
could not be predicted while modeling and implementing
the single units, the individual agents. Hence, the effects of such
behavioral units on the whole agent society or parts of it can
be observed and investigated. Otherwise stated, ABM allows the
interplay between different levels of a social system to be modeled
and observed. As shall be seen later, this important property of
real-world societies has been insufficiently exploited. The main
dynamic investigated by ABM is the way-up of the interaction
among the micro and the macro-level. The complementary process,
the way-down from the macro-level to the micro-level, has
been poorly explored. Closing the loop, however, may require
a high level of ABM complexity. Theory-driven, non-ad-hoc
models of phenomena generated by intelligent behavior may be
relatively difficult to calibrate (Heckbert et al., 2010). Difficulties
usually increase with the model’s level of scale and the number of
parameters. One may perceive a trade-off between vertical scaling,
i.e., agent complexity, and horizontal scaling, i.e., scenario
complexity. Such a trade-off is probably one of the keys for ABM
development and leads us straightforward to one of the weak
points in the field.
2.2. WEAKNESS OF AGENT-BASED MODELING
Some problems and difficulties in the field of ABM and simulation
have been perceived from within the scientific community
since long, while others have only recently come to our attention.
Since the field’s early days, a serious concern of Agent Based
modelers and simulators is how to design large-scale agent-based
simulations. In its initial applications, agent-based models did not
care much about the problem of scale, as they were applied to
observe the emergence of patterns from interaction at the microscopic
level in artificial scenarios sharing some crucial features of
the real-world, but not really aimed to reproduce its details. As
soon as the potential of agent-based models became apparent—
revealing a great occasion for observing and manipulating in silico
models of target phenomena in order to acquire a better control,
and possibly to optimize intervention—upgrading their level of
scale of several orders of magnitude proved necessary. You cannot
optimize a system of traffic if you do not manipulate parameters
in populations of several millions of agents.
Under the pressure of complex systems science, which is gaining
ground in the study of social phenomena (Helbing and
Balietti, 2011b), agent-based simulation is increasingly expected
to meet a further, and connected, important requirement, i.e.,
to be fed by massive data in real-time. To answer the problems
of scale and real-time simulation, a variety of ICT solutions
(parallel and supercomputing infrastructures) are being designed
and tested. To deal with this challenge, agent-based simulations
were bent to applications needs, such as policy modeling
and traffic optimization (Grether et al., 2010), distributed communication
over the Internet (Chen, 2009), electricity market
(Guerci et al., 2010), financial crisis (Sornette, 2003), epidemics
(Pastor-Satorras and Vespignani, 2001). This is not the forum
for discussing sophisticated technical solutions (but for a review
of techniques to that purpose, the reader might be referred to Paolucci et al., 2013) to the problem of making ABM more apt
to the requirements of BigData science. We will instead touch
briefly on the question of model equivalence across disciplines
and applications.
2.2.1. Equivalence of models
Unlike laws of nature, Agent Based models of socio-economic
phenomena are countless and not always consistent (see Alfi et al.,
2009). Think of the various heuristics and rules of thumb applied
in defining microscopic rules for ABM. Most of them generate
results at the macroscopic level, which are applied more or
less the same narratives or metaphors. Hence, cooperation in
Prisoner’s Dilemma is found to emerge from a set of heterogeneous
strategies, from TIT-FOR-TAT (Axelrod, 1997) to strong
reciprocity (Boyd et al., 2003), from image-scoring (Nowak and
Sigmund, 1998) to reputation-building (Pinyol et al., 2012), and
finally group selection (Di Tosto et al., 2007); social control is
found to emerge from ostracism (Xenitidou and Elsenbroich,
2010), but also from partner selection, and finally from gossip
(Giardini and Conte, 2012); norms emerge from punishment
(Galán and Izquierdo, 2005), which in turn is but a TIT-FORTAT
strategy, but can also emerge from conditioned preferences
(Bicchieri, 2006), and from habituation (Epstein, 2008). Is models’
equivalence a major shortcoming of the field, or something
social scientists can put up with? What does it depend upon? Is it
a necessary or a contingent feature of ABM?
We believe the variety of equivalent agent models in part
depends on a property inherent to multi-level systems as complex
social systems are. The property in question is the multirealizability
that we have mentioned above. In part, we believe it
to be a consequence of the shaky foundations, the poor theoretical
justification at the basis of many agent models. This is not equal
to finding poorly realistic the model of agent often proposed by
current modelers, and asking to improve it toward psychological,
cognitive, or sociological plausibility - toward a seemingly human
agent. What is wrong, in our view, is the procedure for model
building and the role of behavioral rules. Let us examine both
points with some detail in the next two sections.
2.3. THE ABM RECIPE FOR MODEL BUILDING
A consensus seems to have emerged in ABM on a minimality
procedure; that is, models are built by setting up the rules
that are minimally required to obtain the macroscopic effect to
be described. While minimality might sound obviously inspired
by the success of hard sciences, the substantial failure to apply
such a minimality procedure to social science is testified by centuries
of failed attempts, starting from what had been announced
as “Social Physics” in the seventeenth century (for an historical
perspective, see Ball, 2002). The reasons for consensus on minimality
might be better described with the tools of the sociology of
science than rooted in the search of theoretically sound and scientific
advances. Indeed, the ABM community, being still relatively
small, is subject to issues of disciplinary recognition, with the consequent
pressure to publish in a limited number of outlets; and it
might still be looking for the right dimension of the contribution -
the ideal paper size, as measured in effort invested and soundness
of results, could be very different from the “correct” paper size in terms of publication chances. This discrepancy causes a motivational
pressure toward minimal (and publishable) models, and
hampers research in the much more interesting issue, why minimality
seems to fail in the social sciences. We will get back to our
intuition on this matter in the conclusions.
Under the rule of minimality, model building is operated (1)
a posteriori, based on backward engineering from the effects
obtained to the generating rules; (2) ad-hoc, so that rules are
suggested by the specific results to be obtained; (3) in a ruleoriented
rather than agent-oriented approach: what is achieved
is a set of rules, rather than an agent view; (4) inspired by the
minimal-conditions logic: modeling consists of finding out a set
of microscopic rules minimally required to reproduce a given
phenomenon of interest. The minimal approach, thus, strongly
reduces the validity of ABMs on two separate accounts. On the
one hand, theory-based, agent models are implausible caricatures
of agent as prescribed by the rationality theory, with a touch
of psychological realism in the best possible case. On the other
hand, when agent models are not derived from any pre-existing
agent theory or vision, whether computational or not, but only
by the behavior they are expected to generate (Epstein, 2006),
agent models become arbitrary, poorly comparable, competent
in highly specific domains of knowledge and disarmingly inapt
in any other. It should not come as a surprise if, as a result, a myriad
of rather inconsistent agent-based models have been produced
over the past 20–30 years or so. Is it possible to find an escape
between implausible models and arbitrary ones, or between
ad-hoc rules and useless ones? Options exist, but are poorly
exploited. Why?
2.3.1. From cognitive models...
One such option is represented by cognitive agent models, which
exist since the late nineties. Their wide range of influence is shown
by the popularity of BDI architectures (about 32,700 “BDI agents”
cites on Google Scholar retrieved on March, 18th 2013) within
and beyond the field of agent systems and theories. Simulation
of social phenomena with BDI based models also abound in the
literature (about 7060 “BDI social simulation” cites on Google
Scholar on March, 18th 2013), and usable platforms to implement
them are under consolidation, from Jason (Bordini et al.,
2007) to Netlogo extensions (Sakellariou et al., 2008). However,
works with this approach receive attention mostly from the computer
science community, and are rarely published in main social
scientific journals.
Although the rich cognitive models tag appeared since the
early nineties (for a recent example see Dignum et al., 2010),
the amount of models inspired from it remains negligible. Subsymbolic
systems and neural nets did not make much better.
Although neural nets and social simulation fare better, relative
publications again do not appear in major social scientific
journals. Why are cognitive theories on agency, whether
symbolic or subsymbolic, so poorly applied in ABM? In part,
there are problems of inner validity and calibration. While
it is difficult to control the inner validity a complex agentbased
model (Cioffi-Revilla, 2002; Windrum et al., 2007), to
calibrate it and manipulate parameters values so to reflect a
real-world system is hard. To gather data on which the agent model is based upon takes more time and more complex empirical
methodology. Therefore, the utility of a complex agent
model to simulate the real-world system (i.e., showing that
the model’s results match the real-world data) is questionable
(Crooks et al., 2008). Undoubtedly, these difficulties reduce the
interest of cognitively grounded models simulators, although
the latter’s foundations are much firmer than those of most of
the models used. The lesson one might draw is that, like it or
not, scientific developments are often due to practical utility
more than theoretical soundness. However, the little success of
cognitively grounded agent-based models is also due to other
factors.
First, unlike other theory-grounded agent models, for example
the rational models, cognitive models are not prescriptive.
Whereas the theory of rationality is a theory of action, cognitive
modeling provides theories of the agent. Hence, the rational agent
model fits only apparently better the objectives of ABM and simulation,
but it does so only because it allows the modeler to get
rid of the tricky part of the modeling, that is, how agents form the
goals, the motivations, the preferences, that will be implied in the
decisions.
2.3.2. .. to generative models.
Secondly, cognitive modeling is a truly generative theory of
behavior, accounting for behavior in terms of the mechanisms
that are supposed to operate while producing it. A generative
explanation of an observed social phenomenon consists of
describing it in terms of the external (environmental and social)
and internal (behavioral) mechanisms that generate them, rather
than by inferring causes from observed co-variations. This is a
vital property of explanation, which cannot easily be realized
otherwise. When describing agent behavior by means of other formalisms
(logic-based or numeric), we describe behavior from the
outside, as perceived by an observer, but do not describe the way
it is generated. ABM explains behavior from within, in terms of
the mechanisms that are supposed to have generated it, that is,
the mechanisms that operate in the agent when s/he behaves one
way or another.
Of course, behavior can be explained otherwise. For example,
the flight of hawks is wonderfully explained by the mathematical
property of logarithmic spiral, such that any tangent from the
center of the spiral yields an angle of the same width. Thanks to
this property, hawks can keep their preys always in their aim while
describing a spiral before pouncing on them. But this explanation
is not generative, in the sense that it does not tell us what
are the internal mechanisms allowing hawks to fly the way they
do. For sure, hawks do not fly based on an understanding of the
properties of logarithmic spiral. How can they show the corresponding
behavior? The often invoked evolutionary explanation
offers poor help: it accounts for behavior in terms of its reproductive
advantage. As the spiral-like flight proved advantageous for
hawks, those who performed it were able to generate more offspring,
while the others extinguished. No generative theory here:
it tells us not how hawks produce the behavior in question. We
could use the mathematical theory to describe their behavior, and
incorporate the mathematical explanation into a set of ad-hoc
behavioral rules for reproducing it. But neither the mathematical explanation, which describes internal causes, nor the set of ad-hoc
rules are generative.
Now, a fully generative explanation implies a more general theory
of how external causes, including fitness-enhancing effects,
get converted into internal reasons (what sometimes are called
proximate causes of behavior). Agent-based models are often
limited in focus, and not easily compatible with the temporal perspective
and the theoretical requirements of a fully generative - in
the sense here intended - explanation. Do we always need a generative
explanation? Not really, as ad-hoc rules sometimes are just
all that is needed to explain behavior. This is the case of entirely
programmed organisms, and it may even be the case of hawks, as
far as we know. Sometimes, instead, you need more. Suppose you
want a hawk to learn a new trick with respect to the approach
behavior. That is, you, or nature, in the form of new environment
- perhaps, but we’re letting imagination run wild here, in
the form of a prey that develops a counterstrategy to the spiral.
Then, immediately how the flight is generated becomes important:
how much learned, how much hard wired, and where; in a
plastic neuronal connection, or in a fixed relative placement of
eye and bone? Suddenly, to reproduce their behavior you would
need more than a rigid set of rules; you need to know how it is
generated.
Cognitive modeling aims at finding the general mechanisms
yielding the wide spectrum of behaviors of relatively autonomous
systems. Of course, you don’t need such mechanisms to simply
reproduce behavior. The more specific the target behavior, the
lesser you need a cognitive agent-based model. Since ABM is often
used to investigate fairly specific phenomena, either mathematical
model or a set of ad-hoc rule are preferred over cognitive modeling.
But together with cognitive modeling, we also dispense away
with truly generative modeling.
2.4. WHY BOTHER WITH GENERATIVE EXPLANATION
One might say, who cares after all? Provided we can reproduce
behavior, observe it and make artificial experiments to optimize
it, why bother with theory-driven generative modeling?
There are several reasons. One is that a truly generative explanation
is needed to model complex social dynamics. For universal
admission, the dynamics of social entities and phenomena is at
least bidirectional if not multidirectional. Entities and properties
emerge from the bottom up and retro-act on the systems
that have generated them. Current agent-based models instead
simulate only emergent properties, i.e., the way up of social
dynamics. To mention only a few examples, the ABM literature
offers countless models of the emergence of segregation,
norms, reciprocity, altruism, cooperation, punishment, conventions,
institutions, coalitions, leadership, hierarchies, the modern
state. Studies of different types and levels of downward causation
are much less frequent (to cite some exceptions, see Gilbert,
2002; Conte et al., 2013). However, how to change self- and otherdamaging
behaviors (i.e., smoking, over-eating, etc.) was ranked
as the fourth most important among the top-ten hard problems
the social sciences will have to address in the near future (Giles,
2011).
Agents should not be taken for granted as they change under
different types and degrees of social influence. Entities at the macroscopic level affect them and their behavior, and we must
understand how this can happen if we want to drive, enforce, or
prevent such an influence. This a line of research that presents
obvious ethical issues, but at the same time addresses themes so
important that social science cannot just leave them alone, or,
even worse, desert them to market solutions. For example, at least
in some fields, we badly need to know how to reduce or control
people’s overconfidence, for example in finance, where it so heavily
contributed to the last financial crisis (see Akerlof and Shiller,
2010), causing a disruption of global scale; how to change people’s
bad food habits, which are mainly responsible for highly diffuse
diseases as diabetes; how to make low compliant populations to
obey the norms, how to increase social trust, reduce hostility
toward out-groups, favor communitarian attitudes, and so on.
All of these questions might find useful answers based on reality
mining. Through Google or Yahoo we may trace people’s habits,
moods, investment decisions, political views, risk propensity and
attitudes toward culture, education and migration. Based on this
information, we may drive production, capital movements, business
strategies, political decisions, and international cooperation.
But we will not be able to suggest effective plans for modifying
such behaviors and the underlying mental states, unless we understand
the mental dynamics and how this interacts with the social
dynamics, and model the cognitive mechanisms that respond to
external influence and rule behavioral change. In absence of such
theory and model, we will not get to the core of hard problems.
2.5. A MISSED OPPORTUNITY
ABM is a powerful means for investigating the hinge between different
domains of reality, including economy, environment, and
society: systems’ behavior at different levels of scale. It is necessary
to explain phenomena pertaining to any domain of reality that is
heavily dependent on the behavior of autonomously interactive
systems, as was convincingly argued by Epstein. More, ABM is
unique for allowing a generative approach to behavioral systems
in the sense here defined, and somewhat different from Epstein’s,
i.e., to describe phenomena in terms of the external and internal
mechanisms that produce them.
However, ABM seems to have fulfilled its mission only in part.
Its generative capacity has been deployed to a lesser extent than
could have been the case. The practice of ABM missed the opportunity
it provided: paradoxically, the same principle that led it
to a fast popularity, like the KISS principle—i.e., keep it simple,
stupid - introduced by Axelrod (1997), and moreover the procedure
to find the minimal required conditions to obtain a given
phenomenon, do now sentence ABM to a premature end. The
KISS principle still drives most of the simulation work: we have
performed a check on a whole year of JASSS, a journal that we
consider representative of the files. In 2013, JASSS published 49
papers, of which 38 could be classified as simulations (the rest is
composed mostly by theoretical papers). Of those, 30 could be
considered as following the KISS advice, which makes about the
80% of published papers.
If internal mechanisms are ad-hoc and arbitrary, why don’t
dispense away with them in favor of more powerful quantitative
modeling allowing the same phenomena to be accurately predicted?
Why bother with agents, if one can apply computational tools to reality mining and platforms to large scale real-world
data-driven simulations, and aim at even higher orders of magnitude,
enabling us to forecast events at aggregate levels, such
as epidemics, climate change, and traffic jams? Couldn’t it be
the case that a mere quantitative use of computational tools be
enough to forecast financial crises, social instability, and even
human well-being?
It could be the case, indeed. However, centuries of failed
attempts (see the “Social Physics” case mentioned above) make
us doubtful. But what is maybe more important, by pursuing
this quantitative approach alone, science would have lost a
wonderful opportunity: to understand the micro-foundations of
phenomena at aggregate levels and how the latter (re)generate
them.
3. COMPUTATIONAL SOCIAL SCIENCE
Science, like history, is not a linear process. A decade ago, social,
and behavioral science dropped the disciplinary label (Conte,
2002) under the influence of an entirely new field, ABM. In the
last couple of years a CSS is being re-proposed. But CSS is being
practiced since a couple of decades if not earlier. What is new to
the current program?
Computational Social Science (from now on CSS) can be
meant in at least three different ways, the deductive, the generative,
and the complex one; and it should be made clear which one
we are referring to. As these are conceptual, rather than empirical,
variants, there is no need to have each of them matching a defined
historical example of CSS, since concrete examples are often a
mix. Let us characterize variants also with reference to existing
programs and try to forecast what their consequences might be.
3.1. THE DEDUCTIVE VARIANT
The second half of the last century is constellated of attempts to
apply the theory-building instruments of mathematics and the
theory-testing tools of computer science on one side, game theoretic,
and logic-based computational models on the other, to
describe and explain social phenomena. The latter, in particular,
attempted at deducing properties at the macro-level from
general assumptions at the micro-level. Expectations á la homo
economicus, allowed by the theoretical framework, turned out to
be wrong, what did not imply that the approach was incorrect,
only that it had been based on the wrong assumption, depending
on the theoretical and sometimes ideological positions of the
authors. What was worse, these position were often left implicit.
The deductive variant consists of formulating the mathematical
equations that account for the phenomena to be explained. With
the support of observation and data gathering, parameters can be
assigned their correct values. Although the theoretical framework
is often much too simple, the general program scarcely interdisciplinary,
and the ambition for social impact mainly based
on a rather prescriptive view of micro-level theory, deductive
CSS yielded a foundational, general, explanatory theory of social
systems. A lesson we should not forget.
3.2. THE GENERATIVE VARIANT
The decline of the rationality paradigm produced several consequences.
One of these was a stronger and more interdisciplinary effort to ground computational models on explicit models of
the micro-foundations. This led to the advent of the generative
variant of ABM, which derives its explanatory vocation
and micro-foundational framework from the deductive variant.
Unlike it, generative science aims at modeling operational microscopic
rules that generate macroscopic phenomena, rather than
formulating mathematical equations from which to deduce them.
The explanatory vocation is declined in a radically different way:
rather than describing a causal process from the outside, the modeler
attempts to show the internal rules that initialize it and follow
the unfolding of it all the way up to the observed effects.
As argued in the preceding section, however, ABM fulfilled its
mission, provide generative theories, to a lesser extent than was
expected. If the deductive variant was found to theorize upon
fairly abstract phenomena and has often been criticized for its
poor predictive capacity, the generative variant did not prove any
better at prediction, partly due to problems of validation and
calibration.
3.3. THE COMPLEX VARIANT
Inductive computational science is certainly not new (Newell and
Simon, 1976). The necessity to combine mathematics and logic
with learning, probability, and induction is receiving a growing
attention since the early nineties in several computational
disciplines such as knowledge representation, reasoning about
uncertainty, data mining, and machine learning. Nor is new the
use of computational instruments for quantitative social science:
it suffices to think of the wide application of statistics package
for social scientific research, and by the number of repositories
and archives of social scientific data (for example, http://
www.data-archive.ac.uk/). However, techniques of data-mining
are exercising an even stronger influence on the social sciences.
The use of advanced computer technology by social scientists is
also shown by sites where freely available web resources are assembled
with information on how access social scientific data (see
for example, http://guides.lib.wayne.edu/socialsciencesdata), and
by funded programs for interfaces between computer and social
sciences.
A new impulse to computer-based quantitative social science
is coming from the science of complexity, which is now going
through a season of deserved popularity. The use of complex
systems’ methods, models and techniques to economic systems
goes back to the nineties (for a rather informative introduction,
see Mantegna and Stanley, 2000), and the welcome received by
mechanical statistics in the field of economics and finance was
such as to encourage its wider application to the rest of the social
sciences. The popularity of sociophysics grew even more under
the influence of success stories, especially concerning the domain
of pedestrians’ crowd (Helbing et al., 2000) and that of epidemics
(Pastor-Satorras and Vespignani, 2001). In the last few years, a
diffuse uncertainty related to globalization, international and cultural
conflicts, and the recent financial crisis, led to the necessity
to anticipate and manage critical events on the front-stage. Not
only stakeholders and policy makers but also, and consequently,
research and development funding agencies and evaluators laid
emphasis on science as a system of warning, a source of anticipatory
information on the performance of aggregate systems, 
simultaneously triggering and guiding the action of politicians,
administrators, and businessmen. But science is more than anticipation.
It is first of all explanation. Accurate prediction can do
without explaining, especially if it is based on large datasets and
sophisticated techniques for extracting knowledge out of them.
Science cannot. Of course, explanation may be allowed by statistical
analysis. For example, topological properties of complex
networks are found among the main factors affecting epidemic
dynamics (for a review, see Yang et al., 2007). But this is not always
the case. Indeed, this is not the paradigmatic case in those social
phenomena in which behavior can be assumed to be irrelevant,
or non-influential.
Behavior is irrelevant or non-influential in social dynamics
where the implications of the phenomenon in question are social,
but its nature is not. To go back to epidemics, the nature of epidemics
is biological. The level of reality involved entities belong
to does not matter for the observed phenomenon to take place:
the nature of entities involved in and target of epidemics matters
not. In the spread of epidemics, the difference between human
behavior and that of particles in the space does not matter,
nor does the difference from carriers and the viruses they carry
around. But in other cases, that is, when the nature of behavior
matters, accurate statistical analyses of social dynamics can
maybe reach predictive power but cannot fully explain what is
going on.
As a hypothetical example, suppose we want to know what are
the main factors responsible for the dynamics of opinions. Again,
current models (Deffuant et al., 2001; Galam, 2002; Hegselmann
and Krause, 2002; Castellano et al., 2009), find that the structure
of the network of communication affects opinion dynamics.
Of course, the source of information also matters; a contrasting
source may inhibit the effect of media broadcasting and the
process is non-linear: under a given critical level of coverage,
the broadcasting message may be inhibited by a “contrarian”
opinion spread through word of mouth. Analogously, below a
critical level of confidence “contrarian” opinions may reach all
agents (Castellano et al., 2009). Social dynamics are often nonlinear
and typically smolder at some length under the ashes and
only subsequently surface in convergent opinions or behaviors.
Suppose one predicts the moment(s) at which this will happen
in real-world dynamics thanks to statistical analysis and
physical models. The question is why it happens. Of course behavior
is irrelevant to predict when convergence will occur. But it
matters if, for example, we want to affect the process, by shortening
or delaying it, or even prevent it; to educate people to a
higher autonomy; to favor info-diversity; finally, to convert opinions
in something more solid and resistant, like knowledge, and
so on.
People withdrawing support from political leaders is a good
example of non-linear opinion dynamics. It is unclear when people
change their minds and turn down their leaders. The destiny
of a popular (and often populist) figure is often decided upon in
a very short time. Today, those who enjoyed the favor of their followers
until yesterday, may suddenly lose popularity and fall in
disgrace, what is again a matter of threshold: after a certain level
of spreading, and perceived spreading, agents are led to modify
their opinions, what probably reveals an interesting effect of shared representations about shared opinions on one’s confidence
level. Possibly, such a lowering confidence leads agents to be more
eager to change opinions.
However, the circuit may be completely different: agents may
resist pressure to change opinions despite contrasting evidence
for reasons of cognitive dissonance. The more the contrasting
evidence they gather, the higher the dissonance. To reduce
it, they try to ignore evidence that is less costly than change
opinion, which imply dropping the previous commitment and
making a new one. As the perceived distance from others’ opinions
increases, however, agents either hide their opinions or
must defend them openly. If they choose the latter strategy, they
may even end up by accepting to form part of a minority. If
they take the former option they cannot get along with deception
too long as cognitive dissonance increases. Consequently,
they accept others’ opinions as own, and are likelier to convert
them in open behaviors to convince others and themselves
about the solidity of their new opinions. Both routes
imply critical thresholds for totally different reasons. To act effi-
caciously on this process, we must be clear what is actually
going on. Confidence has different implications from cognitive
dissonance and self-deception. To increase confidence may
lead to higher stability in the former case, but not in the
latter.
To sum up, to model social dynamics without taking into
account the internal (cognitive) dynamics of the entities involved
in a social phenomenon does not prevent accurate predictions
of critical events and changes. It may even allow to find out factors
responsible for such events and changes, and this is the case
with dynamics for which the social nature of behavior is irrelevant.
To understand internal dynamics is crucial instead whenever
we need not only to anticipate but also to understand events for
which behavior is relevant. Model the internal dynamics of events
is necessary not only for scientific reasons but also for guiding
intervention.
4. TOWARD A NEW INTERDISCIPLINARY FOUNDATION FOR
CSS
The program for CSS needs clarification. Why would such a
program be necessary, if we practice CSS since at least a couple
of decades? Of course one might say that we need to
introduce a new Curriculum at the academic level, and that
to do this implies to form a new, cohesive, scientific community,
form associations, give visibility to this new Curriculum,
strengthen the academic, editorial, and political power of the
underlying community etc. However, the reason for a program
on CSS is not only political but also scientific. As
seen so far, there are different variants of CSS and to take
a pluralistic approach to it may be considered wise. CSS
could be seen today as a larger umbrella under which different
approaches might coexist and somehow feel legitimate.
Hence, generative ABM might be practiced by a subset of
social scientists, while others might prefer a purely quantitative
approach, based on data-mining and numerical simulation,
and still others might continue to formulate abstract theories of
social action in elegant equations and deduce their macro-level
consequences. The main thesis of this paper is that such a multidisciplinary
program for CSS would be another lost occasion for science. It
would but result either in tight but essentially useless theories,
or in accurate predictions of poorly understood social phenomena.
In the best possible case, mathematicians will go on citing
one another in fairly close circuits of beautiful minds, physicists
will find new phenomena affected by the properties of scale-free
networks, and social scientists will give up generative ABM in
the desperate attempt to produce competitive quantitative social
science and get reasonably high scientific scores.
An interesting, innovative program in CSS can only be interdisciplinary.
Why and where is the difference? The reason lies
in the necessity to take advantage from the different modeling
methods and techniques to both understand and predict
the same phenomena! The difference of interdisciplinary
from multidisciplinary CSS consists not only of a convergent
investigation of the same phenomena from different perspectives
and involving different competencies, what would already
be a step ahead with regard to current practice, but in a
more radical process aimed at multilevel and modular modeling.
Such a type of modeling would allow to describe the
dynamics of given phenomena at aggregate levels based on large
datasets, find out the criticalities thanks to complex dynamic
systems models, make hypotheses about the behavioral dynamics
when this is relevant, use ABM to check internal consistency
and observe the resulting states at the aggregate level,
apply cross-methodological experimental methods to validate
the hypotheses against real-world data, update data-mining
methods, models, and probability distribution models to newly
acquired knowledge, and use mathematical equations when possible
to close the number of states resulting at the aggregate
level.
An interdisciplinary endeavor like this certainly points out
some new challenges: not only to extract knowledge from larger
and larger datasets, not only develop simulators that scale up
of several orders of magnitude, or feed simulation and datamining
with online real-data, not only to develop supercomputing
infrastructures and systems to transfer data to supercomputing
platforms, but also develop simulation platforms
that scale up both in terms of systems’ dimensions and in
terms of levels of complexity. We need to account for largescale
systems as well as more complex entities. We need to
apply simulation methods to understand the social and the mental
dynamics and to describe their interrelationships. Last, but
not least, we need incentives that are compatible with such an
endeavor—publication-wise and career-wise. This is a challenge
for a program on CSS that deserves attention and investment.
CSS ought to accept it, or another occasion will be lost for
founding a novel, integrated, interdisciplinary, falsifiable science
of society helping us to solve transformative and foundational
problems.
AUTHOR CONTRIBUTIONS
Rosaria Conte and Mario Paolucci elaborated together the ideas
presented in the paper. Rosaria Conte drafted the first version.
Mario Paolucci substantially revised it for important intellectual
content. FUNDING
This paper benefits from the stimulating discussions in the
FuturICT FET Flagship Pilot Project (EU FP7/2007-2013, g.a. no.
284709). The publication of this work is funded by the PRISMA
project (PON04a2 A), within the Italian National Program
for Research and Innovation (Programma Operativo Nazionale
Ricerca e Competitività 2007–2013).
ACKNOWLEDGMENTS
We thank Neil Yorke-Smith, Gustavo Nardin, and Federico
Cecconi for encouragement and suggestions. We are also thankful
to anonymous reviewers who helped us to tune the paper and
helped to clarify some obscure passages.
Content Analysis and the Algorithmic Coder: What Computational Social Science Means for Traditional Modes of Media Analysis
To deal with ever-larger datasets, media scholars are
increasingly using computational analytic methods.
This article focuses on how the traditional (manual)
approach to conducting a content analysis—a primary
method in the study of media messages—is being
reconfigured, assesses what is gained and lost in turning
to computational solutions, and builds on a “hybrid”
approach to content analysis. We argue that computational
methods are most fruitful when variables are
readily identifiable in texts and when source material is
easily parsed. Manual methods, though, are most
appropriate for complex variables and when source
material is not well digitized. These modes can be
effectively combined throughout the process of content
analysis to facilitate expansive and powerful analyses
that are reliable and meaningful.
Keywords: content analysis; computational social science;
computational content analysis;
hybrid content analysis; digital research
methods; media analysis; algorithms
The abundance of digitized data has become
a defining feature of modern society, and
particularly of communication that is expressed
through digital, social, and mobile platforms:
tweets, likes, links, shares, texts, posts, tags, and
more—literally billions of data points about
social behavior that, potentially, might be
assembled, accessed, and ultimately analyzed
by various institutions and individuals. For
Rodrigo Zamith is an assistant professor in the
Journalism Department at the University of
Massachusetts, Amherst. His research focuses on the
reconfiguration of journalism in a changing media environment
as well as the development of digital research
methods.
Seth C. Lewis is an assistant professor and the Mitchell
V. Charnley Faculty Fellow in the School of Journalism
and Mass Communication at the University of
Minnesota–Twin Cities. His research focuses on journalism
and technology. He is coeditor of Boundaries of
Journalism: Professionalism, Practices, and Participation
(Routledge 2015)
communication and media research, especially, the possibilities are great: as computational
tools and processes have become easier to employ (e.g., via opensource
software), as large-scale datasets of digital media content have become
more readily attainable (e.g., by scraping tweets or websites), and even as print
media content, such as old newspapers and books, have become increasingly
available in digital form, new types of large-scale, algorithm-driven analyses of
media content have become possible, enabling scholars to address novel questions.
To cite just one example, Colleoni, Rozza, and Arvidsson (2014) used more
than a billion data points to reveal key differences in the structures of political
homophily among Democrats and Republicans on social networks such as
Twitter. Such “naturally occurring” data such as public tweets, and the growth in
computing capacity that facilitates the collection and analysis of such voluminous
data, marks a key turn toward computational social science (Shah, Cappella, and
Neuman, this volume).
As a distinct approach to social inquiry, computational social science is characterized
by research that (1) uses large, complex datasets; (2) often involves social
and digital media sources; (3) employs algorithmic or computational solutions to
generate patterns and inferences from data; and (4) is applicable to social theory
in a wide variety of domains (Shah, Cappella, and Neuman, this volume).
Examples of such research may be found across a range of disciplines, including
a growing number and variety at the intersection of the social sciences and the
digital humanities (Bruns 2013). Much of this work involves the quantitative
analysis of textual content. Yet unlike traditional content analyses—which rely
predominantly on human judgments—these studies are largely driven by algorithms
and frameworks that seek to automate the coding process. With this in
mind, we ask, What does this turn toward computational social science mean for
traditional forms of content analysis?
That question speaks to the very future of content analysis as a method—one
of the primary methods of mass communication research for many decades, and
one considered essential to a scientific study of communication (Riffe, Lacy, and
Fico 2014). The purpose of this article, therefore, is to (1) consider the traditional
way of conducting content analysis in light of the algorithmic coder, (2) assess
what is gained and lost in turning to purely algorithmic solutions, and (3) discuss
an alternative approach that leverages traditional and computational approaches
in tandem. Such an approach, we argue, can facilitate more expansive and powerful—yet
still reliable and meaningful—forms of content analysis within the present
turn toward computational social science.
Content Analysis and Algorithmic Approaches
Riffe, Lacy, and Fico (2014, 19) present what is, in our opinion, a particularly
good definition of quantitative content analysis: “the systematic and replicable
examination of symbols of communication, which have been assigned numeric
values according to valid measurement rules, and the analysis of relationships involving those values using statistical methods, to describe the communication,
draw inferences about its meaning, or infer from the communication to its context,
both of production and consumption.” They also present a comprehensive
framework for conducting content analysis—in a traditional fashion—breaking
the process up into three segments: (1) conceptualization and purpose, (2)
design, and (3) analysis.
We focus here on four key processes within that overall procedure: (1) the
development of the coding protocol and sheet; (2) the specification of the population
and, if applicable, the sample; (3) the establishment of intercoder reliability;
and (4) the coding of content. The other steps in the process, such as
identifying the problem and reviewing the relevant literature, we argue, are
generic, in the sense that they are applicable to most scholarly work, regardless
of the quantitative method employed. We also wish to clarify that the use of algorithms
to code content is not a new phenomenon (for an example, see Stone et al.
1962). However, what is novel about current efforts is the desire to automate
virtually the entire procedure—that is, to perform a content analysis with minimal
human intervention. We thus proceed to compare traditional and computational
forms of content analysis, explicating the human-centric and machine-centric
steps associated with each to set up a broader discussion about the relative benefits
and drawbacks of the algorithmic coder, the computational counterpart to a
human coder.
Coding protocol and the code sheet
Traditionally, once the hypotheses and research design have been finalized,
the researcher must develop a coding protocol, or a set of explicit and detailed
coding instructions for a human coder to follow. This, in turn, requires the
researcher to balance the level of instruction to ensure that it is neither so
detailed that it makes the application of the instruction too narrow and the protocol
too complex, nor so vague that it leaves too much room for interpretation
and thereby undermines reliability. A simple code sheet should then be developed
for the coder, listing every variable being coded and leaving some room for
the coder to enter his or her code.
With an algorithmic coder, there is no room for ambiguity in the coding protocol.
For example, a dictionary-based approach necessitates the development of
extensive lists of all possible permutations of distinct units—typically words and
their combinations—that represent a given construct (e.g., a victimization story
frame; see Vliegenthart and Roggeband 2007). Machine learning approaches
require the development of clearly specified models to identify patterns and
make inferences about the object of study from the data, typically by calculating
the probability that content is of a certain class and categorizing it based on the
highest probability (e.g., Grimmer 2010). The algorithmic coder then employs
these lists of words and models with the instructions for how to use them coming
in the form of unambiguous computer code.
The lack of ambiguity in an algorithmic approach sets it apart from traditional
content analysis for two reasons. First, it requires the researcher to be acutely aware of the many ways that a construct may manifest itself in a given text, especially
when utilizing a dictionary-based approach. While researchers have long
needed such awareness to develop comprehensive codebooks in traditional content
analysis, they could rely on the judgment of human coders to adapt to unusual
manifestations; under an algorithmic approach, this is not possible. Second,
it may potentially facilitate replicability and transparency by ensuring that every
step along the way is clearly documented, and further enables content analyses
to be more comparable by virtue of the adoption of the same dictionaries and
models. This is a stark contrast to traditional content analysis, where a considerable
amount of instruction occurs verbally during the coder training process, with
only a portion of that instruction making it into the protocol.
Additionally, an algorithmic approach departs from traditional analysis by
being considerably more iterative. Algorithms and dictionaries must often be
repeatedly revised and tweaked to improve their performance. While it is not
uncommon for content analysts to produce a handful of revisions to a coding
protocol, an algorithmic approach may involve dozens of rounds of changes to
ensure that the classification of items yields a satisfactory level of construct
validity.
Finally, the algorithmic coder does not need a code sheet. As content is coded,
data are automatically added to digital, structured datasets that can be easily
imported into a statistical analysis program. This point, while perhaps obvious to
some, is worth noting because of its significance for validity and reliability. The
algorithmic process removes data-entry error, both on the part of a coder mislabeling
text and as data are transferred from a code sheet to the final dataset.
Specifying the population
The specification of the population requires the researcher to decide which
materials to analyze and how much of those materials to analyze. Here, the population
comprises all the potential content that falls within the restrictions set by
the researcher during the design process. Researchers may, and often do, opt to
use a sample, or portion, of that population to reduce the workload of the coders,
or to enable them to loosen other restrictions (e.g., look at more media outlets or
over a longer period of time).
An algorithmic approach departs from traditional content analysis in that it can
generally be scaled up with ease. Provided that vocabularies, features, and patterns
do not change significantly (e.g., selecting texts that vary greatly in structure
and conventions), the difference between coding a large corpus of material and
a very large corpus of material is relatively minor (Hopkins and King 2010).
Researchers may thus use a larger sample, which is generally more likely to represent
the overall population, or even a census.1 Additionally, an algorithmic
approach may leverage computer scripts to systematically locate, obtain, and
organize data, ensuring that the population—or an appropriate sample—is captured
more systematically than if done by a human being (Karlsson and
Strömbäck 2010). Last, there is often a need to preprocess collected content and
convert it into research-grade data that are in a form that may be easily parsed and analyzed by an algorithm. For example, this may include identifying and correcting
repeated errors in source documents, such as words that were split incorrectly
during optical character recognition (see Leetaru 2012)—a painstaking
process that has only limited parallels to a traditional approach.
Assessing intercoder reliability
Once the coding protocol has been designed and the population specified, the
researcher will want to ensure that the coding protocol’s definitions are reliable.
Typically, this is done through the proxy of having multiple individuals doublecode
a randomly selected subset of the sample. The researcher then assesses
whether the coders made the same coding decisions the majority of the time,
often using statistics such as Scott’s pi or Cohen’s Kappa. Generally, multiple
rounds of reliability testing are necessary to attain acceptable coefficients.
Because computers are deterministic machines, they are able to execute a
given set of instructions with perfect reliability (Grimmer 2010). There is no
need to assess intercoder reliability in an algorithmic approach, thus distinguishing
it from traditional content analysis. This is noteworthy in light of scholars’
concern about the poor reporting of intercoder reliability in published content
analyses (Lovejoy et al. 2014; Riffe and Freitag 1997).
However, to assess the validity of an algorithm, researchers often choose to
measure it against a “gold standard”—typically a human-coded dataset that is
presumed to represent the “correct” coding decisions (Grimmer and Stewart
2013). Additionally, the chosen algorithm may also be compared against competing
ones in a benchmarking process (for an example, see Thelwall et al. 2010).
There is no counterpart to this procedure in traditional content analysis: the decisions
made by human coders operating under one codebook are rarely compared
against those made under comparable codebooks, and the validity of a codebook
is seldom established by measuring it against a specific standard.
Coding materials
Once reliability has been established, the remainder of the sample or census
is coded. Sometimes, multiple coders—all of whom should have participated in
the assessment of reliability—will divvy up the work; otherwise, a single coder
will take on the remaining work. At this point, no major changes should be made
to the coding protocol since it would necessitate a reassessment of its reliability.
Data are then stored, analyzed, and interpreted.
An algorithmic approach departs from traditional content analysis here by
being exponentially faster. Simple analyses that may take human coders months
can often be accomplished in minutes or hours using a desktop computer
(Manovich 2012). Additionally, more sophisticated analyses can often be parallelized.
Rather than adding human coders to expedite the analysis, researchers may
quickly add dozens more central processing units (CPUs) at any point along the
way, provided they have the resources, to speed up the process. Consequently,
recoding items—perhaps in response to suggestions by peers or reviewers—is generally both easy and expeditious with the algorithmic coder. This is a stark
contrast to traditional content analysis, wherein such modifications are generally
infeasible.
An algorithmic approach to content analysis, therefore, does not neatly fit into
the traditional framework for conducting a content analysis. Unlike traditional
content analysis, an algorithmic approach requires the coding protocol and
instructions be unambiguous, be more iterative, be conducted on a far grander
scale, and be considerably more flexible in accommodating post-hoc adjustments.
Moreover, the absence of the need for a code sheet or to perform intercoder reliability,
the need to preprocess content to ensure that it is in a format that is
amenable to machine processing, and the common requirement of “validating”
algorithms by measuring them against a gold standard all make it difficult to map
the work of the algorithmic coder onto the traditional framework for content
analysis. A revised framework for the content analyst operating in a computational
social science environment is therefore warranted (see also Herring 2010).
Challenges for Algorithmic Content Analysis
Communication researchers, especially those interested in studying human perceptions
and practices as expressed on digital and social media platforms, currently
have access to a growing variety and volume of data. This “siren-song of
abundant data,” as Karpf (2012, 648) has called it, is enticing on one hand, but
vexing on the other. Much of the public data available to researchers are ephemeral
and thus hard to capture reliably; they are often polluted by “noise” from the
influence of spammers (e.g., appropriating a popular hashtag with unrelated
information); and they are often more limited than they may appear (e.g., Twitter
allowing access to only a portion, and not all, of its publicly available tweets via
its application program interface [API]). All of this leads Karpf to encourage
“methodological skepticism” because “the glittering promise of online data abundance
too often proves to be fool’s gold” (2012, 652; see related discussion in
boyd and Crawford 2012).
What do such cautions mean for content analysis in particular? In light of the
considerable benefits offered by algorithmic approaches—namely, the potential
to quickly collect and analyze massive amounts of digital content with perfect
reliability and exceptional transparency—many researchers may be tempted to
eschew traditional content analysis for an algorithmic approach in a computational
social science environment. However, in comparison to the algorithmic
coder, a human coder in many cases may be more attuned not only to the richness
and context of the topic at hand, but also to the data-quality issues described
by Karpf (2012). A human, for instance, may be able to better recognize the
presence and relative pervasiveness of spam when studying tweets that include a
particular hashtag. Ultimately, humans can understand the larger sociocultural
contexts through which tweets, like other social media posts, function as a form
of communication (see discussion in Weller et al. 2014). Beyond issues of data quality, as Krippendorff (2013, 210) notes, “programming
a machine to mimic how humans so effortlessly understand, interpret, and rearticulate
text turns out to be an extraordinarily difficult, often impossible, undertaking.”
While this may overstate the near-term aims of researchers in this area, it
points to the broader ambition of limiting the role of human coders. Specifically,
Krippendorff points to the difficulty of attaining acceptable levels of validity with
complex and oftentimes ambiguous textual representations, such as sarcastic
remarks and metaphors.2 As several scholars have argued, while algorithmic
approaches yield satisfactory results in surface-level analyses or analyses that focus
on structural features, their performance is significantly worse when assessing
more complex features of texts (Conway 2006; Sjøvaag and Stavelin 2012).
A Hybrid Approach for Computational Social Science
Recognizing the current challenge of using algorithms to accurately analyze and
classify complex human communication, Lewis, Zamith, and Hermida (2013, 36)
proposed a hybrid approach to content analysis “that combines computational
and manual methods throughout the process . . . [to] retain the strengths of traditional
content analysis while maximizing the accuracy, efficiency, and largescale
capacity of algorithms for examining Big Data.” In their analysis of news
sourcing practices on Twitter during the Arab Spring, they used computational
methods first to organize the data via a Python script, which turned a messy text
file containing tens of thousands of tweets into a standardized, comma separated
values (CSV) data file through which key variables could be identified and studied
for initial patterns. Then, additional computer scripts allowed for the coding
of simple features such as the usernames mentioned in a particular tweet—a
necessary variable for understanding how certain individuals were included in
sourcing the news (Hermida, Lewis, and Zamith 2014). Thereafter, Lewis and
colleagues developed an electronic interface to facilitate the work of human coders
and thereby reduce—or even eliminate—certain sources of error. “Through
it all,” they emphasize, “computational means were enlisted to enhance, rather
than supplant, the work of human coders, enabling them to tackle a larger body
of data while remaining sensitive to contextual nuance” (Lewis, Zamith, and
Hermida 2013, 47).
In a similar vein, Sjøvaag and colleagues (Sjøvaag, Moe, and Stavelin 2012;
Sjøvaag and Stavelin 2012) utilized computer scripts to “freeze the flow of online
news” (Karlsson and Strömbäck 2010, 16) to facilitate a hybrid form of content
analysis. They first used Python scripts to scrape a year’s worth of coverage—
nearly seventy-five thousand news articles produced by the Norwegian
Broadcasting Company (NRK)—and automatically code for web-specific features
such as hyperlinks and multimedia. Thereafter, in a more traditional fashion,
they manually coded a smaller subset of articles to capture contextual
features such as topics, themes, and frames—none of which could be adequately
classified via an algorithm (for details, see Sjøvaag and Stavelin 2012). The development of computational tools and frameworks that can facilitate
the blending of human judgment and algorithmic efficiency strikes us as an area
of research that deserves additional attention from content analysts amid the turn
toward computational social science. This suggestion may appear at first to be
paradoxical: after all, how can researchers analyze massive, complex sets of textual
data with human involvement? We concur that there are instances where
human involvement is simply infeasible. However, we also argue that in a considerable
amount of scholarly applications, human involvement is entirely practical.
First and foremost, we share the position that a census is not always necessary.
Just because researchers can conduct a census does not mean that they should
eschew the tradition of sampling that has served the field well (Riffe, Lacy, and
Fico 2014). As Mahrt and Scharkow (2013, 20) put it, “researchers need to consider
whether the analysis of huge quantities of data is theoretically justified,
given that it may be limited in validity and scope, and that small-scale analyses of
communication content or user behavior can provide equally meaningful inferences.
. . .”
However, in some research, even a good sampling strategy will yield a sizable
corpus of textual data. Consider a hypothetical study of the sourcing practices of
journalists at dozens of news outlets in stories about immigration over a 30-year
period. Such a study may involve the analysis of hundreds of thousands of fulllength
stories—far more than a small group of human coders could analyze in a
reasonable amount of time. While an algorithmic approach may appear to be
ideal for such a research endeavor, it is probable that certain variables, such as
source type, would yield results that are lacking validity because of the limited
amount of explicit attribution information in news articles.
A hybrid approach, however, would make great sense for such a project.
Algorithms could be leveraged to code for simple variables such as the outlet
from which the article was found and the date of the article. In such instances,
Named Entity Recognition3 (see Béchet 2011) could be leveraged to identify
sources within those articles, and dictionary-based or machine learning–based
approaches could be used to estimate the appropriate classification of complex
variables. Additionally, an electronic interface could then be leveraged to allow
coders to quickly verify algorithmic decisions by prefilling code sheets, with visual
cues added to the source document to illustrate the rationale for the algorithm’s
decision.
To further illustrate this point using the aforementioned source-type variable,
consider the possibility of using algorithms to capture every text segment in
which a specific source appears, presenting these segments sequentially to a
human coder and conveying the estimated probability for each source-type category,
and automatically propagating the human’s single coding decision for every
article in which that source appears. While some sources may only appear once
in the entire corpus of coverage (e.g., a man-on-the-street source), a considerable
amount are likely to reappear multiple times (e.g., government officials).
While a myriad of additional examples could be readily offered, this simple
scenario helps to illustrate how a blend of computational tools and human expertise
could reduce the likelihood of having invalid coding decisions—a common criticism of the algorithmic approach (Manovich 2012)—while enabling researchers
to tackle far larger datasets than they otherwise could. This, we argue, enables
content analysis to remain relevant and yield insightful knowledge within a computational
social science paradigm. However, while the turn toward computational
social science has excited a flurry of activity in the development of more
sophisticated techniques for modeling language and classifying phenomena (e.g.,
Grimmer 2010), considerably less work has been done in the development of
tools and frameworks that facilitate the interaction of computational tools and
human expertise.4
Conclusion
The turn toward computational social science requires scholars to reconsider
how content analysis is done and whether it needs to be adapted to remain relevant
in a changing research environment. To deal with the large, complex datasets
that characterize this turn, a number of scholars have turned to computational
forms of content analysis and subsequently shifted from the human coder to the
algorithmic coder (e.g., Grimmer 2010; Vliegenthart and Roggeband 2007). The
work of the algorithmic coder and the design of an algorithmic approach do not
neatly fit into the traditional framework for conducting content analysis. While
some processes, such as the development of a coding protocol, require only
reconceptualization, other processes, such as the need to validate algorithmic
decisions against a gold standard, hardly correspond at all with that traditional
framework. While we would not go so far as to argue that such forms are entirely
distinct, we do believe that a revised framework for the content analyst operating
in this new environment is needed (cf. Herring 2010).
To be sure, computational approaches and the algorithmic coder may yield
great benefits, such as increased efficiency, transparency, and post-hoc malleability.
Such benefits cannot be ignored. Nevertheless, computational approaches
also have clear limitations. Their reliance on digital materials that are often of
questionable quality and their focus on relatively simple and unambiguous content—lest
they run the risk of producing results lacking in validity—may be
problematic (Conway 2006; Mahrt and Scharkow 2013). As such, we argue that
for content analysis to remain relevant in the turn toward computational social
science, a hybrid approach must be further developed, one that preserves the
contextual sensitivity and validity that are central to traditional content analysis
and combines it with the large-scale capacity and reliability of computational
approaches. Specifically, while the work of Lewis and colleagues (2013) and
Sjøvaag and colleagues (Sjøvaag, Moe, and Stavelin 2012; Sjøvaag and Stavelin
2012), among others, offer a good starting point, we believe that there are several
avenues for building on their work. To this end, we advocate for the development
of tools and frameworks that facilitate the interaction of computational tools and
human expertise. In effect, we are arguing not only for preserving the best of the old human
coder while also embracing the best of the new algorithmic coder, in an idealized
or abstract sense, but also that it is time to take tool-building more seriously
within our discipline. We not only need to forge conceptual ways of
thinking about these problems but also build better technical systems to conduct
the kind of content analysis that truly blends the best of both worlds,
human and machine alike. Such a system might not only do what we are already
accustomed to having algorithms do (e.g., automatically coding certain
machine-readable structural characteristics) but also leverage advanced computational
techniques to provide cues for human coders and facilitate their
work, thereby leading to faster, more reliable, and ultimately more valid coding
decisions, even while retaining the important contextual awareness that humans
bring to the equation. Thus, the human coder and algorithmic coder, working
together, may help communication researchers to keep content analysis relevant
in the turn toward computational social science and also in fact propel it
forward as a method, such that it will, perhaps, be as indispensable to the next
generation of communication researchers as it has been to this one.
Notes
1. As other scholars have noted, turning to larger sample sizes or a census may not necessarily yield
better inferences (Mahrt and Scharkow 2013). Put differently, a proper sampling technique may yield
inferences that are just as good as if one had looked at the entire sample (Riffe, Lacy, and Fico 2014).
Additionally, as a practical concern, it is generally inappropriate to utilize inferential statistics when analyzing
a census (and is oftentimes meaningless when looking at massive datasets; see Ruggles 2014),
though many reviewers nevertheless expect to see them and sometimes reject manuscripts for failing to
use them. However, in some instances it is necessary to analyze a census to obtain a complete picture
(e.g., to not miss out on key bridging nodes when generating a network of actors from textual material).
When dealing with large sample sizes or a census, it is especially important that researchers focus on the
practical significance of differences and effects, and not be overreliant on statistical significance (Berman
2013).
2. Béchet (2011, 261) offers a simple illustration by discussing the challenge of having a Named Entity
Recognition algorithm correctly disambiguate, without prior specification, the English football club
Sheffield Wednesday (an organization) from Sheffield (a location) and Wednesday (a day of the week).
3. Named Entity Recognition refers to “a task consisting of detecting segments of a document . . .
expressing a direct reference to a unique identifier” (Béchet 2011, 257). It often uses linguistic models to
computationally identify individuals, organizations, and locations within a given text.
4. There are existing tools such as Crimson Hexagon that facilitate the collection of data and allow
researchers unfamiliar with techniques in the fields of natural language processing to make use of
advanced machine learning algorithms. While such tools may indeed be of great use to some researchers,
they also introduce some potential problems for scientific research. First, most user-friendly tools
are restricted to a certain kind of data, typically social media data and often just Twitter. This restricts
the questions and phenomena social scientists may study. Second, most of these tools—especially userfriendly,
all-in-one solutions—are closed-source and often offer limited capacity for exporting important
information necessary to replicate (or extend) a study. This is especially true for web-based
services, which may change their proprietary algorithms and offer no recourse for using a previous
algorithm. Thus, while such tools can be beneficial for many researchers, their use should be carefully considered.
